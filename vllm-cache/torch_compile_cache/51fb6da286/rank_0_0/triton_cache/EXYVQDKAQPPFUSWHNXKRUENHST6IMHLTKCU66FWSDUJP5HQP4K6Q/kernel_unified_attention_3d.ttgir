#blocked = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [16, 4], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#loc = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0)
#loc1 = loc(unknown)
#loc6 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":344:36)
#loc113 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":496:35)
#loc128 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":505:21)
#mma = #ttg.amd_mfma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>
#shared = #ttg.shared<{vec = 4, perPhase = 1, maxPhase = 16, order = [1, 0], hasLeadingOffset = false}>
#shared1 = #ttg.shared<{vec = 4, perPhase = 1, maxPhase = 16, order = [0, 1], hasLeadingOffset = false}>
#shared2 = #ttg.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0], hasLeadingOffset = false}>
#smem = #ttg.shared_memory
#loc153 = loc(callsite(#loc1 at #loc6))
#loc170 = loc(callsite(#loc1 at #loc113))
#loc173 = loc(callsite(#loc1 at #loc128))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx90a", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @kernel_unified_attention_3d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg4: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg5: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg6: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg7: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg8: f32 loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg9: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg10: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg11: i32 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg12: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg13: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg14: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg15: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg16: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg17: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg18: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg19: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg20: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg21: i64 {tt.divisibility = 16 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0), %arg22: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":296:0)) attributes {noinline = false} {
    %cst = arith.constant dense<4> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_0 = arith.constant dense<1> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_1 = arith.constant dense<0> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_2 = arith.constant dense<32> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_3 = arith.constant dense<1> : tensor<16x1xi32, #mma> loc(#loc1)
    %cst_4 = arith.constant dense<32> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_5 = arith.constant dense<0> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_6 = arith.constant dense<1> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_7 = arith.constant dense<0> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_8 = arith.constant dense<0> : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_9 = arith.constant dense<1> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_10 = arith.constant dense<1> : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_11 = arith.constant dense<128> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_12 = arith.constant dense<128> : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_13 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma> loc(#loc1)
    %cst_14 = arith.constant dense<0xFF800000> : tensor<16x16xf32, #mma> loc(#loc1)
    %cst_15 = arith.constant dense<0.000000e+00> : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_16 = arith.constant dense<1.000000e+00> : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_17 = arith.constant dense<0xFF800000> : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_18 = arith.constant dense<0.000000e+00> : tensor<16x128xf32, #mma> loc(#loc1)
    %cst_19 = arith.constant dense<512> : tensor<16xi64, #blocked2> loc(#loc1)
    %cst_20 = arith.constant dense<16> : tensor<16xi32, #blocked2> loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %c15_i32 = arith.constant 15 : i32 loc(#loc1)
    %c255_i32 = arith.constant 255 : i32 loc(#loc1)
    %cst_21 = arith.constant dense<32> : tensor<16xi32, #blocked2> loc(#loc1)
    %cst_22 = arith.constant dense<0> : tensor<16xi32, #blocked2> loc(#loc1)
    %cst_23 = arith.constant dense<1> : tensor<16xi32, #blocked2> loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %cst_24 = arith.constant dense<0.000000e+00> : tensor<128x16xbf16, #blocked1> loc(#loc1)
    %cst_25 = arith.constant dense<0.000000e+00> : tensor<16x128xbf16, #blocked> loc(#loc1)
    %c4_i32 = arith.constant 4 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c16_i32 = arith.constant 16 : i32 loc(#loc1)
    %cst_26 = arith.constant dense<4> : tensor<16xi32, #blocked2> loc(#loc1)
    %cst_27 = arith.constant dense<4> : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_28 = arith.constant dense<128> : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc1)
    %cst_29 = arith.constant dense<1> : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc1)
    %cst_30 = arith.constant dense<0> : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc1)
    %cst_31 = arith.constant dense<65536> : tensor<16x1xi64, #mma> loc(#loc1)
    %cst_32 = arith.constant dense<2048> : tensor<16x1xi32, #mma> loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = tt.get_program_id y : i32 loc(#loc3)
    %2 = tt.get_program_id z : i32 loc(#loc4)
    %3:2 = scf.while (%arg23 = %c0_i32, %arg24 = %c1_i32) : (i32, i32) -> (i32, i32) {
      %260 = arith.cmpi slt, %arg23, %arg24 : i32 loc(#loc152)
      scf.condition(%260) %arg23, %arg24 : i32, i32 loc(#loc152)
    } do {
    ^bb0(%arg23: i32 loc(callsite(#loc1 at #loc6)), %arg24: i32 loc(callsite(#loc1 at #loc6))):
      %260 = arith.addi %arg23, %arg24 : i32 loc(#loc154)
      %261 = arith.divsi %260, %c2_i32 : i32 loc(#loc155)
      %262 = tt.addptr %arg22, %261 : !tt.ptr<i32>, i32 loc(#loc156)
      %263 = tt.load %262 : !tt.ptr<i32> loc(#loc157)
      %264 = arith.divsi %263, %c4_i32 : i32 loc(#loc158)
      %265 = arith.addi %264, %261 : i32 loc(#loc159)
      %266 = arith.cmpi sle, %265, %0 : i32 loc(#loc160)
      %267 = arith.select %266, %arg24, %261 : i32 loc(#loc161)
      %268 = scf.if %266 -> (i32) {
        %269 = arith.addi %261, %c1_i32 : i32 loc(#loc162)
        scf.yield %269 : i32 loc(#loc162)
      } else {
        scf.yield %arg23 : i32 loc(#loc153)
      } loc(#loc161)
      scf.yield %268, %267 : i32, i32 loc(#loc163)
    } loc(#loc151)
    %4 = arith.subi %3#0, %c1_i32 : i32 loc(#loc164)
    %5 = tt.addptr %arg22, %4 : !tt.ptr<i32>, i32 loc(#loc19)
    %6 = tt.load %5 : !tt.ptr<i32> loc(#loc20)
    %7 = arith.divsi %6, %c4_i32 : i32 loc(#loc21)
    %8 = arith.addi %7, %4 : i32 loc(#loc22)
    %9 = arith.subi %0, %8 : i32 loc(#loc23)
    %10 = tt.addptr %5, %c1_i32 : !tt.ptr<i32>, i32 loc(#loc24)
    %11 = tt.load %10 : !tt.ptr<i32> loc(#loc25)
    %12 = arith.subi %11, %6 : i32 loc(#loc26)
    %13 = arith.muli %9, %c4_i32 : i32 loc(#loc27)
    %14 = arith.cmpi sge, %13, %12 : i32 loc(#loc28)
    cf.cond_br %14, ^bb1, ^bb2 loc(#loc28)
  ^bb1:  // 2 preds: ^bb0, ^bb2
    tt.return loc(#loc29)
  ^bb2:  // pred: ^bb0
    %15 = tt.addptr %arg7, %4 : !tt.ptr<i32>, i32 loc(#loc30)
    %16 = tt.load %15 : !tt.ptr<i32> loc(#loc31)
    %17 = arith.addi %16, %c255_i32 : i32 loc(#loc165)
    %18 = arith.divsi %17, %c256_i32 : i32 loc(#loc166)
    %19 = arith.muli %2, %18 : i32 loc(#loc35)
    %20 = arith.muli %19, %c16_i32 : i32 loc(#loc36)
    %21 = arith.cmpi sge, %20, %16 : i32 loc(#loc37)
    cf.cond_br %21, ^bb1, ^bb3 loc(#loc37)
  ^bb3:  // pred: ^bb2
    %22 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc38)
    %23 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc38)
    %24 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc38)
    %25 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2> loc(#loc38)
    %26 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc39)
    %27 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc39)
    %28 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc39)
    %29 = arith.divsi %22, %cst_27 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc40)
    %30 = arith.divsi %23, %cst : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc40)
    %31 = arith.divsi %25, %cst_26 : tensor<16xi32, #blocked2> loc(#loc40)
    %32 = tt.splat %13 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc41)
    %33 = tt.splat %13 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc41)
    %34 = tt.splat %13 : i32 -> tensor<16xi32, #blocked2> loc(#loc41)
    %35 = arith.addi %32, %29 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc41)
    %36 = arith.addi %33, %30 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc41)
    %37 = arith.addi %34, %31 : tensor<16xi32, #blocked2> loc(#loc41)
    %38 = tt.splat %6 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc42)
    %39 = tt.splat %6 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc42)
    %40 = tt.splat %6 : i32 -> tensor<16xi32, #blocked2> loc(#loc42)
    %41 = arith.addi %38, %35 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc42)
    %42 = arith.addi %39, %36 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc42)
    %43 = arith.addi %40, %37 : tensor<16xi32, #blocked2> loc(#loc42)
    %44 = arith.muli %1, %c4_i32 : i32 loc(#loc43)
    %45 = arith.remsi %22, %cst_27 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc44)
    %46 = arith.remsi %23, %cst : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc44)
    %47 = arith.remsi %25, %cst_26 : tensor<16xi32, #blocked2> loc(#loc44)
    %48 = tt.splat %44 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc45)
    %49 = tt.splat %44 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc45)
    %50 = tt.splat %44 : i32 -> tensor<16xi32, #blocked2> loc(#loc45)
    %51 = arith.addi %48, %45 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc45)
    %52 = arith.addi %49, %46 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc45)
    %53 = arith.addi %50, %47 : tensor<16xi32, #blocked2> loc(#loc45)
    %54 = tt.expand_dims %41 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi32, #blocked> loc(#loc46)
    %55 = tt.expand_dims %42 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xi32, #mma> loc(#loc46)
    %56 = arith.extsi %54 : tensor<16x1xi32, #blocked> to tensor<16x1xi64, #blocked> loc(#loc47)
    %57 = arith.extsi %55 : tensor<16x1xi32, #mma> to tensor<16x1xi64, #mma> loc(#loc47)
    %58 = tt.splat %arg13 : i64 -> tensor<16x1xi64, #blocked> loc(#loc47)
    %59 = arith.muli %56, %58 : tensor<16x1xi64, #blocked> loc(#loc47)
    %60 = tt.expand_dims %51 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi32, #blocked> loc(#loc48)
    %61 = tt.expand_dims %52 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xi32, #mma> loc(#loc48)
    %62 = arith.extsi %60 : tensor<16x1xi32, #blocked> to tensor<16x1xi64, #blocked> loc(#loc49)
    %63 = tt.splat %arg14 : i64 -> tensor<16x1xi64, #blocked> loc(#loc49)
    %64 = arith.muli %62, %63 : tensor<16x1xi64, #blocked> loc(#loc49)
    %65 = arith.addi %59, %64 : tensor<16x1xi64, #blocked> loc(#loc50)
    %66 = tt.expand_dims %26 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x128xi32, #blocked> loc(#loc51)
    %67 = tt.expand_dims %27 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x128xi32, #mma> loc(#loc51)
    %68 = arith.extsi %66 : tensor<1x128xi32, #blocked> to tensor<1x128xi64, #blocked> loc(#loc52)
    %69 = arith.extsi %67 : tensor<1x128xi32, #mma> to tensor<1x128xi64, #mma> loc(#loc52)
    %70 = tt.broadcast %65 : tensor<16x1xi64, #blocked> -> tensor<16x128xi64, #blocked> loc(#loc52)
    %71 = tt.broadcast %68 : tensor<1x128xi64, #blocked> -> tensor<16x128xi64, #blocked> loc(#loc52)
    %72 = tt.broadcast %69 : tensor<1x128xi64, #mma> -> tensor<16x128xi64, #mma> loc(#loc52)
    %73 = arith.addi %70, %71 : tensor<16x128xi64, #blocked> loc(#loc52)
    %74 = arith.cmpi slt, %26, %cst_12 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc53)
    %75 = arith.cmpi slt, %27, %cst_28 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc53)
    %76 = arith.cmpi slt, %28, %cst_11 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc53)
    %77 = arith.select %74, %cst_10, %cst_8 : tensor<128xi1, #ttg.slice<{dim = 0, parent = #blocked}>>, tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc54)
    %78 = arith.select %75, %cst_29, %cst_30 : tensor<128xi1, #ttg.slice<{dim = 0, parent = #mma}>>, tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc54)
    %79 = arith.select %76, %cst_9, %cst_7 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked1}>>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc54)
    %80 = arith.cmpi ne, %77, %cst_8 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc55)
    %81 = arith.cmpi ne, %78, %cst_30 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc55)
    %82 = arith.cmpi ne, %79, %cst_7 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc55)
    %83 = tt.splat %12 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc56)
    %84 = tt.splat %12 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc56)
    %85 = tt.splat %12 : i32 -> tensor<16xi32, #blocked2> loc(#loc56)
    %86 = arith.cmpi slt, %35, %83 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc56)
    %87 = arith.cmpi slt, %36, %84 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc56)
    %88 = arith.cmpi slt, %37, %85 : tensor<16xi32, #blocked2> loc(#loc56)
    %89 = arith.select %86, %cst_6, %cst_5 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc57)
    %90 = arith.select %87, %cst_0, %cst_1 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc57)
    %91 = arith.select %88, %cst_23, %cst_22 : tensor<16xi1, #blocked2>, tensor<16xi32, #blocked2> loc(#loc57)
    %92 = arith.cmpi ne, %89, %cst_5 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc58)
    %93 = arith.cmpi ne, %90, %cst_1 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc58)
    %94 = arith.cmpi ne, %91, %cst_22 : tensor<16xi32, #blocked2> loc(#loc58)
    %95 = arith.cmpi slt, %51, %cst_4 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc59)
    %96 = arith.cmpi slt, %52, %cst_2 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc59)
    %97 = arith.cmpi slt, %53, %cst_21 : tensor<16xi32, #blocked2> loc(#loc59)
    %98 = arith.select %95, %cst_6, %cst_5 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc60)
    %99 = arith.select %96, %cst_0, %cst_1 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc60)
    %100 = arith.select %97, %cst_23, %cst_22 : tensor<16xi1, #blocked2>, tensor<16xi32, #blocked2> loc(#loc60)
    %101 = arith.cmpi ne, %98, %cst_5 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc61)
    %102 = arith.cmpi ne, %99, %cst_1 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc61)
    %103 = arith.cmpi ne, %100, %cst_22 : tensor<16xi32, #blocked2> loc(#loc61)
    %104 = tt.expand_dims %80 {axis = 0 : i32} : tensor<128xi1, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x128xi1, #blocked> loc(#loc62)
    %105 = tt.expand_dims %81 {axis = 0 : i32} : tensor<128xi1, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x128xi1, #mma> loc(#loc62)
    %106 = tt.expand_dims %92 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi1, #blocked> loc(#loc63)
    %107 = tt.expand_dims %93 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xi1, #mma> loc(#loc63)
    %108 = tt.broadcast %104 : tensor<1x128xi1, #blocked> -> tensor<16x128xi1, #blocked> loc(#loc64)
    %109 = tt.broadcast %105 : tensor<1x128xi1, #mma> -> tensor<16x128xi1, #mma> loc(#loc64)
    %110 = tt.broadcast %106 : tensor<16x1xi1, #blocked> -> tensor<16x128xi1, #blocked> loc(#loc64)
    %111 = tt.broadcast %107 : tensor<16x1xi1, #mma> -> tensor<16x128xi1, #mma> loc(#loc64)
    %112 = arith.andi %108, %110 : tensor<16x128xi1, #blocked> loc(#loc64)
    %113 = arith.andi %109, %111 : tensor<16x128xi1, #mma> loc(#loc64)
    %114 = tt.expand_dims %101 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi1, #blocked> loc(#loc65)
    %115 = tt.expand_dims %102 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xi1, #mma> loc(#loc65)
    %116 = tt.broadcast %114 : tensor<16x1xi1, #blocked> -> tensor<16x128xi1, #blocked> loc(#loc66)
    %117 = tt.broadcast %115 : tensor<16x1xi1, #mma> -> tensor<16x128xi1, #mma> loc(#loc66)
    %118 = arith.andi %112, %116 : tensor<16x128xi1, #blocked> loc(#loc66)
    %119 = arith.andi %113, %117 : tensor<16x128xi1, #mma> loc(#loc66)
    %120 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x128x!tt.ptr<bf16>, #blocked> loc(#loc67)
    %121 = tt.addptr %120, %73 : tensor<16x128x!tt.ptr<bf16>, #blocked>, tensor<16x128xi64, #blocked> loc(#loc67)
    %122 = tt.load %121, %118, %cst_25 : tensor<16x128x!tt.ptr<bf16>, #blocked> loc(#loc68)
    %123 = ttg.local_alloc %122 : (tensor<16x128xbf16, #blocked>) -> !ttg.memdesc<16x128xbf16, #shared, #smem> loc(#loc68)
    %124 = ttg.local_load %123 : !ttg.memdesc<16x128xbf16, #shared, #smem> -> tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc68)
    %125 = arith.extsi %4 : i32 to i64 loc(#loc69)
    %126 = arith.muli %125, %arg12 : i64 loc(#loc69)
    %127 = arith.subi %16, %12 : i32 loc(#loc70)
    %128 = arith.addi %16, %c15_i32 : i32 loc(#loc167)
    %129 = arith.divsi %128, %c16_i32 : i32 loc(#loc168)
    %130 = arith.addi %2, %c1_i32 : i32 loc(#loc72)
    %131 = arith.muli %130, %18 : i32 loc(#loc73)
    %132 = arith.minsi %131, %129 : i32 loc(#loc74)
    %133 = tt.addptr %arg6, %126 : !tt.ptr<i32>, i64 loc(#loc75)
    %134 = arith.extsi %1 : i32 to i64 loc(#loc76)
    %135 = arith.muli %134, %arg21 : i64 loc(#loc76)
    %136 = tt.expand_dims %22 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi32, #blocked> loc(#loc77)
    %137 = arith.extsi %136 : tensor<16x1xi32, #blocked> to tensor<16x1xi64, #blocked> loc(#loc78)
    %138 = tt.splat %arg20 : i64 -> tensor<16x1xi64, #blocked> loc(#loc78)
    %139 = arith.muli %137, %138 : tensor<16x1xi64, #blocked> loc(#loc78)
    %140 = tt.broadcast %139 : tensor<16x1xi64, #blocked> -> tensor<16x128xi64, #blocked> loc(#loc79)
    %141 = arith.muli %134, %arg18 : i64 loc(#loc80)
    %142 = tt.expand_dims %28 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc81)
    %143 = arith.extsi %142 : tensor<128x1xi32, #blocked1> to tensor<128x1xi64, #blocked1> loc(#loc82)
    %144 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc83)
    %145 = tt.expand_dims %144 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x16xi32, #blocked1> loc(#loc83)
    %146 = arith.extsi %145 : tensor<1x16xi32, #blocked1> to tensor<1x16xi64, #blocked1> loc(#loc84)
    %147 = tt.splat %arg17 : i64 -> tensor<1x16xi64, #blocked1> loc(#loc84)
    %148 = arith.muli %146, %147 : tensor<1x16xi64, #blocked1> loc(#loc84)
    %149 = tt.broadcast %148 : tensor<1x16xi64, #blocked1> -> tensor<128x16xi64, #blocked1> loc(#loc85)
    %150 = tt.expand_dims %82 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi1, #blocked1> loc(#loc86)
    %151 = tt.splat %arg4 : !tt.ptr<bf16> -> tensor<128x16x!tt.ptr<bf16>, #blocked1> loc(#loc87)
    %152 = tt.broadcast %150 : tensor<128x1xi1, #blocked1> -> tensor<128x16xi1, #blocked1> loc(#loc88)
    %153 = tt.splat %arg5 : !tt.ptr<bf16> -> tensor<16x128x!tt.ptr<bf16>, #blocked> loc(#loc89)
    %154 = tt.expand_dims %36 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xi32, #mma> loc(#loc90)
    %155 = tt.splat %127 : i32 -> tensor<16x1xi32, #mma> loc(#loc91)
    %156 = arith.addi %155, %154 : tensor<16x1xi32, #mma> loc(#loc91)
    %157 = arith.addi %156, %cst_3 : tensor<16x1xi32, #mma> loc(#loc92)
    %158 = tt.broadcast %157 : tensor<16x1xi32, #mma> -> tensor<16x16xi32, #mma> loc(#loc93)
    %159 = tt.splat %arg8 : f32 -> tensor<16x16xf32, #mma> loc(#loc94)
    %160 = arith.andi %115, %107 : tensor<16x1xi1, #mma> loc(#loc95)
    %161 = tt.broadcast %160 : tensor<16x1xi1, #mma> -> tensor<16x16xi1, #mma> loc(#loc96)
    %162 = ttg.local_alloc  : () -> !ttg.memdesc<1x128x16xbf16, #shared1, #smem, mutable> loc(#loc88)
    %163 = ttg.local_alloc  : () -> !ttg.memdesc<1x16x128xbf16, #shared2, #smem, mutable> loc(#loc97)
    %164 = arith.cmpi slt, %19, %132 : i32 loc(#loc98)
    %165 = tt.addptr %133, %19 : !tt.ptr<i32>, i32 loc(#loc99)
    %166 = tt.load %165, %164 : !tt.ptr<i32> loc(#loc100)
    %167 = arith.extsi %166 : i32 to i64 loc(#loc101)
    %168 = arith.muli %167, %arg19 : i64 loc(#loc101)
    %169 = arith.addi %168, %135 : i64 loc(#loc102)
    %170 = tt.splat %169 : i64 -> tensor<1x128xi64, #blocked> loc(#loc103)
    %171 = arith.addi %170, %68 : tensor<1x128xi64, #blocked> loc(#loc103)
    %172 = tt.broadcast %171 : tensor<1x128xi64, #blocked> -> tensor<16x128xi64, #blocked> loc(#loc79)
    %173 = arith.addi %172, %140 : tensor<16x128xi64, #blocked> loc(#loc79)
    %174 = arith.muli %167, %arg16 : i64 loc(#loc104)
    %175 = arith.addi %174, %141 : i64 loc(#loc105)
    %176 = tt.splat %175 : i64 -> tensor<128x1xi64, #blocked1> loc(#loc82)
    %177 = arith.addi %176, %143 : tensor<128x1xi64, #blocked1> loc(#loc82)
    %178 = tt.broadcast %177 : tensor<128x1xi64, #blocked1> -> tensor<128x16xi64, #blocked1> loc(#loc85)
    %179 = arith.addi %178, %149 : tensor<128x16xi64, #blocked1> loc(#loc85)
    %180 = tt.addptr %151, %179 : tensor<128x16x!tt.ptr<bf16>, #blocked1>, tensor<128x16xi64, #blocked1> loc(#loc87)
    %181 = tt.splat %164 : i1 -> tensor<128x16xi1, #blocked1> loc(#loc98)
    %182 = arith.andi %181, %152 : tensor<128x16xi1, #blocked1> loc(#loc98)
    %183 = tt.load %180, %182, %cst_24 : tensor<128x16x!tt.ptr<bf16>, #blocked1> loc(#loc88)
    %184 = tt.addptr %153, %173 : tensor<16x128x!tt.ptr<bf16>, #blocked>, tensor<16x128xi64, #blocked> loc(#loc89)
    %185 = tt.splat %164 : i1 -> tensor<16x128xi1, #blocked> loc(#loc98)
    %186 = arith.andi %185, %108 : tensor<16x128xi1, #blocked> loc(#loc98)
    %187 = tt.load %184, %186, %cst_25 : tensor<16x128x!tt.ptr<bf16>, #blocked> loc(#loc97)
    %188 = ttg.memdesc_subview %162[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x16xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable> loc(#loc88)
    ttg.local_store %183, %188 : tensor<128x16xbf16, #blocked1> -> !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable> loc(#loc88)
    %189 = ttg.memdesc_subview %163[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<1x16x128xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> loc(#loc97)
    ttg.local_store %187, %189 : tensor<16x128xbf16, #blocked> -> !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> loc(#loc97)
    %190 = arith.subi %132, %c1_i32 : i32 loc(#loc98)
    %191:6 = scf.for %arg23 = %19 to %190 step %c1_i32 iter_args(%arg24 = %cst_18, %arg25 = %cst_16, %arg26 = %cst_17, %arg27 = %c0_i32, %arg28 = %188, %arg29 = %189) -> (tensor<16x128xf32, #mma>, tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>>, i32, !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable>, !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable>)  : i32 {
      %260 = arith.addi %arg23, %c1_i32 : i32 loc(#loc98)
      %261 = tt.addptr %133, %260 : !tt.ptr<i32>, i32 loc(#loc99)
      %262 = tt.load %261 : !tt.ptr<i32> loc(#loc100)
      %263 = arith.extsi %262 : i32 to i64 loc(#loc101)
      %264 = arith.muli %263, %arg19 : i64 loc(#loc101)
      %265 = arith.addi %264, %135 : i64 loc(#loc102)
      %266 = tt.splat %265 : i64 -> tensor<1x128xi64, #blocked> loc(#loc103)
      %267 = arith.addi %266, %68 : tensor<1x128xi64, #blocked> loc(#loc103)
      %268 = tt.broadcast %267 : tensor<1x128xi64, #blocked> -> tensor<16x128xi64, #blocked> loc(#loc79)
      %269 = arith.addi %268, %140 : tensor<16x128xi64, #blocked> loc(#loc79)
      %270 = arith.muli %263, %arg16 : i64 loc(#loc104)
      %271 = arith.addi %270, %141 : i64 loc(#loc105)
      %272 = tt.splat %271 : i64 -> tensor<128x1xi64, #blocked1> loc(#loc82)
      %273 = arith.addi %272, %143 : tensor<128x1xi64, #blocked1> loc(#loc82)
      %274 = tt.broadcast %273 : tensor<128x1xi64, #blocked1> -> tensor<128x16xi64, #blocked1> loc(#loc85)
      %275 = arith.addi %274, %149 : tensor<128x16xi64, #blocked1> loc(#loc85)
      %276 = tt.addptr %151, %275 : tensor<128x16x!tt.ptr<bf16>, #blocked1>, tensor<128x16xi64, #blocked1> loc(#loc87)
      %277 = tt.load %276, %152, %cst_24 : tensor<128x16x!tt.ptr<bf16>, #blocked1> loc(#loc88)
      %278 = ttg.local_load %arg28 : !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable> -> tensor<128x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc88)
      %279 = tt.addptr %153, %269 : tensor<16x128x!tt.ptr<bf16>, #blocked>, tensor<16x128xi64, #blocked> loc(#loc89)
      %280 = tt.load %279, %108, %cst_25 : tensor<16x128x!tt.ptr<bf16>, #blocked> loc(#loc97)
      %281 = ttg.local_load %arg29 : !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> -> tensor<16x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc97)
      %282 = arith.muli %arg23, %c16_i32 : i32 loc(#loc106)
      %283 = tt.splat %282 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc107)
      %284 = arith.addi %283, %24 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc107)
      %285 = tt.expand_dims %284 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x16xi32, #mma> loc(#loc108)
      %286 = tt.broadcast %285 : tensor<1x16xi32, #mma> -> tensor<16x16xi32, #mma> loc(#loc93)
      %287 = arith.cmpi slt, %286, %158 : tensor<16x16xi32, #mma> loc(#loc93)
      %288 = tt.dot %124, %278, %cst_13 : tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<128x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x16xf32, #mma> loc(#loc109)
      %289 = arith.mulf %159, %288 : tensor<16x16xf32, #mma> loc(#loc94)
      %290 = arith.addf %289, %cst_13 : tensor<16x16xf32, #mma> loc(#loc110)
      %291 = arith.andi %161, %287 : tensor<16x16xi1, #mma> loc(#loc96)
      %292 = arith.select %291, %290, %cst_14 : tensor<16x16xi1, #mma>, tensor<16x16xf32, #mma> loc(#loc111)
      %293 = "tt.reduce"(%292) <{axis = 1 : i32}> ({
      ^bb0(%arg30: f32 loc(callsite(#loc1 at #loc113)), %arg31: f32 loc(callsite(#loc1 at #loc113))):
        %317 = arith.maxnumf %arg30, %arg31 : f32 loc(#loc175)
        tt.reduce.return %317 : f32 loc(#loc169)
      }) : (tensor<16x16xf32, #mma>) -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc169)
      %294 = arith.maxnumf %arg26, %293 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc115)
      %295 = arith.cmpf ogt, %294, %cst_17 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc116)
      %296 = arith.select %295, %294, %cst_15 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc117)
      %297 = tt.expand_dims %296 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xf32, #mma> loc(#loc118)
      %298 = tt.broadcast %297 : tensor<16x1xf32, #mma> -> tensor<16x16xf32, #mma> loc(#loc119)
      %299 = arith.subf %292, %298 : tensor<16x16xf32, #mma> loc(#loc119)
      %300 = math.exp %299 : tensor<16x16xf32, #mma> loc(#loc120)
      %301 = arith.subf %arg26, %296 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc121)
      %302 = math.exp %301 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc122)
      %303 = tt.expand_dims %302 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xf32, #mma> loc(#loc123)
      %304 = tt.broadcast %303 : tensor<16x1xf32, #mma> -> tensor<16x128xf32, #mma> loc(#loc124)
      %305 = arith.mulf %arg24, %304 : tensor<16x128xf32, #mma> loc(#loc124)
      %306 = arith.truncf %300 : tensor<16x16xf32, #mma> to tensor<16x16xbf16, #mma> loc(#loc125)
      %307 = ttg.convert_layout %306 : tensor<16x16xbf16, #mma> -> tensor<16x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc125)
      %308 = tt.dot %307, %281, %305 : tensor<16x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x128xf32, #mma> loc(#loc126)
      %309 = arith.addi %arg27, %c1_i32 : i32 loc(#loc98)
      %310 = arith.cmpi slt, %309, %c1_i32 : i32 loc(#loc98)
      %311 = arith.select %310, %309, %c0_i32 : i32 loc(#loc98)
      %312 = ttg.memdesc_subview %162[%311, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x16xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable> loc(#loc88)
      ttg.local_store %277, %312 : tensor<128x16xbf16, #blocked1> -> !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable> loc(#loc88)
      %313 = ttg.memdesc_subview %163[%311, %c0_i32, %c0_i32] : !ttg.memdesc<1x16x128xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> loc(#loc97)
      ttg.local_store %280, %313 : tensor<16x128xbf16, #blocked> -> !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> loc(#loc97)
      %314 = "tt.reduce"(%300) <{axis = 1 : i32}> ({
      ^bb0(%arg30: f32 loc(callsite(#loc1 at #loc128)), %arg31: f32 loc(callsite(#loc1 at #loc128))):
        %317 = arith.addf %arg30, %arg31 : f32 loc(#loc176)
        tt.reduce.return %317 : f32 loc(#loc172)
      }) : (tensor<16x16xf32, #mma>) -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc172)
      %315 = arith.mulf %arg25, %302 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc130)
      %316 = arith.addf %315, %314 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc131)
      scf.yield %308, %316, %296, %311, %312, %313 : tensor<16x128xf32, #mma>, tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>>, i32, !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable>, !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> loc(#loc98)
    } loc(#loc98)
    %192 = arith.subi %132, %19 : i32 loc(#loc98)
    %193 = arith.subi %192, %c1_i32 : i32 loc(#loc98)
    %194 = arith.maxsi %193, %c0_i32 : i32 loc(#loc98)
    %195 = arith.addi %19, %194 : i32 loc(#loc98)
    %196 = arith.cmpi sge, %192, %c1_i32 : i32 loc(#loc98)
    %197 = ttg.local_load %191#4 : !ttg.memdesc<128x16xbf16, #shared1, #smem, mutable> -> tensor<128x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc88)
    %198 = ttg.local_load %191#5 : !ttg.memdesc<16x128xbf16, #shared2, #smem, mutable> -> tensor<16x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc97)
    %199 = arith.muli %195, %c16_i32 : i32 loc(#loc106)
    %200 = tt.splat %199 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc107)
    %201 = arith.addi %200, %24 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc107)
    %202 = tt.expand_dims %201 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x16xi32, #mma> loc(#loc108)
    %203 = tt.broadcast %202 : tensor<1x16xi32, #mma> -> tensor<16x16xi32, #mma> loc(#loc93)
    %204 = arith.cmpi slt, %203, %158 : tensor<16x16xi32, #mma> loc(#loc93)
    %205 = ttg.local_load %123 : !ttg.memdesc<16x128xbf16, #shared, #smem> -> tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc68)
    %206 = scf.if %196 -> (tensor<16x16xf32, #mma>) {
      %260 = tt.dot %205, %197, %cst_13 : tensor<16x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<128x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x16xf32, #mma> loc(#loc109)
      scf.yield %260 : tensor<16x16xf32, #mma> loc(#loc109)
    } else {
      scf.yield %cst_13 : tensor<16x16xf32, #mma> loc(#loc109)
    } loc(#loc109)
    %207 = arith.mulf %159, %206 : tensor<16x16xf32, #mma> loc(#loc94)
    %208 = arith.addf %207, %cst_13 : tensor<16x16xf32, #mma> loc(#loc110)
    %209 = arith.andi %161, %204 : tensor<16x16xi1, #mma> loc(#loc96)
    %210 = arith.select %209, %208, %cst_14 : tensor<16x16xi1, #mma>, tensor<16x16xf32, #mma> loc(#loc111)
    %211 = "tt.reduce"(%210) <{axis = 1 : i32}> ({
    ^bb0(%arg23: f32 loc(callsite(#loc1 at #loc113)), %arg24: f32 loc(callsite(#loc1 at #loc113))):
      %260 = arith.maxnumf %arg23, %arg24 : f32 loc(#loc175)
      tt.reduce.return %260 : f32 loc(#loc169)
    }) : (tensor<16x16xf32, #mma>) -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc169)
    %212 = arith.maxnumf %191#2, %211 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc115)
    %213 = arith.cmpf ogt, %212, %cst_17 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc116)
    %214 = arith.select %213, %212, %cst_15 : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc117)
    %215 = tt.expand_dims %214 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xf32, #mma> loc(#loc118)
    %216 = tt.broadcast %215 : tensor<16x1xf32, #mma> -> tensor<16x16xf32, #mma> loc(#loc119)
    %217 = arith.subf %210, %216 : tensor<16x16xf32, #mma> loc(#loc119)
    %218 = math.exp %217 : tensor<16x16xf32, #mma> loc(#loc120)
    %219 = arith.subf %191#2, %214 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc121)
    %220 = math.exp %219 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc122)
    %221 = tt.expand_dims %220 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16x1xf32, #mma> loc(#loc123)
    %222 = tt.broadcast %221 : tensor<16x1xf32, #mma> -> tensor<16x128xf32, #mma> loc(#loc124)
    %223 = arith.mulf %191#0, %222 : tensor<16x128xf32, #mma> loc(#loc124)
    %224 = arith.truncf %218 : tensor<16x16xf32, #mma> to tensor<16x16xbf16, #mma> loc(#loc125)
    %225 = ttg.convert_layout %224 : tensor<16x16xbf16, #mma> -> tensor<16x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc125)
    %226 = scf.if %196 -> (tensor<16x128xf32, #mma>) {
      %260 = tt.dot %225, %198, %223 : tensor<16x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x128xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x128xf32, #mma> loc(#loc126)
      scf.yield %260 : tensor<16x128xf32, #mma> loc(#loc126)
    } else {
      scf.yield %223 : tensor<16x128xf32, #mma> loc(#loc126)
    } loc(#loc126)
    %227 = "tt.reduce"(%218) <{axis = 1 : i32}> ({
    ^bb0(%arg23: f32 loc(callsite(#loc1 at #loc128)), %arg24: f32 loc(callsite(#loc1 at #loc128))):
      %260 = arith.addf %arg23, %arg24 : f32 loc(#loc176)
      tt.reduce.return %260 : f32 loc(#loc172)
    }) : (tensor<16x16xf32, #mma>) -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc172)
    %228 = arith.mulf %191#1, %220 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc130)
    %229 = arith.addf %228, %227 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc131)
    %230 = arith.select %196, %226, %191#0 : tensor<16x128xf32, #mma> loc(#loc98)
    %231 = arith.select %196, %229, %191#1 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc98)
    %232 = arith.select %196, %214, %191#2 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc98)
    ttg.local_dealloc %162 : !ttg.memdesc<1x128x16xbf16, #shared1, #smem, mutable> loc(#loc98)
    ttg.local_dealloc %163 : !ttg.memdesc<1x16x128xbf16, #shared2, #smem, mutable> loc(#loc98)
    %233 = arith.muli %57, %cst_31 : tensor<16x1xi64, #mma> loc(#loc132)
    %234 = arith.muli %61, %cst_32 : tensor<16x1xi32, #mma> loc(#loc133)
    %235 = arith.extsi %234 : tensor<16x1xi32, #mma> to tensor<16x1xi64, #mma> loc(#loc134)
    %236 = arith.addi %233, %235 : tensor<16x1xi64, #mma> loc(#loc134)
    %237 = arith.muli %2, %c128_i32 : i32 loc(#loc135)
    %238 = arith.extsi %237 : i32 to i64 loc(#loc136)
    %239 = tt.splat %238 : i64 -> tensor<16x1xi64, #mma> loc(#loc136)
    %240 = arith.addi %236, %239 : tensor<16x1xi64, #mma> loc(#loc136)
    %241 = tt.broadcast %240 : tensor<16x1xi64, #mma> -> tensor<16x128xi64, #mma> loc(#loc137)
    %242 = arith.addi %241, %72 : tensor<16x128xi64, #mma> loc(#loc137)
    %243 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<16x128x!tt.ptr<f32>, #mma> loc(#loc138)
    %244 = tt.addptr %243, %242 : tensor<16x128x!tt.ptr<f32>, #mma>, tensor<16x128xi64, #mma> loc(#loc138)
    tt.store %244, %230, %119 : tensor<16x128x!tt.ptr<f32>, #mma> loc(#loc139)
    %245 = arith.extsi %43 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2> loc(#loc140)
    %246 = arith.muli %245, %cst_19 : tensor<16xi64, #blocked2> loc(#loc141)
    %247 = arith.muli %53, %cst_20 : tensor<16xi32, #blocked2> loc(#loc142)
    %248 = arith.extsi %247 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2> loc(#loc143)
    %249 = arith.addi %246, %248 : tensor<16xi64, #blocked2> loc(#loc143)
    %250 = arith.extsi %2 : i32 to i64 loc(#loc144)
    %251 = tt.splat %250 : i64 -> tensor<16xi64, #blocked2> loc(#loc144)
    %252 = arith.addi %249, %251 : tensor<16xi64, #blocked2> loc(#loc144)
    %253 = arith.andi %94, %103 : tensor<16xi1, #blocked2> loc(#loc145)
    %254 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2> loc(#loc146)
    %255 = tt.addptr %254, %252 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi64, #blocked2> loc(#loc146)
    %256 = ttg.convert_layout %232 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16xf32, #blocked2> loc(#loc147)
    tt.store %255, %256, %253 : tensor<16x!tt.ptr<f32>, #blocked2> loc(#loc147)
    %257 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2> loc(#loc148)
    %258 = tt.addptr %257, %252 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi64, #blocked2> loc(#loc148)
    %259 = ttg.convert_layout %231 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<16xf32, #blocked2> loc(#loc149)
    tt.store %258, %259, %253 : tensor<16x!tt.ptr<f32>, #blocked2> loc(#loc149)
    tt.return loc(#loc150)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":339:39)
#loc3 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":340:32)
#loc4 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":341:29)
#loc5 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":38:4)
#loc7 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":38:17)
#loc8 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":39:22)
#loc9 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":39:32)
#loc10 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":40:44)
#loc11 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":40:22)
#loc12 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":41:25)
#loc13 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":41:35)
#loc14 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":43:22)
#loc15 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":43:11)
#loc16 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":44:25)
#loc17 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":43:8)
#loc18 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":48:18)
#loc19 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":347:32)
#loc20 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":346:32)
#loc21 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":347:44)
#loc22 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":347:54)
#loc23 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":349:45)
#loc24 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":352:74)
#loc25 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":352:42)
#loc26 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":355:10)
#loc27 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":357:27)
#loc28 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":357:38)
#loc29 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":358:8)
#loc30 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":361:37)
#loc31 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":361:22)
#loc32 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":22:20)
#loc33 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":365:42)
#loc34 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":22:26)
#loc35 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":367:18)
#loc36 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":367:39)
#loc37 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":367:53)
#loc38 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":370:26)
#loc39 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":371:26)
#loc40 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":373:56)
#loc41 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":373:46)
#loc42 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":375:52)
#loc43 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":376:35)
#loc44 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":377:17)
#loc45 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":377:8)
#loc46 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":379:35)
#loc47 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":379:46)
#loc48 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":380:35)
#loc49 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":380:46)
#loc50 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":380:20)
#loc51 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":380:70)
#loc52 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":380:63)
#loc53 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":382:33)
#loc54 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":382:47)
#loc55 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":382:53)
#loc56 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":383:40)
#loc57 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":383:64)
#loc58 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":383:70)
#loc59 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":384:45)
#loc60 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":384:65)
#loc61 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":384:71)
#loc62 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":389:22)
#loc63 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":389:46)
#loc64 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":389:33)
#loc65 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":389:70)
#loc66 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":389:57)
#loc67 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":388:20)
#loc68 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":388:8)
#loc69 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":393:35)
#loc70 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":400:28)
#loc71 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":413:34)
#loc72 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":418:28)
#loc73 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":418:33)
#loc74 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":418:53)
#loc75 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":420:56)
#loc76 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":425:34)
#loc77 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":427:27)
#loc78 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":427:38)
#loc79 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":427:20)
#loc80 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":430:34)
#loc81 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":431:27)
#loc82 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":431:20)
#loc83 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":432:27)
#loc84 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":432:38)
#loc85 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":432:20)
#loc86 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":436:39)
#loc87 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":435:41)
#loc88 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":435:25)
#loc89 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":448:43)
#loc90 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":462:65)
#loc91 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":462:55)
#loc92 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":462:76)
#loc93 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":462:41)
#loc94 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":467:21)
#loc95 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":472:45)
#loc96 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":472:69)
#loc97 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":448:25)
#loc98 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":418:12)
#loc99 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":420:77)
#loc100 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":420:37)
#loc101 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":424:41)
#loc102 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":425:20)
#loc103 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":426:20)
#loc104 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":429:41)
#loc105 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":430:20)
#loc106 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":460:25)
#loc107 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":460:38)
#loc108 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":462:30)
#loc109 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":467:31)
#loc110 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":467:13)
#loc111 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":473:24)
#loc112 = loc("/usr/local/lib/python3.12/dist-packages/triton/language/standard.py":184:40)
#loc114 = loc("/usr/local/lib/python3.12/dist-packages/triton/language/standard.py":163:27)
#loc115 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":496:28)
#loc116 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":499:29)
#loc117 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":499:49)
#loc118 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":502:27)
#loc119 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":502:23)
#loc120 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":502:19)
#loc121 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":508:27)
#loc122 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":508:23)
#loc123 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":511:26)
#loc124 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":511:20)
#loc125 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":518:27)
#loc126 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":518:37)
#loc127 = loc("/usr/local/lib/python3.12/dist-packages/triton/language/standard.py":267:36)
#loc129 = loc("/usr/local/lib/python3.12/dist-packages/triton/language/standard.py":256:15)
#loc130 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":514:16)
#loc131 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":514:24)
#loc132 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":522:9)
#loc133 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":523:35)
#loc134 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":523:8)
#loc135 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":524:19)
#loc136 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":524:8)
#loc137 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":524:38)
#loc138 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":526:26)
#loc139 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":527:8)
#loc140 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":530:37)
#loc141 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":531:20)
#loc142 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":532:36)
#loc143 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":532:19)
#loc144 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":532:59)
#loc145 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":533:64)
#loc146 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":533:28)
#loc147 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":533:41)
#loc148 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":534:31)
#loc149 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":535:13)
#loc150 = loc("/usr/local/lib/python3.12/dist-packages/vllm/attention/ops/triton_unified_attention.py":534:4)
#loc151 = loc(callsite(#loc5 at #loc6))
#loc152 = loc(callsite(#loc7 at #loc6))
#loc154 = loc(callsite(#loc8 at #loc6))
#loc155 = loc(callsite(#loc9 at #loc6))
#loc156 = loc(callsite(#loc10 at #loc6))
#loc157 = loc(callsite(#loc11 at #loc6))
#loc158 = loc(callsite(#loc12 at #loc6))
#loc159 = loc(callsite(#loc13 at #loc6))
#loc160 = loc(callsite(#loc14 at #loc6))
#loc161 = loc(callsite(#loc15 at #loc6))
#loc162 = loc(callsite(#loc16 at #loc6))
#loc163 = loc(callsite(#loc17 at #loc6))
#loc164 = loc(callsite(#loc18 at #loc6))
#loc165 = loc(callsite(#loc32 at #loc33))
#loc166 = loc(callsite(#loc34 at #loc33))
#loc167 = loc(callsite(#loc32 at #loc71))
#loc168 = loc(callsite(#loc34 at #loc71))
#loc169 = loc(callsite(#loc112 at #loc113))
#loc171 = loc(callsite(#loc114 at #loc112))
#loc172 = loc(callsite(#loc127 at #loc128))
#loc174 = loc(callsite(#loc129 at #loc127))
#loc175 = loc(callsite(#loc171 at #loc113))
#loc176 = loc(callsite(#loc174 at #loc128))

from __future__ import annotations
import torch
class GraphModule(torch.nn.Module):
    def forward(self, s0: "Sym(s0)", L_input_ids_: "i32[s0]", L_self_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096]", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", s1: "Sym(s0)", L_positions_: "i64[s0]", L_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]", L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", L_self_modules_norm_parameters_weight_: "bf16[4096]"):
        l_input_ids_ = L_input_ids_
        l_self_modules_embed_tokens_parameters_weight_ = L_self_modules_embed_tokens_parameters_weight_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_positions_ = L_positions_
        l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = L_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_
        l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_norm_parameters_weight_ = L_self_modules_norm_parameters_weight_
        
        # No stacktrace found for following nodes
        submod_0 = self.submod_0(l_input_ids_, s0, l_self_modules_embed_tokens_parameters_weight_, l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_norm_parameters_weight_);  l_input_ids_ = s0 = l_self_modules_embed_tokens_parameters_weight_ = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_positions_ = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_norm_parameters_weight_ = None
        return (submod_0,)
        
    class submod_0(torch.nn.Module):
        def forward(self, l_input_ids_: "i32[s0]", s0: "Sym(s0)", l_self_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096]", l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_positions_: "i64[s0]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]", l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[6144, 4096]", l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[4096, 4096]", l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_: "bf16[4096]", l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[28672, 4096]", l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_: "bf16[4096, 14336]", l_self_modules_norm_parameters_weight_: "bf16[4096]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py:407 in forward, code: masked_input.long())
            long: "i64[s0]" = l_input_ids_.long();  l_input_ids_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py:50 in embedding, code: return F.embedding(input_, layer.weight)
            embedding: "bf16[s0, 4096]" = torch.nn.functional.embedding(long, l_self_modules_embed_tokens_parameters_weight_);  long = l_self_modules_embed_tokens_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
            _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:175 in forward_cuda, code: return norm_func(x, self.weight.data, self.variance_epsilon)
            _get_data_attr: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:23 in rms_norm, code: out = torch.empty_like(x)
            empty_like: "bf16[s0, 4096]" = torch.empty_like(embedding)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:274 in rms_norm, code: input_contiguous = input.contiguous()
            contiguous: "bf16[s0, 4096]" = embedding.contiguous()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:275 in rms_norm, code: torch.ops._C.rms_norm(out, input_contiguous, weight, epsilon)
            rms_norm = torch.ops._C.rms_norm(empty_like, contiguous, _get_data_attr, 1e-05);  contiguous = _get_data_attr = rms_norm = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = rocm_unquantized_gemm_impl.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl = None
            getitem: "bf16[s0, 4096]" = split[0]
            getitem_1: "bf16[s0, 1024]" = split[1]
            getitem_2: "bf16[s0, 1024]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_3: "bf16[s0, 64]" = chunk[0]
            getitem_4: "bf16[s0, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size = getitem.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view: "bf16[s0, 32, 128]" = getitem.view(s0, -1, 128);  getitem = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_5: "bf16[s0, 32, 128]" = view[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_6: "bf16[s0, 32, 0]" = view[(Ellipsis, slice(128, None, None))];  view = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s0, 1, 64]" = getitem_3.unsqueeze(-2)
            to: "bf16[s0, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s0, 1, 64]" = getitem_4.unsqueeze(-2)
            to_1: "bf16[s0, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_5, 2, dim = -1);  getitem_5 = None
            getitem_7: "bf16[s0, 32, 64]" = chunk_1[0]
            getitem_8: "bf16[s0, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul: "bf16[s0, 32, 64]" = getitem_7 * to
            mul_1: "bf16[s0, 32, 64]" = getitem_8 * to_1
            sub: "bf16[s0, 32, 64]" = mul - mul_1;  mul = mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_2: "bf16[s0, 32, 64]" = getitem_8 * to;  getitem_8 = to = None
            mul_3: "bf16[s0, 32, 64]" = getitem_7 * to_1;  getitem_7 = to_1 = None
            add: "bf16[s0, 32, 64]" = mul_2 + mul_3;  mul_2 = mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat: "bf16[s0, 32, 128]" = torch.cat((sub, add), dim = -1);  sub = add = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s0, 32, 128]" = torch.cat((cat, getitem_6), dim = -1);  cat = getitem_6 = None
            reshape: "bf16[s0, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_1 = getitem_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_1: "bf16[s0, 8, 128]" = getitem_1.view(s0, -1, 128);  getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_9: "bf16[s0, 8, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_10: "bf16[s0, 8, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s0, 1, 64]" = getitem_3.unsqueeze(-2);  getitem_3 = None
            to_2: "bf16[s0, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s0, 1, 64]" = getitem_4.unsqueeze(-2);  getitem_4 = None
            to_3: "bf16[s0, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_9, 2, dim = -1);  getitem_9 = None
            getitem_11: "bf16[s0, 8, 64]" = chunk_2[0]
            getitem_12: "bf16[s0, 8, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_4: "bf16[s0, 8, 64]" = getitem_11 * to_2
            mul_5: "bf16[s0, 8, 64]" = getitem_12 * to_3
            sub_1: "bf16[s0, 8, 64]" = mul_4 - mul_5;  mul_4 = mul_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_6: "bf16[s0, 8, 64]" = getitem_12 * to_2;  getitem_12 = to_2 = None
            mul_7: "bf16[s0, 8, 64]" = getitem_11 * to_3;  getitem_11 = to_3 = None
            add_1: "bf16[s0, 8, 64]" = mul_6 + mul_7;  mul_6 = mul_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s0, 8, 128]" = torch.cat((sub_1, add_1), dim = -1);  sub_1 = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s0, 8, 128]" = torch.cat((cat_2, getitem_10), dim = -1);  cat_2 = getitem_10 = None
            reshape_1: "bf16[s0, 1024]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_2 = reshape.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros: "bf16[s0, 4096]" = torch.zeros(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_2: "bf16[s0, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s0, 32, 128]" = zeros.view(-1, 32, 128);  zeros = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_4: "bf16[s0, 8, 128]" = reshape_1.view(-1, 8, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s0, 8, 128]" = getitem_2.view(-1, 8, 128);  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(view_2, view_4, view_5, view_3, 'model.layers.0.self_attn.attn');  view_2 = view_4 = view_5 = unified_attention_with_output = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_6: "bf16[s0, 4096]" = view_3.view(-1, 4096);  view_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_1: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_6, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_6 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_1: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_1: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_1)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_2: "bf16[s0, 4096]" = torch.empty_like(embedding)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm = torch.ops._C.fused_add_rms_norm(empty_like_1, rocm_unquantized_gemm_impl_1, empty_like_2, embedding, _get_data_attr_1, 1e-05);  rocm_unquantized_gemm_impl_1 = embedding = _get_data_attr_1 = fused_add_rms_norm = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_2: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_1, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_1 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul = torch.ops._C.silu_and_mul(empty, rocm_unquantized_gemm_impl_2);  rocm_unquantized_gemm_impl_2 = silu_and_mul = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_3: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, None);  empty = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_2: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_3: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_3)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_4: "bf16[s0, 4096]" = torch.empty_like(empty_like_2)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_1 = torch.ops._C.fused_add_rms_norm(empty_like_3, rocm_unquantized_gemm_impl_3, empty_like_4, empty_like_2, _get_data_attr_2, 1e-05);  rocm_unquantized_gemm_impl_3 = empty_like_2 = _get_data_attr_2 = fused_add_rms_norm_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_4: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_3, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_3 = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_1 = rocm_unquantized_gemm_impl_4.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_4 = None
            getitem_13: "bf16[s0, 4096]" = split_1[0]
            getitem_14: "bf16[s0, 1024]" = split_1[1]
            getitem_15: "bf16[s0, 1024]" = split_1[2];  split_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_1: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_1: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_1);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_3 = index_select_1.chunk(2, dim = -1);  index_select_1 = None
            getitem_16: "bf16[s0, 64]" = chunk_3[0]
            getitem_17: "bf16[s0, 64]" = chunk_3[1];  chunk_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_3 = getitem_13.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_7: "bf16[s0, 32, 128]" = getitem_13.view(s0, -1, 128);  getitem_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_18: "bf16[s0, 32, 128]" = view_7[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_19: "bf16[s0, 32, 0]" = view_7[(Ellipsis, slice(128, None, None))];  view_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_4: "bf16[s0, 1, 64]" = getitem_16.unsqueeze(-2)
            to_4: "bf16[s0, 1, 64]" = unsqueeze_4.to(torch.bfloat16);  unsqueeze_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_5: "bf16[s0, 1, 64]" = getitem_17.unsqueeze(-2)
            to_5: "bf16[s0, 1, 64]" = unsqueeze_5.to(torch.bfloat16);  unsqueeze_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_4 = torch.chunk(getitem_18, 2, dim = -1);  getitem_18 = None
            getitem_20: "bf16[s0, 32, 64]" = chunk_4[0]
            getitem_21: "bf16[s0, 32, 64]" = chunk_4[1];  chunk_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_8: "bf16[s0, 32, 64]" = getitem_20 * to_4
            mul_9: "bf16[s0, 32, 64]" = getitem_21 * to_5
            sub_2: "bf16[s0, 32, 64]" = mul_8 - mul_9;  mul_8 = mul_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_10: "bf16[s0, 32, 64]" = getitem_21 * to_4;  getitem_21 = to_4 = None
            mul_11: "bf16[s0, 32, 64]" = getitem_20 * to_5;  getitem_20 = to_5 = None
            add_2: "bf16[s0, 32, 64]" = mul_10 + mul_11;  mul_10 = mul_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_4: "bf16[s0, 32, 128]" = torch.cat((sub_2, add_2), dim = -1);  sub_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_5: "bf16[s0, 32, 128]" = torch.cat((cat_4, getitem_19), dim = -1);  cat_4 = getitem_19 = None
            reshape_2: "bf16[s0, 4096]" = cat_5.reshape(size_3);  cat_5 = size_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_4 = getitem_14.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_8: "bf16[s0, 8, 128]" = getitem_14.view(s0, -1, 128);  getitem_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_22: "bf16[s0, 8, 128]" = view_8[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_23: "bf16[s0, 8, 0]" = view_8[(Ellipsis, slice(128, None, None))];  view_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_6: "bf16[s0, 1, 64]" = getitem_16.unsqueeze(-2);  getitem_16 = None
            to_6: "bf16[s0, 1, 64]" = unsqueeze_6.to(torch.bfloat16);  unsqueeze_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_7: "bf16[s0, 1, 64]" = getitem_17.unsqueeze(-2);  getitem_17 = None
            to_7: "bf16[s0, 1, 64]" = unsqueeze_7.to(torch.bfloat16);  unsqueeze_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_5 = torch.chunk(getitem_22, 2, dim = -1);  getitem_22 = None
            getitem_24: "bf16[s0, 8, 64]" = chunk_5[0]
            getitem_25: "bf16[s0, 8, 64]" = chunk_5[1];  chunk_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_12: "bf16[s0, 8, 64]" = getitem_24 * to_6
            mul_13: "bf16[s0, 8, 64]" = getitem_25 * to_7
            sub_3: "bf16[s0, 8, 64]" = mul_12 - mul_13;  mul_12 = mul_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_14: "bf16[s0, 8, 64]" = getitem_25 * to_6;  getitem_25 = to_6 = None
            mul_15: "bf16[s0, 8, 64]" = getitem_24 * to_7;  getitem_24 = to_7 = None
            add_3: "bf16[s0, 8, 64]" = mul_14 + mul_15;  mul_14 = mul_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_6: "bf16[s0, 8, 128]" = torch.cat((sub_3, add_3), dim = -1);  sub_3 = add_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_7: "bf16[s0, 8, 128]" = torch.cat((cat_6, getitem_23), dim = -1);  cat_6 = getitem_23 = None
            reshape_3: "bf16[s0, 1024]" = cat_7.reshape(size_4);  cat_7 = size_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_5 = reshape_2.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_1: "bf16[s0, 4096]" = torch.zeros(size_5, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_9: "bf16[s0, 32, 128]" = reshape_2.view(-1, 32, 128);  reshape_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_10: "bf16[s0, 32, 128]" = zeros_1.view(-1, 32, 128);  zeros_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_11: "bf16[s0, 8, 128]" = reshape_3.view(-1, 8, 128);  reshape_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_12: "bf16[s0, 8, 128]" = getitem_15.view(-1, 8, 128);  getitem_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_1 = torch.ops.vllm.unified_attention_with_output(view_9, view_11, view_12, view_10, 'model.layers.1.self_attn.attn');  view_9 = view_11 = view_12 = unified_attention_with_output_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_13: "bf16[s0, 4096]" = view_10.view(-1, 4096);  view_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_5: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_13, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_13 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_3: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_5: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_5)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_6: "bf16[s0, 4096]" = torch.empty_like(empty_like_4)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_2 = torch.ops._C.fused_add_rms_norm(empty_like_5, rocm_unquantized_gemm_impl_5, empty_like_6, empty_like_4, _get_data_attr_3, 1e-05);  rocm_unquantized_gemm_impl_5 = empty_like_4 = _get_data_attr_3 = fused_add_rms_norm_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_6: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_5, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_5 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_1: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_1 = torch.ops._C.silu_and_mul(empty_1, rocm_unquantized_gemm_impl_6);  rocm_unquantized_gemm_impl_6 = silu_and_mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_7: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_1, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_1 = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_4: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_7: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_7)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_8: "bf16[s0, 4096]" = torch.empty_like(empty_like_6)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_3 = torch.ops._C.fused_add_rms_norm(empty_like_7, rocm_unquantized_gemm_impl_7, empty_like_8, empty_like_6, _get_data_attr_4, 1e-05);  rocm_unquantized_gemm_impl_7 = empty_like_6 = _get_data_attr_4 = fused_add_rms_norm_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_8: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_7, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_7 = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_2 = rocm_unquantized_gemm_impl_8.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_8 = None
            getitem_26: "bf16[s0, 4096]" = split_2[0]
            getitem_27: "bf16[s0, 1024]" = split_2[1]
            getitem_28: "bf16[s0, 1024]" = split_2[2];  split_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_2: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_2: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_6 = index_select_2.chunk(2, dim = -1);  index_select_2 = None
            getitem_29: "bf16[s0, 64]" = chunk_6[0]
            getitem_30: "bf16[s0, 64]" = chunk_6[1];  chunk_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_6 = getitem_26.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_14: "bf16[s0, 32, 128]" = getitem_26.view(s0, -1, 128);  getitem_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_31: "bf16[s0, 32, 128]" = view_14[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_32: "bf16[s0, 32, 0]" = view_14[(Ellipsis, slice(128, None, None))];  view_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_8: "bf16[s0, 1, 64]" = getitem_29.unsqueeze(-2)
            to_8: "bf16[s0, 1, 64]" = unsqueeze_8.to(torch.bfloat16);  unsqueeze_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_9: "bf16[s0, 1, 64]" = getitem_30.unsqueeze(-2)
            to_9: "bf16[s0, 1, 64]" = unsqueeze_9.to(torch.bfloat16);  unsqueeze_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_7 = torch.chunk(getitem_31, 2, dim = -1);  getitem_31 = None
            getitem_33: "bf16[s0, 32, 64]" = chunk_7[0]
            getitem_34: "bf16[s0, 32, 64]" = chunk_7[1];  chunk_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_16: "bf16[s0, 32, 64]" = getitem_33 * to_8
            mul_17: "bf16[s0, 32, 64]" = getitem_34 * to_9
            sub_4: "bf16[s0, 32, 64]" = mul_16 - mul_17;  mul_16 = mul_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_18: "bf16[s0, 32, 64]" = getitem_34 * to_8;  getitem_34 = to_8 = None
            mul_19: "bf16[s0, 32, 64]" = getitem_33 * to_9;  getitem_33 = to_9 = None
            add_4: "bf16[s0, 32, 64]" = mul_18 + mul_19;  mul_18 = mul_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_8: "bf16[s0, 32, 128]" = torch.cat((sub_4, add_4), dim = -1);  sub_4 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_9: "bf16[s0, 32, 128]" = torch.cat((cat_8, getitem_32), dim = -1);  cat_8 = getitem_32 = None
            reshape_4: "bf16[s0, 4096]" = cat_9.reshape(size_6);  cat_9 = size_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_7 = getitem_27.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_15: "bf16[s0, 8, 128]" = getitem_27.view(s0, -1, 128);  getitem_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_35: "bf16[s0, 8, 128]" = view_15[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_36: "bf16[s0, 8, 0]" = view_15[(Ellipsis, slice(128, None, None))];  view_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_10: "bf16[s0, 1, 64]" = getitem_29.unsqueeze(-2);  getitem_29 = None
            to_10: "bf16[s0, 1, 64]" = unsqueeze_10.to(torch.bfloat16);  unsqueeze_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_11: "bf16[s0, 1, 64]" = getitem_30.unsqueeze(-2);  getitem_30 = None
            to_11: "bf16[s0, 1, 64]" = unsqueeze_11.to(torch.bfloat16);  unsqueeze_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_8 = torch.chunk(getitem_35, 2, dim = -1);  getitem_35 = None
            getitem_37: "bf16[s0, 8, 64]" = chunk_8[0]
            getitem_38: "bf16[s0, 8, 64]" = chunk_8[1];  chunk_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_20: "bf16[s0, 8, 64]" = getitem_37 * to_10
            mul_21: "bf16[s0, 8, 64]" = getitem_38 * to_11
            sub_5: "bf16[s0, 8, 64]" = mul_20 - mul_21;  mul_20 = mul_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_22: "bf16[s0, 8, 64]" = getitem_38 * to_10;  getitem_38 = to_10 = None
            mul_23: "bf16[s0, 8, 64]" = getitem_37 * to_11;  getitem_37 = to_11 = None
            add_5: "bf16[s0, 8, 64]" = mul_22 + mul_23;  mul_22 = mul_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_10: "bf16[s0, 8, 128]" = torch.cat((sub_5, add_5), dim = -1);  sub_5 = add_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_11: "bf16[s0, 8, 128]" = torch.cat((cat_10, getitem_36), dim = -1);  cat_10 = getitem_36 = None
            reshape_5: "bf16[s0, 1024]" = cat_11.reshape(size_7);  cat_11 = size_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_8 = reshape_4.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_2: "bf16[s0, 4096]" = torch.zeros(size_8, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_16: "bf16[s0, 32, 128]" = reshape_4.view(-1, 32, 128);  reshape_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_17: "bf16[s0, 32, 128]" = zeros_2.view(-1, 32, 128);  zeros_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_18: "bf16[s0, 8, 128]" = reshape_5.view(-1, 8, 128);  reshape_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_19: "bf16[s0, 8, 128]" = getitem_28.view(-1, 8, 128);  getitem_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_2 = torch.ops.vllm.unified_attention_with_output(view_16, view_18, view_19, view_17, 'model.layers.2.self_attn.attn');  view_16 = view_18 = view_19 = unified_attention_with_output_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_20: "bf16[s0, 4096]" = view_17.view(-1, 4096);  view_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_9: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_20, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_20 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_5: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_9: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_9)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_10: "bf16[s0, 4096]" = torch.empty_like(empty_like_8)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_4 = torch.ops._C.fused_add_rms_norm(empty_like_9, rocm_unquantized_gemm_impl_9, empty_like_10, empty_like_8, _get_data_attr_5, 1e-05);  rocm_unquantized_gemm_impl_9 = empty_like_8 = _get_data_attr_5 = fused_add_rms_norm_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_10: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_9, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_9 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_2: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_2 = torch.ops._C.silu_and_mul(empty_2, rocm_unquantized_gemm_impl_10);  rocm_unquantized_gemm_impl_10 = silu_and_mul_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_11: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_2, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_2 = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_6: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_11: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_11)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_12: "bf16[s0, 4096]" = torch.empty_like(empty_like_10)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_5 = torch.ops._C.fused_add_rms_norm(empty_like_11, rocm_unquantized_gemm_impl_11, empty_like_12, empty_like_10, _get_data_attr_6, 1e-05);  rocm_unquantized_gemm_impl_11 = empty_like_10 = _get_data_attr_6 = fused_add_rms_norm_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_12: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_11, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_11 = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_3 = rocm_unquantized_gemm_impl_12.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_12 = None
            getitem_39: "bf16[s0, 4096]" = split_3[0]
            getitem_40: "bf16[s0, 1024]" = split_3[1]
            getitem_41: "bf16[s0, 1024]" = split_3[2];  split_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_3: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_3: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_3);  flatten_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_9 = index_select_3.chunk(2, dim = -1);  index_select_3 = None
            getitem_42: "bf16[s0, 64]" = chunk_9[0]
            getitem_43: "bf16[s0, 64]" = chunk_9[1];  chunk_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_9 = getitem_39.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_21: "bf16[s0, 32, 128]" = getitem_39.view(s0, -1, 128);  getitem_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_44: "bf16[s0, 32, 128]" = view_21[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_45: "bf16[s0, 32, 0]" = view_21[(Ellipsis, slice(128, None, None))];  view_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_12: "bf16[s0, 1, 64]" = getitem_42.unsqueeze(-2)
            to_12: "bf16[s0, 1, 64]" = unsqueeze_12.to(torch.bfloat16);  unsqueeze_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_13: "bf16[s0, 1, 64]" = getitem_43.unsqueeze(-2)
            to_13: "bf16[s0, 1, 64]" = unsqueeze_13.to(torch.bfloat16);  unsqueeze_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_10 = torch.chunk(getitem_44, 2, dim = -1);  getitem_44 = None
            getitem_46: "bf16[s0, 32, 64]" = chunk_10[0]
            getitem_47: "bf16[s0, 32, 64]" = chunk_10[1];  chunk_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_24: "bf16[s0, 32, 64]" = getitem_46 * to_12
            mul_25: "bf16[s0, 32, 64]" = getitem_47 * to_13
            sub_6: "bf16[s0, 32, 64]" = mul_24 - mul_25;  mul_24 = mul_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_26: "bf16[s0, 32, 64]" = getitem_47 * to_12;  getitem_47 = to_12 = None
            mul_27: "bf16[s0, 32, 64]" = getitem_46 * to_13;  getitem_46 = to_13 = None
            add_6: "bf16[s0, 32, 64]" = mul_26 + mul_27;  mul_26 = mul_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_12: "bf16[s0, 32, 128]" = torch.cat((sub_6, add_6), dim = -1);  sub_6 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_13: "bf16[s0, 32, 128]" = torch.cat((cat_12, getitem_45), dim = -1);  cat_12 = getitem_45 = None
            reshape_6: "bf16[s0, 4096]" = cat_13.reshape(size_9);  cat_13 = size_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_10 = getitem_40.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_22: "bf16[s0, 8, 128]" = getitem_40.view(s0, -1, 128);  getitem_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_48: "bf16[s0, 8, 128]" = view_22[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_49: "bf16[s0, 8, 0]" = view_22[(Ellipsis, slice(128, None, None))];  view_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_14: "bf16[s0, 1, 64]" = getitem_42.unsqueeze(-2);  getitem_42 = None
            to_14: "bf16[s0, 1, 64]" = unsqueeze_14.to(torch.bfloat16);  unsqueeze_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_15: "bf16[s0, 1, 64]" = getitem_43.unsqueeze(-2);  getitem_43 = None
            to_15: "bf16[s0, 1, 64]" = unsqueeze_15.to(torch.bfloat16);  unsqueeze_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_11 = torch.chunk(getitem_48, 2, dim = -1);  getitem_48 = None
            getitem_50: "bf16[s0, 8, 64]" = chunk_11[0]
            getitem_51: "bf16[s0, 8, 64]" = chunk_11[1];  chunk_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_28: "bf16[s0, 8, 64]" = getitem_50 * to_14
            mul_29: "bf16[s0, 8, 64]" = getitem_51 * to_15
            sub_7: "bf16[s0, 8, 64]" = mul_28 - mul_29;  mul_28 = mul_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_30: "bf16[s0, 8, 64]" = getitem_51 * to_14;  getitem_51 = to_14 = None
            mul_31: "bf16[s0, 8, 64]" = getitem_50 * to_15;  getitem_50 = to_15 = None
            add_7: "bf16[s0, 8, 64]" = mul_30 + mul_31;  mul_30 = mul_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_14: "bf16[s0, 8, 128]" = torch.cat((sub_7, add_7), dim = -1);  sub_7 = add_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_15: "bf16[s0, 8, 128]" = torch.cat((cat_14, getitem_49), dim = -1);  cat_14 = getitem_49 = None
            reshape_7: "bf16[s0, 1024]" = cat_15.reshape(size_10);  cat_15 = size_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_11 = reshape_6.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_3: "bf16[s0, 4096]" = torch.zeros(size_11, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_23: "bf16[s0, 32, 128]" = reshape_6.view(-1, 32, 128);  reshape_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_24: "bf16[s0, 32, 128]" = zeros_3.view(-1, 32, 128);  zeros_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_25: "bf16[s0, 8, 128]" = reshape_7.view(-1, 8, 128);  reshape_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_26: "bf16[s0, 8, 128]" = getitem_41.view(-1, 8, 128);  getitem_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_3 = torch.ops.vllm.unified_attention_with_output(view_23, view_25, view_26, view_24, 'model.layers.3.self_attn.attn');  view_23 = view_25 = view_26 = unified_attention_with_output_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_27: "bf16[s0, 4096]" = view_24.view(-1, 4096);  view_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_13: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_27, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_27 = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_7: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_13: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_13)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_14: "bf16[s0, 4096]" = torch.empty_like(empty_like_12)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_6 = torch.ops._C.fused_add_rms_norm(empty_like_13, rocm_unquantized_gemm_impl_13, empty_like_14, empty_like_12, _get_data_attr_7, 1e-05);  rocm_unquantized_gemm_impl_13 = empty_like_12 = _get_data_attr_7 = fused_add_rms_norm_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_14: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_13, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_13 = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_3: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_3 = torch.ops._C.silu_and_mul(empty_3, rocm_unquantized_gemm_impl_14);  rocm_unquantized_gemm_impl_14 = silu_and_mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_15: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_3, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_3 = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_8: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_15: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_15)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_16: "bf16[s0, 4096]" = torch.empty_like(empty_like_14)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_7 = torch.ops._C.fused_add_rms_norm(empty_like_15, rocm_unquantized_gemm_impl_15, empty_like_16, empty_like_14, _get_data_attr_8, 1e-05);  rocm_unquantized_gemm_impl_15 = empty_like_14 = _get_data_attr_8 = fused_add_rms_norm_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_16: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_15, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_15 = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_4 = rocm_unquantized_gemm_impl_16.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_16 = None
            getitem_52: "bf16[s0, 4096]" = split_4[0]
            getitem_53: "bf16[s0, 1024]" = split_4[1]
            getitem_54: "bf16[s0, 1024]" = split_4[2];  split_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_4: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_4: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_4);  flatten_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_12 = index_select_4.chunk(2, dim = -1);  index_select_4 = None
            getitem_55: "bf16[s0, 64]" = chunk_12[0]
            getitem_56: "bf16[s0, 64]" = chunk_12[1];  chunk_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_12 = getitem_52.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_28: "bf16[s0, 32, 128]" = getitem_52.view(s0, -1, 128);  getitem_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_57: "bf16[s0, 32, 128]" = view_28[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_58: "bf16[s0, 32, 0]" = view_28[(Ellipsis, slice(128, None, None))];  view_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_16: "bf16[s0, 1, 64]" = getitem_55.unsqueeze(-2)
            to_16: "bf16[s0, 1, 64]" = unsqueeze_16.to(torch.bfloat16);  unsqueeze_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_17: "bf16[s0, 1, 64]" = getitem_56.unsqueeze(-2)
            to_17: "bf16[s0, 1, 64]" = unsqueeze_17.to(torch.bfloat16);  unsqueeze_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_13 = torch.chunk(getitem_57, 2, dim = -1);  getitem_57 = None
            getitem_59: "bf16[s0, 32, 64]" = chunk_13[0]
            getitem_60: "bf16[s0, 32, 64]" = chunk_13[1];  chunk_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_32: "bf16[s0, 32, 64]" = getitem_59 * to_16
            mul_33: "bf16[s0, 32, 64]" = getitem_60 * to_17
            sub_8: "bf16[s0, 32, 64]" = mul_32 - mul_33;  mul_32 = mul_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_34: "bf16[s0, 32, 64]" = getitem_60 * to_16;  getitem_60 = to_16 = None
            mul_35: "bf16[s0, 32, 64]" = getitem_59 * to_17;  getitem_59 = to_17 = None
            add_8: "bf16[s0, 32, 64]" = mul_34 + mul_35;  mul_34 = mul_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_16: "bf16[s0, 32, 128]" = torch.cat((sub_8, add_8), dim = -1);  sub_8 = add_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_17: "bf16[s0, 32, 128]" = torch.cat((cat_16, getitem_58), dim = -1);  cat_16 = getitem_58 = None
            reshape_8: "bf16[s0, 4096]" = cat_17.reshape(size_12);  cat_17 = size_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_13 = getitem_53.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_29: "bf16[s0, 8, 128]" = getitem_53.view(s0, -1, 128);  getitem_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_61: "bf16[s0, 8, 128]" = view_29[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_62: "bf16[s0, 8, 0]" = view_29[(Ellipsis, slice(128, None, None))];  view_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_18: "bf16[s0, 1, 64]" = getitem_55.unsqueeze(-2);  getitem_55 = None
            to_18: "bf16[s0, 1, 64]" = unsqueeze_18.to(torch.bfloat16);  unsqueeze_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_19: "bf16[s0, 1, 64]" = getitem_56.unsqueeze(-2);  getitem_56 = None
            to_19: "bf16[s0, 1, 64]" = unsqueeze_19.to(torch.bfloat16);  unsqueeze_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_14 = torch.chunk(getitem_61, 2, dim = -1);  getitem_61 = None
            getitem_63: "bf16[s0, 8, 64]" = chunk_14[0]
            getitem_64: "bf16[s0, 8, 64]" = chunk_14[1];  chunk_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_36: "bf16[s0, 8, 64]" = getitem_63 * to_18
            mul_37: "bf16[s0, 8, 64]" = getitem_64 * to_19
            sub_9: "bf16[s0, 8, 64]" = mul_36 - mul_37;  mul_36 = mul_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_38: "bf16[s0, 8, 64]" = getitem_64 * to_18;  getitem_64 = to_18 = None
            mul_39: "bf16[s0, 8, 64]" = getitem_63 * to_19;  getitem_63 = to_19 = None
            add_9: "bf16[s0, 8, 64]" = mul_38 + mul_39;  mul_38 = mul_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_18: "bf16[s0, 8, 128]" = torch.cat((sub_9, add_9), dim = -1);  sub_9 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_19: "bf16[s0, 8, 128]" = torch.cat((cat_18, getitem_62), dim = -1);  cat_18 = getitem_62 = None
            reshape_9: "bf16[s0, 1024]" = cat_19.reshape(size_13);  cat_19 = size_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_14 = reshape_8.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_4: "bf16[s0, 4096]" = torch.zeros(size_14, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_30: "bf16[s0, 32, 128]" = reshape_8.view(-1, 32, 128);  reshape_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_31: "bf16[s0, 32, 128]" = zeros_4.view(-1, 32, 128);  zeros_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_32: "bf16[s0, 8, 128]" = reshape_9.view(-1, 8, 128);  reshape_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_33: "bf16[s0, 8, 128]" = getitem_54.view(-1, 8, 128);  getitem_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_4 = torch.ops.vllm.unified_attention_with_output(view_30, view_32, view_33, view_31, 'model.layers.4.self_attn.attn');  view_30 = view_32 = view_33 = unified_attention_with_output_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_34: "bf16[s0, 4096]" = view_31.view(-1, 4096);  view_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_17: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_34, l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_34 = l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_9: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_17: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_17)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_18: "bf16[s0, 4096]" = torch.empty_like(empty_like_16)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_8 = torch.ops._C.fused_add_rms_norm(empty_like_17, rocm_unquantized_gemm_impl_17, empty_like_18, empty_like_16, _get_data_attr_9, 1e-05);  rocm_unquantized_gemm_impl_17 = empty_like_16 = _get_data_attr_9 = fused_add_rms_norm_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_18: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_17, l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_17 = l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_4: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_4 = torch.ops._C.silu_and_mul(empty_4, rocm_unquantized_gemm_impl_18);  rocm_unquantized_gemm_impl_18 = silu_and_mul_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_19: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_4, l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_4 = l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_10: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_19: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_19)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_20: "bf16[s0, 4096]" = torch.empty_like(empty_like_18)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_9 = torch.ops._C.fused_add_rms_norm(empty_like_19, rocm_unquantized_gemm_impl_19, empty_like_20, empty_like_18, _get_data_attr_10, 1e-05);  rocm_unquantized_gemm_impl_19 = empty_like_18 = _get_data_attr_10 = fused_add_rms_norm_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_20: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_19, l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_19 = l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_5 = rocm_unquantized_gemm_impl_20.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_20 = None
            getitem_65: "bf16[s0, 4096]" = split_5[0]
            getitem_66: "bf16[s0, 1024]" = split_5[1]
            getitem_67: "bf16[s0, 1024]" = split_5[2];  split_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_5: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_5: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_5);  flatten_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_15 = index_select_5.chunk(2, dim = -1);  index_select_5 = None
            getitem_68: "bf16[s0, 64]" = chunk_15[0]
            getitem_69: "bf16[s0, 64]" = chunk_15[1];  chunk_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_15 = getitem_65.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_35: "bf16[s0, 32, 128]" = getitem_65.view(s0, -1, 128);  getitem_65 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_70: "bf16[s0, 32, 128]" = view_35[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_71: "bf16[s0, 32, 0]" = view_35[(Ellipsis, slice(128, None, None))];  view_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_20: "bf16[s0, 1, 64]" = getitem_68.unsqueeze(-2)
            to_20: "bf16[s0, 1, 64]" = unsqueeze_20.to(torch.bfloat16);  unsqueeze_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_21: "bf16[s0, 1, 64]" = getitem_69.unsqueeze(-2)
            to_21: "bf16[s0, 1, 64]" = unsqueeze_21.to(torch.bfloat16);  unsqueeze_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_16 = torch.chunk(getitem_70, 2, dim = -1);  getitem_70 = None
            getitem_72: "bf16[s0, 32, 64]" = chunk_16[0]
            getitem_73: "bf16[s0, 32, 64]" = chunk_16[1];  chunk_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_40: "bf16[s0, 32, 64]" = getitem_72 * to_20
            mul_41: "bf16[s0, 32, 64]" = getitem_73 * to_21
            sub_10: "bf16[s0, 32, 64]" = mul_40 - mul_41;  mul_40 = mul_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_42: "bf16[s0, 32, 64]" = getitem_73 * to_20;  getitem_73 = to_20 = None
            mul_43: "bf16[s0, 32, 64]" = getitem_72 * to_21;  getitem_72 = to_21 = None
            add_10: "bf16[s0, 32, 64]" = mul_42 + mul_43;  mul_42 = mul_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_20: "bf16[s0, 32, 128]" = torch.cat((sub_10, add_10), dim = -1);  sub_10 = add_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_21: "bf16[s0, 32, 128]" = torch.cat((cat_20, getitem_71), dim = -1);  cat_20 = getitem_71 = None
            reshape_10: "bf16[s0, 4096]" = cat_21.reshape(size_15);  cat_21 = size_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_16 = getitem_66.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_36: "bf16[s0, 8, 128]" = getitem_66.view(s0, -1, 128);  getitem_66 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_74: "bf16[s0, 8, 128]" = view_36[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_75: "bf16[s0, 8, 0]" = view_36[(Ellipsis, slice(128, None, None))];  view_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_22: "bf16[s0, 1, 64]" = getitem_68.unsqueeze(-2);  getitem_68 = None
            to_22: "bf16[s0, 1, 64]" = unsqueeze_22.to(torch.bfloat16);  unsqueeze_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_23: "bf16[s0, 1, 64]" = getitem_69.unsqueeze(-2);  getitem_69 = None
            to_23: "bf16[s0, 1, 64]" = unsqueeze_23.to(torch.bfloat16);  unsqueeze_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_17 = torch.chunk(getitem_74, 2, dim = -1);  getitem_74 = None
            getitem_76: "bf16[s0, 8, 64]" = chunk_17[0]
            getitem_77: "bf16[s0, 8, 64]" = chunk_17[1];  chunk_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_44: "bf16[s0, 8, 64]" = getitem_76 * to_22
            mul_45: "bf16[s0, 8, 64]" = getitem_77 * to_23
            sub_11: "bf16[s0, 8, 64]" = mul_44 - mul_45;  mul_44 = mul_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_46: "bf16[s0, 8, 64]" = getitem_77 * to_22;  getitem_77 = to_22 = None
            mul_47: "bf16[s0, 8, 64]" = getitem_76 * to_23;  getitem_76 = to_23 = None
            add_11: "bf16[s0, 8, 64]" = mul_46 + mul_47;  mul_46 = mul_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_22: "bf16[s0, 8, 128]" = torch.cat((sub_11, add_11), dim = -1);  sub_11 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_23: "bf16[s0, 8, 128]" = torch.cat((cat_22, getitem_75), dim = -1);  cat_22 = getitem_75 = None
            reshape_11: "bf16[s0, 1024]" = cat_23.reshape(size_16);  cat_23 = size_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_17 = reshape_10.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_5: "bf16[s0, 4096]" = torch.zeros(size_17, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_37: "bf16[s0, 32, 128]" = reshape_10.view(-1, 32, 128);  reshape_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_38: "bf16[s0, 32, 128]" = zeros_5.view(-1, 32, 128);  zeros_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_39: "bf16[s0, 8, 128]" = reshape_11.view(-1, 8, 128);  reshape_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_40: "bf16[s0, 8, 128]" = getitem_67.view(-1, 8, 128);  getitem_67 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_5 = torch.ops.vllm.unified_attention_with_output(view_37, view_39, view_40, view_38, 'model.layers.5.self_attn.attn');  view_37 = view_39 = view_40 = unified_attention_with_output_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_41: "bf16[s0, 4096]" = view_38.view(-1, 4096);  view_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_21: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_41, l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_41 = l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_11: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_21: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_21)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_22: "bf16[s0, 4096]" = torch.empty_like(empty_like_20)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_10 = torch.ops._C.fused_add_rms_norm(empty_like_21, rocm_unquantized_gemm_impl_21, empty_like_22, empty_like_20, _get_data_attr_11, 1e-05);  rocm_unquantized_gemm_impl_21 = empty_like_20 = _get_data_attr_11 = fused_add_rms_norm_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_22: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_21, l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_21 = l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_5: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_5 = torch.ops._C.silu_and_mul(empty_5, rocm_unquantized_gemm_impl_22);  rocm_unquantized_gemm_impl_22 = silu_and_mul_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_23: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_5, l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_5 = l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_12: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_23: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_23)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_24: "bf16[s0, 4096]" = torch.empty_like(empty_like_22)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_11 = torch.ops._C.fused_add_rms_norm(empty_like_23, rocm_unquantized_gemm_impl_23, empty_like_24, empty_like_22, _get_data_attr_12, 1e-05);  rocm_unquantized_gemm_impl_23 = empty_like_22 = _get_data_attr_12 = fused_add_rms_norm_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_24: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_23, l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_23 = l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_6 = rocm_unquantized_gemm_impl_24.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_24 = None
            getitem_78: "bf16[s0, 4096]" = split_6[0]
            getitem_79: "bf16[s0, 1024]" = split_6[1]
            getitem_80: "bf16[s0, 1024]" = split_6[2];  split_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_6: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_6: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_6);  flatten_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_18 = index_select_6.chunk(2, dim = -1);  index_select_6 = None
            getitem_81: "bf16[s0, 64]" = chunk_18[0]
            getitem_82: "bf16[s0, 64]" = chunk_18[1];  chunk_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_18 = getitem_78.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_42: "bf16[s0, 32, 128]" = getitem_78.view(s0, -1, 128);  getitem_78 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_83: "bf16[s0, 32, 128]" = view_42[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_84: "bf16[s0, 32, 0]" = view_42[(Ellipsis, slice(128, None, None))];  view_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_24: "bf16[s0, 1, 64]" = getitem_81.unsqueeze(-2)
            to_24: "bf16[s0, 1, 64]" = unsqueeze_24.to(torch.bfloat16);  unsqueeze_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_25: "bf16[s0, 1, 64]" = getitem_82.unsqueeze(-2)
            to_25: "bf16[s0, 1, 64]" = unsqueeze_25.to(torch.bfloat16);  unsqueeze_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_19 = torch.chunk(getitem_83, 2, dim = -1);  getitem_83 = None
            getitem_85: "bf16[s0, 32, 64]" = chunk_19[0]
            getitem_86: "bf16[s0, 32, 64]" = chunk_19[1];  chunk_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_48: "bf16[s0, 32, 64]" = getitem_85 * to_24
            mul_49: "bf16[s0, 32, 64]" = getitem_86 * to_25
            sub_12: "bf16[s0, 32, 64]" = mul_48 - mul_49;  mul_48 = mul_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_50: "bf16[s0, 32, 64]" = getitem_86 * to_24;  getitem_86 = to_24 = None
            mul_51: "bf16[s0, 32, 64]" = getitem_85 * to_25;  getitem_85 = to_25 = None
            add_12: "bf16[s0, 32, 64]" = mul_50 + mul_51;  mul_50 = mul_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_24: "bf16[s0, 32, 128]" = torch.cat((sub_12, add_12), dim = -1);  sub_12 = add_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_25: "bf16[s0, 32, 128]" = torch.cat((cat_24, getitem_84), dim = -1);  cat_24 = getitem_84 = None
            reshape_12: "bf16[s0, 4096]" = cat_25.reshape(size_18);  cat_25 = size_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_19 = getitem_79.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_43: "bf16[s0, 8, 128]" = getitem_79.view(s0, -1, 128);  getitem_79 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_87: "bf16[s0, 8, 128]" = view_43[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_88: "bf16[s0, 8, 0]" = view_43[(Ellipsis, slice(128, None, None))];  view_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_26: "bf16[s0, 1, 64]" = getitem_81.unsqueeze(-2);  getitem_81 = None
            to_26: "bf16[s0, 1, 64]" = unsqueeze_26.to(torch.bfloat16);  unsqueeze_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_27: "bf16[s0, 1, 64]" = getitem_82.unsqueeze(-2);  getitem_82 = None
            to_27: "bf16[s0, 1, 64]" = unsqueeze_27.to(torch.bfloat16);  unsqueeze_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_20 = torch.chunk(getitem_87, 2, dim = -1);  getitem_87 = None
            getitem_89: "bf16[s0, 8, 64]" = chunk_20[0]
            getitem_90: "bf16[s0, 8, 64]" = chunk_20[1];  chunk_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_52: "bf16[s0, 8, 64]" = getitem_89 * to_26
            mul_53: "bf16[s0, 8, 64]" = getitem_90 * to_27
            sub_13: "bf16[s0, 8, 64]" = mul_52 - mul_53;  mul_52 = mul_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_54: "bf16[s0, 8, 64]" = getitem_90 * to_26;  getitem_90 = to_26 = None
            mul_55: "bf16[s0, 8, 64]" = getitem_89 * to_27;  getitem_89 = to_27 = None
            add_13: "bf16[s0, 8, 64]" = mul_54 + mul_55;  mul_54 = mul_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_26: "bf16[s0, 8, 128]" = torch.cat((sub_13, add_13), dim = -1);  sub_13 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_27: "bf16[s0, 8, 128]" = torch.cat((cat_26, getitem_88), dim = -1);  cat_26 = getitem_88 = None
            reshape_13: "bf16[s0, 1024]" = cat_27.reshape(size_19);  cat_27 = size_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_20 = reshape_12.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_6: "bf16[s0, 4096]" = torch.zeros(size_20, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_44: "bf16[s0, 32, 128]" = reshape_12.view(-1, 32, 128);  reshape_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_45: "bf16[s0, 32, 128]" = zeros_6.view(-1, 32, 128);  zeros_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_46: "bf16[s0, 8, 128]" = reshape_13.view(-1, 8, 128);  reshape_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_47: "bf16[s0, 8, 128]" = getitem_80.view(-1, 8, 128);  getitem_80 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_6 = torch.ops.vllm.unified_attention_with_output(view_44, view_46, view_47, view_45, 'model.layers.6.self_attn.attn');  view_44 = view_46 = view_47 = unified_attention_with_output_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_48: "bf16[s0, 4096]" = view_45.view(-1, 4096);  view_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_25: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_48, l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_48 = l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_13: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_25: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_25)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_26: "bf16[s0, 4096]" = torch.empty_like(empty_like_24)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_12 = torch.ops._C.fused_add_rms_norm(empty_like_25, rocm_unquantized_gemm_impl_25, empty_like_26, empty_like_24, _get_data_attr_13, 1e-05);  rocm_unquantized_gemm_impl_25 = empty_like_24 = _get_data_attr_13 = fused_add_rms_norm_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_26: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_25, l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_25 = l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_6: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_6 = torch.ops._C.silu_and_mul(empty_6, rocm_unquantized_gemm_impl_26);  rocm_unquantized_gemm_impl_26 = silu_and_mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_27: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_6, l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_6 = l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_14: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_27: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_27)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_28: "bf16[s0, 4096]" = torch.empty_like(empty_like_26)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_13 = torch.ops._C.fused_add_rms_norm(empty_like_27, rocm_unquantized_gemm_impl_27, empty_like_28, empty_like_26, _get_data_attr_14, 1e-05);  rocm_unquantized_gemm_impl_27 = empty_like_26 = _get_data_attr_14 = fused_add_rms_norm_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_28: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_27, l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_27 = l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_7 = rocm_unquantized_gemm_impl_28.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_28 = None
            getitem_91: "bf16[s0, 4096]" = split_7[0]
            getitem_92: "bf16[s0, 1024]" = split_7[1]
            getitem_93: "bf16[s0, 1024]" = split_7[2];  split_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_7: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_7: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_7);  flatten_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_21 = index_select_7.chunk(2, dim = -1);  index_select_7 = None
            getitem_94: "bf16[s0, 64]" = chunk_21[0]
            getitem_95: "bf16[s0, 64]" = chunk_21[1];  chunk_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_21 = getitem_91.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_49: "bf16[s0, 32, 128]" = getitem_91.view(s0, -1, 128);  getitem_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_96: "bf16[s0, 32, 128]" = view_49[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_97: "bf16[s0, 32, 0]" = view_49[(Ellipsis, slice(128, None, None))];  view_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_28: "bf16[s0, 1, 64]" = getitem_94.unsqueeze(-2)
            to_28: "bf16[s0, 1, 64]" = unsqueeze_28.to(torch.bfloat16);  unsqueeze_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_29: "bf16[s0, 1, 64]" = getitem_95.unsqueeze(-2)
            to_29: "bf16[s0, 1, 64]" = unsqueeze_29.to(torch.bfloat16);  unsqueeze_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_22 = torch.chunk(getitem_96, 2, dim = -1);  getitem_96 = None
            getitem_98: "bf16[s0, 32, 64]" = chunk_22[0]
            getitem_99: "bf16[s0, 32, 64]" = chunk_22[1];  chunk_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_56: "bf16[s0, 32, 64]" = getitem_98 * to_28
            mul_57: "bf16[s0, 32, 64]" = getitem_99 * to_29
            sub_14: "bf16[s0, 32, 64]" = mul_56 - mul_57;  mul_56 = mul_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_58: "bf16[s0, 32, 64]" = getitem_99 * to_28;  getitem_99 = to_28 = None
            mul_59: "bf16[s0, 32, 64]" = getitem_98 * to_29;  getitem_98 = to_29 = None
            add_14: "bf16[s0, 32, 64]" = mul_58 + mul_59;  mul_58 = mul_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_28: "bf16[s0, 32, 128]" = torch.cat((sub_14, add_14), dim = -1);  sub_14 = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_29: "bf16[s0, 32, 128]" = torch.cat((cat_28, getitem_97), dim = -1);  cat_28 = getitem_97 = None
            reshape_14: "bf16[s0, 4096]" = cat_29.reshape(size_21);  cat_29 = size_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_22 = getitem_92.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_50: "bf16[s0, 8, 128]" = getitem_92.view(s0, -1, 128);  getitem_92 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_100: "bf16[s0, 8, 128]" = view_50[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_101: "bf16[s0, 8, 0]" = view_50[(Ellipsis, slice(128, None, None))];  view_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_30: "bf16[s0, 1, 64]" = getitem_94.unsqueeze(-2);  getitem_94 = None
            to_30: "bf16[s0, 1, 64]" = unsqueeze_30.to(torch.bfloat16);  unsqueeze_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_31: "bf16[s0, 1, 64]" = getitem_95.unsqueeze(-2);  getitem_95 = None
            to_31: "bf16[s0, 1, 64]" = unsqueeze_31.to(torch.bfloat16);  unsqueeze_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_23 = torch.chunk(getitem_100, 2, dim = -1);  getitem_100 = None
            getitem_102: "bf16[s0, 8, 64]" = chunk_23[0]
            getitem_103: "bf16[s0, 8, 64]" = chunk_23[1];  chunk_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_60: "bf16[s0, 8, 64]" = getitem_102 * to_30
            mul_61: "bf16[s0, 8, 64]" = getitem_103 * to_31
            sub_15: "bf16[s0, 8, 64]" = mul_60 - mul_61;  mul_60 = mul_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_62: "bf16[s0, 8, 64]" = getitem_103 * to_30;  getitem_103 = to_30 = None
            mul_63: "bf16[s0, 8, 64]" = getitem_102 * to_31;  getitem_102 = to_31 = None
            add_15: "bf16[s0, 8, 64]" = mul_62 + mul_63;  mul_62 = mul_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_30: "bf16[s0, 8, 128]" = torch.cat((sub_15, add_15), dim = -1);  sub_15 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_31: "bf16[s0, 8, 128]" = torch.cat((cat_30, getitem_101), dim = -1);  cat_30 = getitem_101 = None
            reshape_15: "bf16[s0, 1024]" = cat_31.reshape(size_22);  cat_31 = size_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_23 = reshape_14.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_7: "bf16[s0, 4096]" = torch.zeros(size_23, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_51: "bf16[s0, 32, 128]" = reshape_14.view(-1, 32, 128);  reshape_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_52: "bf16[s0, 32, 128]" = zeros_7.view(-1, 32, 128);  zeros_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_53: "bf16[s0, 8, 128]" = reshape_15.view(-1, 8, 128);  reshape_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_54: "bf16[s0, 8, 128]" = getitem_93.view(-1, 8, 128);  getitem_93 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_7 = torch.ops.vllm.unified_attention_with_output(view_51, view_53, view_54, view_52, 'model.layers.7.self_attn.attn');  view_51 = view_53 = view_54 = unified_attention_with_output_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_55: "bf16[s0, 4096]" = view_52.view(-1, 4096);  view_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_29: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_55, l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_55 = l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_15: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_29: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_29)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_30: "bf16[s0, 4096]" = torch.empty_like(empty_like_28)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_14 = torch.ops._C.fused_add_rms_norm(empty_like_29, rocm_unquantized_gemm_impl_29, empty_like_30, empty_like_28, _get_data_attr_15, 1e-05);  rocm_unquantized_gemm_impl_29 = empty_like_28 = _get_data_attr_15 = fused_add_rms_norm_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_30: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_29, l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_29 = l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_7: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_7 = torch.ops._C.silu_and_mul(empty_7, rocm_unquantized_gemm_impl_30);  rocm_unquantized_gemm_impl_30 = silu_and_mul_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_31: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_7, l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_7 = l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_16: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_31: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_31)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_32: "bf16[s0, 4096]" = torch.empty_like(empty_like_30)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_15 = torch.ops._C.fused_add_rms_norm(empty_like_31, rocm_unquantized_gemm_impl_31, empty_like_32, empty_like_30, _get_data_attr_16, 1e-05);  rocm_unquantized_gemm_impl_31 = empty_like_30 = _get_data_attr_16 = fused_add_rms_norm_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_32: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_31, l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_31 = l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_8 = rocm_unquantized_gemm_impl_32.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_32 = None
            getitem_104: "bf16[s0, 4096]" = split_8[0]
            getitem_105: "bf16[s0, 1024]" = split_8[1]
            getitem_106: "bf16[s0, 1024]" = split_8[2];  split_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_8: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_8: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_8);  flatten_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_24 = index_select_8.chunk(2, dim = -1);  index_select_8 = None
            getitem_107: "bf16[s0, 64]" = chunk_24[0]
            getitem_108: "bf16[s0, 64]" = chunk_24[1];  chunk_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_24 = getitem_104.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_56: "bf16[s0, 32, 128]" = getitem_104.view(s0, -1, 128);  getitem_104 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_109: "bf16[s0, 32, 128]" = view_56[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_110: "bf16[s0, 32, 0]" = view_56[(Ellipsis, slice(128, None, None))];  view_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_32: "bf16[s0, 1, 64]" = getitem_107.unsqueeze(-2)
            to_32: "bf16[s0, 1, 64]" = unsqueeze_32.to(torch.bfloat16);  unsqueeze_32 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_33: "bf16[s0, 1, 64]" = getitem_108.unsqueeze(-2)
            to_33: "bf16[s0, 1, 64]" = unsqueeze_33.to(torch.bfloat16);  unsqueeze_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_25 = torch.chunk(getitem_109, 2, dim = -1);  getitem_109 = None
            getitem_111: "bf16[s0, 32, 64]" = chunk_25[0]
            getitem_112: "bf16[s0, 32, 64]" = chunk_25[1];  chunk_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_64: "bf16[s0, 32, 64]" = getitem_111 * to_32
            mul_65: "bf16[s0, 32, 64]" = getitem_112 * to_33
            sub_16: "bf16[s0, 32, 64]" = mul_64 - mul_65;  mul_64 = mul_65 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_66: "bf16[s0, 32, 64]" = getitem_112 * to_32;  getitem_112 = to_32 = None
            mul_67: "bf16[s0, 32, 64]" = getitem_111 * to_33;  getitem_111 = to_33 = None
            add_16: "bf16[s0, 32, 64]" = mul_66 + mul_67;  mul_66 = mul_67 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_32: "bf16[s0, 32, 128]" = torch.cat((sub_16, add_16), dim = -1);  sub_16 = add_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_33: "bf16[s0, 32, 128]" = torch.cat((cat_32, getitem_110), dim = -1);  cat_32 = getitem_110 = None
            reshape_16: "bf16[s0, 4096]" = cat_33.reshape(size_24);  cat_33 = size_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_25 = getitem_105.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_57: "bf16[s0, 8, 128]" = getitem_105.view(s0, -1, 128);  getitem_105 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_113: "bf16[s0, 8, 128]" = view_57[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_114: "bf16[s0, 8, 0]" = view_57[(Ellipsis, slice(128, None, None))];  view_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_34: "bf16[s0, 1, 64]" = getitem_107.unsqueeze(-2);  getitem_107 = None
            to_34: "bf16[s0, 1, 64]" = unsqueeze_34.to(torch.bfloat16);  unsqueeze_34 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_35: "bf16[s0, 1, 64]" = getitem_108.unsqueeze(-2);  getitem_108 = None
            to_35: "bf16[s0, 1, 64]" = unsqueeze_35.to(torch.bfloat16);  unsqueeze_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_26 = torch.chunk(getitem_113, 2, dim = -1);  getitem_113 = None
            getitem_115: "bf16[s0, 8, 64]" = chunk_26[0]
            getitem_116: "bf16[s0, 8, 64]" = chunk_26[1];  chunk_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_68: "bf16[s0, 8, 64]" = getitem_115 * to_34
            mul_69: "bf16[s0, 8, 64]" = getitem_116 * to_35
            sub_17: "bf16[s0, 8, 64]" = mul_68 - mul_69;  mul_68 = mul_69 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_70: "bf16[s0, 8, 64]" = getitem_116 * to_34;  getitem_116 = to_34 = None
            mul_71: "bf16[s0, 8, 64]" = getitem_115 * to_35;  getitem_115 = to_35 = None
            add_17: "bf16[s0, 8, 64]" = mul_70 + mul_71;  mul_70 = mul_71 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_34: "bf16[s0, 8, 128]" = torch.cat((sub_17, add_17), dim = -1);  sub_17 = add_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_35: "bf16[s0, 8, 128]" = torch.cat((cat_34, getitem_114), dim = -1);  cat_34 = getitem_114 = None
            reshape_17: "bf16[s0, 1024]" = cat_35.reshape(size_25);  cat_35 = size_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_26 = reshape_16.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_8: "bf16[s0, 4096]" = torch.zeros(size_26, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_58: "bf16[s0, 32, 128]" = reshape_16.view(-1, 32, 128);  reshape_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_59: "bf16[s0, 32, 128]" = zeros_8.view(-1, 32, 128);  zeros_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_60: "bf16[s0, 8, 128]" = reshape_17.view(-1, 8, 128);  reshape_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_61: "bf16[s0, 8, 128]" = getitem_106.view(-1, 8, 128);  getitem_106 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_8 = torch.ops.vllm.unified_attention_with_output(view_58, view_60, view_61, view_59, 'model.layers.8.self_attn.attn');  view_58 = view_60 = view_61 = unified_attention_with_output_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_62: "bf16[s0, 4096]" = view_59.view(-1, 4096);  view_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_33: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_62, l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_62 = l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_17: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_33: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_33)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_34: "bf16[s0, 4096]" = torch.empty_like(empty_like_32)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_16 = torch.ops._C.fused_add_rms_norm(empty_like_33, rocm_unquantized_gemm_impl_33, empty_like_34, empty_like_32, _get_data_attr_17, 1e-05);  rocm_unquantized_gemm_impl_33 = empty_like_32 = _get_data_attr_17 = fused_add_rms_norm_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_34: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_33, l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_33 = l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_8: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_8 = torch.ops._C.silu_and_mul(empty_8, rocm_unquantized_gemm_impl_34);  rocm_unquantized_gemm_impl_34 = silu_and_mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_35: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_8, l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_8 = l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_18: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_35: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_35)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_36: "bf16[s0, 4096]" = torch.empty_like(empty_like_34)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_17 = torch.ops._C.fused_add_rms_norm(empty_like_35, rocm_unquantized_gemm_impl_35, empty_like_36, empty_like_34, _get_data_attr_18, 1e-05);  rocm_unquantized_gemm_impl_35 = empty_like_34 = _get_data_attr_18 = fused_add_rms_norm_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_36: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_35, l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_35 = l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_9 = rocm_unquantized_gemm_impl_36.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_36 = None
            getitem_117: "bf16[s0, 4096]" = split_9[0]
            getitem_118: "bf16[s0, 1024]" = split_9[1]
            getitem_119: "bf16[s0, 1024]" = split_9[2];  split_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_9: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_9: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_9);  flatten_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_27 = index_select_9.chunk(2, dim = -1);  index_select_9 = None
            getitem_120: "bf16[s0, 64]" = chunk_27[0]
            getitem_121: "bf16[s0, 64]" = chunk_27[1];  chunk_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_27 = getitem_117.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_63: "bf16[s0, 32, 128]" = getitem_117.view(s0, -1, 128);  getitem_117 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_122: "bf16[s0, 32, 128]" = view_63[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_123: "bf16[s0, 32, 0]" = view_63[(Ellipsis, slice(128, None, None))];  view_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_36: "bf16[s0, 1, 64]" = getitem_120.unsqueeze(-2)
            to_36: "bf16[s0, 1, 64]" = unsqueeze_36.to(torch.bfloat16);  unsqueeze_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_37: "bf16[s0, 1, 64]" = getitem_121.unsqueeze(-2)
            to_37: "bf16[s0, 1, 64]" = unsqueeze_37.to(torch.bfloat16);  unsqueeze_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_28 = torch.chunk(getitem_122, 2, dim = -1);  getitem_122 = None
            getitem_124: "bf16[s0, 32, 64]" = chunk_28[0]
            getitem_125: "bf16[s0, 32, 64]" = chunk_28[1];  chunk_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_72: "bf16[s0, 32, 64]" = getitem_124 * to_36
            mul_73: "bf16[s0, 32, 64]" = getitem_125 * to_37
            sub_18: "bf16[s0, 32, 64]" = mul_72 - mul_73;  mul_72 = mul_73 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_74: "bf16[s0, 32, 64]" = getitem_125 * to_36;  getitem_125 = to_36 = None
            mul_75: "bf16[s0, 32, 64]" = getitem_124 * to_37;  getitem_124 = to_37 = None
            add_18: "bf16[s0, 32, 64]" = mul_74 + mul_75;  mul_74 = mul_75 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_36: "bf16[s0, 32, 128]" = torch.cat((sub_18, add_18), dim = -1);  sub_18 = add_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_37: "bf16[s0, 32, 128]" = torch.cat((cat_36, getitem_123), dim = -1);  cat_36 = getitem_123 = None
            reshape_18: "bf16[s0, 4096]" = cat_37.reshape(size_27);  cat_37 = size_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_28 = getitem_118.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_64: "bf16[s0, 8, 128]" = getitem_118.view(s0, -1, 128);  getitem_118 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_126: "bf16[s0, 8, 128]" = view_64[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_127: "bf16[s0, 8, 0]" = view_64[(Ellipsis, slice(128, None, None))];  view_64 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_38: "bf16[s0, 1, 64]" = getitem_120.unsqueeze(-2);  getitem_120 = None
            to_38: "bf16[s0, 1, 64]" = unsqueeze_38.to(torch.bfloat16);  unsqueeze_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_39: "bf16[s0, 1, 64]" = getitem_121.unsqueeze(-2);  getitem_121 = None
            to_39: "bf16[s0, 1, 64]" = unsqueeze_39.to(torch.bfloat16);  unsqueeze_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_29 = torch.chunk(getitem_126, 2, dim = -1);  getitem_126 = None
            getitem_128: "bf16[s0, 8, 64]" = chunk_29[0]
            getitem_129: "bf16[s0, 8, 64]" = chunk_29[1];  chunk_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_76: "bf16[s0, 8, 64]" = getitem_128 * to_38
            mul_77: "bf16[s0, 8, 64]" = getitem_129 * to_39
            sub_19: "bf16[s0, 8, 64]" = mul_76 - mul_77;  mul_76 = mul_77 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_78: "bf16[s0, 8, 64]" = getitem_129 * to_38;  getitem_129 = to_38 = None
            mul_79: "bf16[s0, 8, 64]" = getitem_128 * to_39;  getitem_128 = to_39 = None
            add_19: "bf16[s0, 8, 64]" = mul_78 + mul_79;  mul_78 = mul_79 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_38: "bf16[s0, 8, 128]" = torch.cat((sub_19, add_19), dim = -1);  sub_19 = add_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_39: "bf16[s0, 8, 128]" = torch.cat((cat_38, getitem_127), dim = -1);  cat_38 = getitem_127 = None
            reshape_19: "bf16[s0, 1024]" = cat_39.reshape(size_28);  cat_39 = size_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_29 = reshape_18.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_9: "bf16[s0, 4096]" = torch.zeros(size_29, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_65: "bf16[s0, 32, 128]" = reshape_18.view(-1, 32, 128);  reshape_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_66: "bf16[s0, 32, 128]" = zeros_9.view(-1, 32, 128);  zeros_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_67: "bf16[s0, 8, 128]" = reshape_19.view(-1, 8, 128);  reshape_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_68: "bf16[s0, 8, 128]" = getitem_119.view(-1, 8, 128);  getitem_119 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_9 = torch.ops.vllm.unified_attention_with_output(view_65, view_67, view_68, view_66, 'model.layers.9.self_attn.attn');  view_65 = view_67 = view_68 = unified_attention_with_output_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_69: "bf16[s0, 4096]" = view_66.view(-1, 4096);  view_66 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_37: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_69, l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_69 = l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_19: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_37: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_37)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_38: "bf16[s0, 4096]" = torch.empty_like(empty_like_36)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_18 = torch.ops._C.fused_add_rms_norm(empty_like_37, rocm_unquantized_gemm_impl_37, empty_like_38, empty_like_36, _get_data_attr_19, 1e-05);  rocm_unquantized_gemm_impl_37 = empty_like_36 = _get_data_attr_19 = fused_add_rms_norm_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_38: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_37, l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_37 = l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_9: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_9 = torch.ops._C.silu_and_mul(empty_9, rocm_unquantized_gemm_impl_38);  rocm_unquantized_gemm_impl_38 = silu_and_mul_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_39: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_9, l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_9 = l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_20: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_39: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_39)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_40: "bf16[s0, 4096]" = torch.empty_like(empty_like_38)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_19 = torch.ops._C.fused_add_rms_norm(empty_like_39, rocm_unquantized_gemm_impl_39, empty_like_40, empty_like_38, _get_data_attr_20, 1e-05);  rocm_unquantized_gemm_impl_39 = empty_like_38 = _get_data_attr_20 = fused_add_rms_norm_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_40: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_39, l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_39 = l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_10 = rocm_unquantized_gemm_impl_40.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_40 = None
            getitem_130: "bf16[s0, 4096]" = split_10[0]
            getitem_131: "bf16[s0, 1024]" = split_10[1]
            getitem_132: "bf16[s0, 1024]" = split_10[2];  split_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_10: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_10: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_10);  flatten_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_30 = index_select_10.chunk(2, dim = -1);  index_select_10 = None
            getitem_133: "bf16[s0, 64]" = chunk_30[0]
            getitem_134: "bf16[s0, 64]" = chunk_30[1];  chunk_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_30 = getitem_130.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_70: "bf16[s0, 32, 128]" = getitem_130.view(s0, -1, 128);  getitem_130 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_135: "bf16[s0, 32, 128]" = view_70[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_136: "bf16[s0, 32, 0]" = view_70[(Ellipsis, slice(128, None, None))];  view_70 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_40: "bf16[s0, 1, 64]" = getitem_133.unsqueeze(-2)
            to_40: "bf16[s0, 1, 64]" = unsqueeze_40.to(torch.bfloat16);  unsqueeze_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_41: "bf16[s0, 1, 64]" = getitem_134.unsqueeze(-2)
            to_41: "bf16[s0, 1, 64]" = unsqueeze_41.to(torch.bfloat16);  unsqueeze_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_31 = torch.chunk(getitem_135, 2, dim = -1);  getitem_135 = None
            getitem_137: "bf16[s0, 32, 64]" = chunk_31[0]
            getitem_138: "bf16[s0, 32, 64]" = chunk_31[1];  chunk_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_80: "bf16[s0, 32, 64]" = getitem_137 * to_40
            mul_81: "bf16[s0, 32, 64]" = getitem_138 * to_41
            sub_20: "bf16[s0, 32, 64]" = mul_80 - mul_81;  mul_80 = mul_81 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_82: "bf16[s0, 32, 64]" = getitem_138 * to_40;  getitem_138 = to_40 = None
            mul_83: "bf16[s0, 32, 64]" = getitem_137 * to_41;  getitem_137 = to_41 = None
            add_20: "bf16[s0, 32, 64]" = mul_82 + mul_83;  mul_82 = mul_83 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_40: "bf16[s0, 32, 128]" = torch.cat((sub_20, add_20), dim = -1);  sub_20 = add_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_41: "bf16[s0, 32, 128]" = torch.cat((cat_40, getitem_136), dim = -1);  cat_40 = getitem_136 = None
            reshape_20: "bf16[s0, 4096]" = cat_41.reshape(size_30);  cat_41 = size_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_31 = getitem_131.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_71: "bf16[s0, 8, 128]" = getitem_131.view(s0, -1, 128);  getitem_131 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_139: "bf16[s0, 8, 128]" = view_71[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_140: "bf16[s0, 8, 0]" = view_71[(Ellipsis, slice(128, None, None))];  view_71 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_42: "bf16[s0, 1, 64]" = getitem_133.unsqueeze(-2);  getitem_133 = None
            to_42: "bf16[s0, 1, 64]" = unsqueeze_42.to(torch.bfloat16);  unsqueeze_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_43: "bf16[s0, 1, 64]" = getitem_134.unsqueeze(-2);  getitem_134 = None
            to_43: "bf16[s0, 1, 64]" = unsqueeze_43.to(torch.bfloat16);  unsqueeze_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_32 = torch.chunk(getitem_139, 2, dim = -1);  getitem_139 = None
            getitem_141: "bf16[s0, 8, 64]" = chunk_32[0]
            getitem_142: "bf16[s0, 8, 64]" = chunk_32[1];  chunk_32 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_84: "bf16[s0, 8, 64]" = getitem_141 * to_42
            mul_85: "bf16[s0, 8, 64]" = getitem_142 * to_43
            sub_21: "bf16[s0, 8, 64]" = mul_84 - mul_85;  mul_84 = mul_85 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_86: "bf16[s0, 8, 64]" = getitem_142 * to_42;  getitem_142 = to_42 = None
            mul_87: "bf16[s0, 8, 64]" = getitem_141 * to_43;  getitem_141 = to_43 = None
            add_21: "bf16[s0, 8, 64]" = mul_86 + mul_87;  mul_86 = mul_87 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_42: "bf16[s0, 8, 128]" = torch.cat((sub_21, add_21), dim = -1);  sub_21 = add_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_43: "bf16[s0, 8, 128]" = torch.cat((cat_42, getitem_140), dim = -1);  cat_42 = getitem_140 = None
            reshape_21: "bf16[s0, 1024]" = cat_43.reshape(size_31);  cat_43 = size_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_32 = reshape_20.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_10: "bf16[s0, 4096]" = torch.zeros(size_32, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_32 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_72: "bf16[s0, 32, 128]" = reshape_20.view(-1, 32, 128);  reshape_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_73: "bf16[s0, 32, 128]" = zeros_10.view(-1, 32, 128);  zeros_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_74: "bf16[s0, 8, 128]" = reshape_21.view(-1, 8, 128);  reshape_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_75: "bf16[s0, 8, 128]" = getitem_132.view(-1, 8, 128);  getitem_132 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_10 = torch.ops.vllm.unified_attention_with_output(view_72, view_74, view_75, view_73, 'model.layers.10.self_attn.attn');  view_72 = view_74 = view_75 = unified_attention_with_output_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_76: "bf16[s0, 4096]" = view_73.view(-1, 4096);  view_73 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_41: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_76, l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_76 = l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_21: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_41: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_41)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_42: "bf16[s0, 4096]" = torch.empty_like(empty_like_40)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_20 = torch.ops._C.fused_add_rms_norm(empty_like_41, rocm_unquantized_gemm_impl_41, empty_like_42, empty_like_40, _get_data_attr_21, 1e-05);  rocm_unquantized_gemm_impl_41 = empty_like_40 = _get_data_attr_21 = fused_add_rms_norm_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_42: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_41, l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_41 = l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_10: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_10 = torch.ops._C.silu_and_mul(empty_10, rocm_unquantized_gemm_impl_42);  rocm_unquantized_gemm_impl_42 = silu_and_mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_43: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_10, l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_10 = l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_22: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_43: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_43)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_44: "bf16[s0, 4096]" = torch.empty_like(empty_like_42)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_21 = torch.ops._C.fused_add_rms_norm(empty_like_43, rocm_unquantized_gemm_impl_43, empty_like_44, empty_like_42, _get_data_attr_22, 1e-05);  rocm_unquantized_gemm_impl_43 = empty_like_42 = _get_data_attr_22 = fused_add_rms_norm_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_44: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_43, l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_43 = l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_11 = rocm_unquantized_gemm_impl_44.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_44 = None
            getitem_143: "bf16[s0, 4096]" = split_11[0]
            getitem_144: "bf16[s0, 1024]" = split_11[1]
            getitem_145: "bf16[s0, 1024]" = split_11[2];  split_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_11: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_11: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_11);  flatten_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_33 = index_select_11.chunk(2, dim = -1);  index_select_11 = None
            getitem_146: "bf16[s0, 64]" = chunk_33[0]
            getitem_147: "bf16[s0, 64]" = chunk_33[1];  chunk_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_33 = getitem_143.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_77: "bf16[s0, 32, 128]" = getitem_143.view(s0, -1, 128);  getitem_143 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_148: "bf16[s0, 32, 128]" = view_77[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_149: "bf16[s0, 32, 0]" = view_77[(Ellipsis, slice(128, None, None))];  view_77 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_44: "bf16[s0, 1, 64]" = getitem_146.unsqueeze(-2)
            to_44: "bf16[s0, 1, 64]" = unsqueeze_44.to(torch.bfloat16);  unsqueeze_44 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_45: "bf16[s0, 1, 64]" = getitem_147.unsqueeze(-2)
            to_45: "bf16[s0, 1, 64]" = unsqueeze_45.to(torch.bfloat16);  unsqueeze_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_34 = torch.chunk(getitem_148, 2, dim = -1);  getitem_148 = None
            getitem_150: "bf16[s0, 32, 64]" = chunk_34[0]
            getitem_151: "bf16[s0, 32, 64]" = chunk_34[1];  chunk_34 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_88: "bf16[s0, 32, 64]" = getitem_150 * to_44
            mul_89: "bf16[s0, 32, 64]" = getitem_151 * to_45
            sub_22: "bf16[s0, 32, 64]" = mul_88 - mul_89;  mul_88 = mul_89 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_90: "bf16[s0, 32, 64]" = getitem_151 * to_44;  getitem_151 = to_44 = None
            mul_91: "bf16[s0, 32, 64]" = getitem_150 * to_45;  getitem_150 = to_45 = None
            add_22: "bf16[s0, 32, 64]" = mul_90 + mul_91;  mul_90 = mul_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_44: "bf16[s0, 32, 128]" = torch.cat((sub_22, add_22), dim = -1);  sub_22 = add_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_45: "bf16[s0, 32, 128]" = torch.cat((cat_44, getitem_149), dim = -1);  cat_44 = getitem_149 = None
            reshape_22: "bf16[s0, 4096]" = cat_45.reshape(size_33);  cat_45 = size_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_34 = getitem_144.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_78: "bf16[s0, 8, 128]" = getitem_144.view(s0, -1, 128);  getitem_144 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_152: "bf16[s0, 8, 128]" = view_78[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_153: "bf16[s0, 8, 0]" = view_78[(Ellipsis, slice(128, None, None))];  view_78 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_46: "bf16[s0, 1, 64]" = getitem_146.unsqueeze(-2);  getitem_146 = None
            to_46: "bf16[s0, 1, 64]" = unsqueeze_46.to(torch.bfloat16);  unsqueeze_46 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_47: "bf16[s0, 1, 64]" = getitem_147.unsqueeze(-2);  getitem_147 = None
            to_47: "bf16[s0, 1, 64]" = unsqueeze_47.to(torch.bfloat16);  unsqueeze_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_35 = torch.chunk(getitem_152, 2, dim = -1);  getitem_152 = None
            getitem_154: "bf16[s0, 8, 64]" = chunk_35[0]
            getitem_155: "bf16[s0, 8, 64]" = chunk_35[1];  chunk_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_92: "bf16[s0, 8, 64]" = getitem_154 * to_46
            mul_93: "bf16[s0, 8, 64]" = getitem_155 * to_47
            sub_23: "bf16[s0, 8, 64]" = mul_92 - mul_93;  mul_92 = mul_93 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_94: "bf16[s0, 8, 64]" = getitem_155 * to_46;  getitem_155 = to_46 = None
            mul_95: "bf16[s0, 8, 64]" = getitem_154 * to_47;  getitem_154 = to_47 = None
            add_23: "bf16[s0, 8, 64]" = mul_94 + mul_95;  mul_94 = mul_95 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_46: "bf16[s0, 8, 128]" = torch.cat((sub_23, add_23), dim = -1);  sub_23 = add_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_47: "bf16[s0, 8, 128]" = torch.cat((cat_46, getitem_153), dim = -1);  cat_46 = getitem_153 = None
            reshape_23: "bf16[s0, 1024]" = cat_47.reshape(size_34);  cat_47 = size_34 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_35 = reshape_22.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_11: "bf16[s0, 4096]" = torch.zeros(size_35, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_79: "bf16[s0, 32, 128]" = reshape_22.view(-1, 32, 128);  reshape_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_80: "bf16[s0, 32, 128]" = zeros_11.view(-1, 32, 128);  zeros_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_81: "bf16[s0, 8, 128]" = reshape_23.view(-1, 8, 128);  reshape_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_82: "bf16[s0, 8, 128]" = getitem_145.view(-1, 8, 128);  getitem_145 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_11 = torch.ops.vllm.unified_attention_with_output(view_79, view_81, view_82, view_80, 'model.layers.11.self_attn.attn');  view_79 = view_81 = view_82 = unified_attention_with_output_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_83: "bf16[s0, 4096]" = view_80.view(-1, 4096);  view_80 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_45: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_83, l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_83 = l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_23: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_45: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_45)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_46: "bf16[s0, 4096]" = torch.empty_like(empty_like_44)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_22 = torch.ops._C.fused_add_rms_norm(empty_like_45, rocm_unquantized_gemm_impl_45, empty_like_46, empty_like_44, _get_data_attr_23, 1e-05);  rocm_unquantized_gemm_impl_45 = empty_like_44 = _get_data_attr_23 = fused_add_rms_norm_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_46: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_45, l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_45 = l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_11: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_11 = torch.ops._C.silu_and_mul(empty_11, rocm_unquantized_gemm_impl_46);  rocm_unquantized_gemm_impl_46 = silu_and_mul_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_47: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_11, l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_11 = l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_24: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_47: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_47)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_48: "bf16[s0, 4096]" = torch.empty_like(empty_like_46)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_23 = torch.ops._C.fused_add_rms_norm(empty_like_47, rocm_unquantized_gemm_impl_47, empty_like_48, empty_like_46, _get_data_attr_24, 1e-05);  rocm_unquantized_gemm_impl_47 = empty_like_46 = _get_data_attr_24 = fused_add_rms_norm_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_48: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_47, l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_47 = l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_12 = rocm_unquantized_gemm_impl_48.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_48 = None
            getitem_156: "bf16[s0, 4096]" = split_12[0]
            getitem_157: "bf16[s0, 1024]" = split_12[1]
            getitem_158: "bf16[s0, 1024]" = split_12[2];  split_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_12: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_12: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_12);  flatten_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_36 = index_select_12.chunk(2, dim = -1);  index_select_12 = None
            getitem_159: "bf16[s0, 64]" = chunk_36[0]
            getitem_160: "bf16[s0, 64]" = chunk_36[1];  chunk_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_36 = getitem_156.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_84: "bf16[s0, 32, 128]" = getitem_156.view(s0, -1, 128);  getitem_156 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_161: "bf16[s0, 32, 128]" = view_84[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_162: "bf16[s0, 32, 0]" = view_84[(Ellipsis, slice(128, None, None))];  view_84 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_48: "bf16[s0, 1, 64]" = getitem_159.unsqueeze(-2)
            to_48: "bf16[s0, 1, 64]" = unsqueeze_48.to(torch.bfloat16);  unsqueeze_48 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_49: "bf16[s0, 1, 64]" = getitem_160.unsqueeze(-2)
            to_49: "bf16[s0, 1, 64]" = unsqueeze_49.to(torch.bfloat16);  unsqueeze_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_37 = torch.chunk(getitem_161, 2, dim = -1);  getitem_161 = None
            getitem_163: "bf16[s0, 32, 64]" = chunk_37[0]
            getitem_164: "bf16[s0, 32, 64]" = chunk_37[1];  chunk_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_96: "bf16[s0, 32, 64]" = getitem_163 * to_48
            mul_97: "bf16[s0, 32, 64]" = getitem_164 * to_49
            sub_24: "bf16[s0, 32, 64]" = mul_96 - mul_97;  mul_96 = mul_97 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_98: "bf16[s0, 32, 64]" = getitem_164 * to_48;  getitem_164 = to_48 = None
            mul_99: "bf16[s0, 32, 64]" = getitem_163 * to_49;  getitem_163 = to_49 = None
            add_24: "bf16[s0, 32, 64]" = mul_98 + mul_99;  mul_98 = mul_99 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_48: "bf16[s0, 32, 128]" = torch.cat((sub_24, add_24), dim = -1);  sub_24 = add_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_49: "bf16[s0, 32, 128]" = torch.cat((cat_48, getitem_162), dim = -1);  cat_48 = getitem_162 = None
            reshape_24: "bf16[s0, 4096]" = cat_49.reshape(size_36);  cat_49 = size_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_37 = getitem_157.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_85: "bf16[s0, 8, 128]" = getitem_157.view(s0, -1, 128);  getitem_157 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_165: "bf16[s0, 8, 128]" = view_85[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_166: "bf16[s0, 8, 0]" = view_85[(Ellipsis, slice(128, None, None))];  view_85 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_50: "bf16[s0, 1, 64]" = getitem_159.unsqueeze(-2);  getitem_159 = None
            to_50: "bf16[s0, 1, 64]" = unsqueeze_50.to(torch.bfloat16);  unsqueeze_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_51: "bf16[s0, 1, 64]" = getitem_160.unsqueeze(-2);  getitem_160 = None
            to_51: "bf16[s0, 1, 64]" = unsqueeze_51.to(torch.bfloat16);  unsqueeze_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_38 = torch.chunk(getitem_165, 2, dim = -1);  getitem_165 = None
            getitem_167: "bf16[s0, 8, 64]" = chunk_38[0]
            getitem_168: "bf16[s0, 8, 64]" = chunk_38[1];  chunk_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_100: "bf16[s0, 8, 64]" = getitem_167 * to_50
            mul_101: "bf16[s0, 8, 64]" = getitem_168 * to_51
            sub_25: "bf16[s0, 8, 64]" = mul_100 - mul_101;  mul_100 = mul_101 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_102: "bf16[s0, 8, 64]" = getitem_168 * to_50;  getitem_168 = to_50 = None
            mul_103: "bf16[s0, 8, 64]" = getitem_167 * to_51;  getitem_167 = to_51 = None
            add_25: "bf16[s0, 8, 64]" = mul_102 + mul_103;  mul_102 = mul_103 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_50: "bf16[s0, 8, 128]" = torch.cat((sub_25, add_25), dim = -1);  sub_25 = add_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_51: "bf16[s0, 8, 128]" = torch.cat((cat_50, getitem_166), dim = -1);  cat_50 = getitem_166 = None
            reshape_25: "bf16[s0, 1024]" = cat_51.reshape(size_37);  cat_51 = size_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_38 = reshape_24.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_12: "bf16[s0, 4096]" = torch.zeros(size_38, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_86: "bf16[s0, 32, 128]" = reshape_24.view(-1, 32, 128);  reshape_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_87: "bf16[s0, 32, 128]" = zeros_12.view(-1, 32, 128);  zeros_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_88: "bf16[s0, 8, 128]" = reshape_25.view(-1, 8, 128);  reshape_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_89: "bf16[s0, 8, 128]" = getitem_158.view(-1, 8, 128);  getitem_158 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_12 = torch.ops.vllm.unified_attention_with_output(view_86, view_88, view_89, view_87, 'model.layers.12.self_attn.attn');  view_86 = view_88 = view_89 = unified_attention_with_output_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_90: "bf16[s0, 4096]" = view_87.view(-1, 4096);  view_87 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_49: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_90, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_90 = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_25: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_49: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_49)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_50: "bf16[s0, 4096]" = torch.empty_like(empty_like_48)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_24 = torch.ops._C.fused_add_rms_norm(empty_like_49, rocm_unquantized_gemm_impl_49, empty_like_50, empty_like_48, _get_data_attr_25, 1e-05);  rocm_unquantized_gemm_impl_49 = empty_like_48 = _get_data_attr_25 = fused_add_rms_norm_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_50: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_49, l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_49 = l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_12: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_12 = torch.ops._C.silu_and_mul(empty_12, rocm_unquantized_gemm_impl_50);  rocm_unquantized_gemm_impl_50 = silu_and_mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_51: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_12, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_12 = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_26: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_51: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_51)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_52: "bf16[s0, 4096]" = torch.empty_like(empty_like_50)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_25 = torch.ops._C.fused_add_rms_norm(empty_like_51, rocm_unquantized_gemm_impl_51, empty_like_52, empty_like_50, _get_data_attr_26, 1e-05);  rocm_unquantized_gemm_impl_51 = empty_like_50 = _get_data_attr_26 = fused_add_rms_norm_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_52: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_51, l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_51 = l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_13 = rocm_unquantized_gemm_impl_52.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_52 = None
            getitem_169: "bf16[s0, 4096]" = split_13[0]
            getitem_170: "bf16[s0, 1024]" = split_13[1]
            getitem_171: "bf16[s0, 1024]" = split_13[2];  split_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_13: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_13: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_13);  flatten_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_39 = index_select_13.chunk(2, dim = -1);  index_select_13 = None
            getitem_172: "bf16[s0, 64]" = chunk_39[0]
            getitem_173: "bf16[s0, 64]" = chunk_39[1];  chunk_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_39 = getitem_169.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_91: "bf16[s0, 32, 128]" = getitem_169.view(s0, -1, 128);  getitem_169 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_174: "bf16[s0, 32, 128]" = view_91[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_175: "bf16[s0, 32, 0]" = view_91[(Ellipsis, slice(128, None, None))];  view_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_52: "bf16[s0, 1, 64]" = getitem_172.unsqueeze(-2)
            to_52: "bf16[s0, 1, 64]" = unsqueeze_52.to(torch.bfloat16);  unsqueeze_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_53: "bf16[s0, 1, 64]" = getitem_173.unsqueeze(-2)
            to_53: "bf16[s0, 1, 64]" = unsqueeze_53.to(torch.bfloat16);  unsqueeze_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_40 = torch.chunk(getitem_174, 2, dim = -1);  getitem_174 = None
            getitem_176: "bf16[s0, 32, 64]" = chunk_40[0]
            getitem_177: "bf16[s0, 32, 64]" = chunk_40[1];  chunk_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_104: "bf16[s0, 32, 64]" = getitem_176 * to_52
            mul_105: "bf16[s0, 32, 64]" = getitem_177 * to_53
            sub_26: "bf16[s0, 32, 64]" = mul_104 - mul_105;  mul_104 = mul_105 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_106: "bf16[s0, 32, 64]" = getitem_177 * to_52;  getitem_177 = to_52 = None
            mul_107: "bf16[s0, 32, 64]" = getitem_176 * to_53;  getitem_176 = to_53 = None
            add_26: "bf16[s0, 32, 64]" = mul_106 + mul_107;  mul_106 = mul_107 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_52: "bf16[s0, 32, 128]" = torch.cat((sub_26, add_26), dim = -1);  sub_26 = add_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_53: "bf16[s0, 32, 128]" = torch.cat((cat_52, getitem_175), dim = -1);  cat_52 = getitem_175 = None
            reshape_26: "bf16[s0, 4096]" = cat_53.reshape(size_39);  cat_53 = size_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_40 = getitem_170.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_92: "bf16[s0, 8, 128]" = getitem_170.view(s0, -1, 128);  getitem_170 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_178: "bf16[s0, 8, 128]" = view_92[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_179: "bf16[s0, 8, 0]" = view_92[(Ellipsis, slice(128, None, None))];  view_92 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_54: "bf16[s0, 1, 64]" = getitem_172.unsqueeze(-2);  getitem_172 = None
            to_54: "bf16[s0, 1, 64]" = unsqueeze_54.to(torch.bfloat16);  unsqueeze_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_55: "bf16[s0, 1, 64]" = getitem_173.unsqueeze(-2);  getitem_173 = None
            to_55: "bf16[s0, 1, 64]" = unsqueeze_55.to(torch.bfloat16);  unsqueeze_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_41 = torch.chunk(getitem_178, 2, dim = -1);  getitem_178 = None
            getitem_180: "bf16[s0, 8, 64]" = chunk_41[0]
            getitem_181: "bf16[s0, 8, 64]" = chunk_41[1];  chunk_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_108: "bf16[s0, 8, 64]" = getitem_180 * to_54
            mul_109: "bf16[s0, 8, 64]" = getitem_181 * to_55
            sub_27: "bf16[s0, 8, 64]" = mul_108 - mul_109;  mul_108 = mul_109 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_110: "bf16[s0, 8, 64]" = getitem_181 * to_54;  getitem_181 = to_54 = None
            mul_111: "bf16[s0, 8, 64]" = getitem_180 * to_55;  getitem_180 = to_55 = None
            add_27: "bf16[s0, 8, 64]" = mul_110 + mul_111;  mul_110 = mul_111 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_54: "bf16[s0, 8, 128]" = torch.cat((sub_27, add_27), dim = -1);  sub_27 = add_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_55: "bf16[s0, 8, 128]" = torch.cat((cat_54, getitem_179), dim = -1);  cat_54 = getitem_179 = None
            reshape_27: "bf16[s0, 1024]" = cat_55.reshape(size_40);  cat_55 = size_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_41 = reshape_26.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_13: "bf16[s0, 4096]" = torch.zeros(size_41, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_93: "bf16[s0, 32, 128]" = reshape_26.view(-1, 32, 128);  reshape_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_94: "bf16[s0, 32, 128]" = zeros_13.view(-1, 32, 128);  zeros_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_95: "bf16[s0, 8, 128]" = reshape_27.view(-1, 8, 128);  reshape_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_96: "bf16[s0, 8, 128]" = getitem_171.view(-1, 8, 128);  getitem_171 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_13 = torch.ops.vllm.unified_attention_with_output(view_93, view_95, view_96, view_94, 'model.layers.13.self_attn.attn');  view_93 = view_95 = view_96 = unified_attention_with_output_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_97: "bf16[s0, 4096]" = view_94.view(-1, 4096);  view_94 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_53: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_97, l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_97 = l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_27: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_53: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_53)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_54: "bf16[s0, 4096]" = torch.empty_like(empty_like_52)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_26 = torch.ops._C.fused_add_rms_norm(empty_like_53, rocm_unquantized_gemm_impl_53, empty_like_54, empty_like_52, _get_data_attr_27, 1e-05);  rocm_unquantized_gemm_impl_53 = empty_like_52 = _get_data_attr_27 = fused_add_rms_norm_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_54: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_53, l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_53 = l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_13: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_13 = torch.ops._C.silu_and_mul(empty_13, rocm_unquantized_gemm_impl_54);  rocm_unquantized_gemm_impl_54 = silu_and_mul_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_55: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_13, l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_13 = l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_28: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_55: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_55)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_56: "bf16[s0, 4096]" = torch.empty_like(empty_like_54)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_27 = torch.ops._C.fused_add_rms_norm(empty_like_55, rocm_unquantized_gemm_impl_55, empty_like_56, empty_like_54, _get_data_attr_28, 1e-05);  rocm_unquantized_gemm_impl_55 = empty_like_54 = _get_data_attr_28 = fused_add_rms_norm_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_56: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_55, l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_55 = l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_14 = rocm_unquantized_gemm_impl_56.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_56 = None
            getitem_182: "bf16[s0, 4096]" = split_14[0]
            getitem_183: "bf16[s0, 1024]" = split_14[1]
            getitem_184: "bf16[s0, 1024]" = split_14[2];  split_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_14: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_14: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_14);  flatten_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_42 = index_select_14.chunk(2, dim = -1);  index_select_14 = None
            getitem_185: "bf16[s0, 64]" = chunk_42[0]
            getitem_186: "bf16[s0, 64]" = chunk_42[1];  chunk_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_42 = getitem_182.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_98: "bf16[s0, 32, 128]" = getitem_182.view(s0, -1, 128);  getitem_182 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_187: "bf16[s0, 32, 128]" = view_98[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_188: "bf16[s0, 32, 0]" = view_98[(Ellipsis, slice(128, None, None))];  view_98 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_56: "bf16[s0, 1, 64]" = getitem_185.unsqueeze(-2)
            to_56: "bf16[s0, 1, 64]" = unsqueeze_56.to(torch.bfloat16);  unsqueeze_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_57: "bf16[s0, 1, 64]" = getitem_186.unsqueeze(-2)
            to_57: "bf16[s0, 1, 64]" = unsqueeze_57.to(torch.bfloat16);  unsqueeze_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_43 = torch.chunk(getitem_187, 2, dim = -1);  getitem_187 = None
            getitem_189: "bf16[s0, 32, 64]" = chunk_43[0]
            getitem_190: "bf16[s0, 32, 64]" = chunk_43[1];  chunk_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_112: "bf16[s0, 32, 64]" = getitem_189 * to_56
            mul_113: "bf16[s0, 32, 64]" = getitem_190 * to_57
            sub_28: "bf16[s0, 32, 64]" = mul_112 - mul_113;  mul_112 = mul_113 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_114: "bf16[s0, 32, 64]" = getitem_190 * to_56;  getitem_190 = to_56 = None
            mul_115: "bf16[s0, 32, 64]" = getitem_189 * to_57;  getitem_189 = to_57 = None
            add_28: "bf16[s0, 32, 64]" = mul_114 + mul_115;  mul_114 = mul_115 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_56: "bf16[s0, 32, 128]" = torch.cat((sub_28, add_28), dim = -1);  sub_28 = add_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_57: "bf16[s0, 32, 128]" = torch.cat((cat_56, getitem_188), dim = -1);  cat_56 = getitem_188 = None
            reshape_28: "bf16[s0, 4096]" = cat_57.reshape(size_42);  cat_57 = size_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_43 = getitem_183.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_99: "bf16[s0, 8, 128]" = getitem_183.view(s0, -1, 128);  getitem_183 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_191: "bf16[s0, 8, 128]" = view_99[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_192: "bf16[s0, 8, 0]" = view_99[(Ellipsis, slice(128, None, None))];  view_99 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_58: "bf16[s0, 1, 64]" = getitem_185.unsqueeze(-2);  getitem_185 = None
            to_58: "bf16[s0, 1, 64]" = unsqueeze_58.to(torch.bfloat16);  unsqueeze_58 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_59: "bf16[s0, 1, 64]" = getitem_186.unsqueeze(-2);  getitem_186 = None
            to_59: "bf16[s0, 1, 64]" = unsqueeze_59.to(torch.bfloat16);  unsqueeze_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_44 = torch.chunk(getitem_191, 2, dim = -1);  getitem_191 = None
            getitem_193: "bf16[s0, 8, 64]" = chunk_44[0]
            getitem_194: "bf16[s0, 8, 64]" = chunk_44[1];  chunk_44 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_116: "bf16[s0, 8, 64]" = getitem_193 * to_58
            mul_117: "bf16[s0, 8, 64]" = getitem_194 * to_59
            sub_29: "bf16[s0, 8, 64]" = mul_116 - mul_117;  mul_116 = mul_117 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_118: "bf16[s0, 8, 64]" = getitem_194 * to_58;  getitem_194 = to_58 = None
            mul_119: "bf16[s0, 8, 64]" = getitem_193 * to_59;  getitem_193 = to_59 = None
            add_29: "bf16[s0, 8, 64]" = mul_118 + mul_119;  mul_118 = mul_119 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_58: "bf16[s0, 8, 128]" = torch.cat((sub_29, add_29), dim = -1);  sub_29 = add_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_59: "bf16[s0, 8, 128]" = torch.cat((cat_58, getitem_192), dim = -1);  cat_58 = getitem_192 = None
            reshape_29: "bf16[s0, 1024]" = cat_59.reshape(size_43);  cat_59 = size_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_44 = reshape_28.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_14: "bf16[s0, 4096]" = torch.zeros(size_44, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_44 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_100: "bf16[s0, 32, 128]" = reshape_28.view(-1, 32, 128);  reshape_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_101: "bf16[s0, 32, 128]" = zeros_14.view(-1, 32, 128);  zeros_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_102: "bf16[s0, 8, 128]" = reshape_29.view(-1, 8, 128);  reshape_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_103: "bf16[s0, 8, 128]" = getitem_184.view(-1, 8, 128);  getitem_184 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_14 = torch.ops.vllm.unified_attention_with_output(view_100, view_102, view_103, view_101, 'model.layers.14.self_attn.attn');  view_100 = view_102 = view_103 = unified_attention_with_output_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_104: "bf16[s0, 4096]" = view_101.view(-1, 4096);  view_101 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_57: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_104, l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_104 = l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_29: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_57: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_57)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_58: "bf16[s0, 4096]" = torch.empty_like(empty_like_56)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_28 = torch.ops._C.fused_add_rms_norm(empty_like_57, rocm_unquantized_gemm_impl_57, empty_like_58, empty_like_56, _get_data_attr_29, 1e-05);  rocm_unquantized_gemm_impl_57 = empty_like_56 = _get_data_attr_29 = fused_add_rms_norm_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_58: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_57, l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_57 = l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_14: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_14 = torch.ops._C.silu_and_mul(empty_14, rocm_unquantized_gemm_impl_58);  rocm_unquantized_gemm_impl_58 = silu_and_mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_59: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_14, l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_14 = l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_30: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_59: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_59)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_60: "bf16[s0, 4096]" = torch.empty_like(empty_like_58)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_29 = torch.ops._C.fused_add_rms_norm(empty_like_59, rocm_unquantized_gemm_impl_59, empty_like_60, empty_like_58, _get_data_attr_30, 1e-05);  rocm_unquantized_gemm_impl_59 = empty_like_58 = _get_data_attr_30 = fused_add_rms_norm_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_60: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_59, l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_59 = l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_15 = rocm_unquantized_gemm_impl_60.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_60 = None
            getitem_195: "bf16[s0, 4096]" = split_15[0]
            getitem_196: "bf16[s0, 1024]" = split_15[1]
            getitem_197: "bf16[s0, 1024]" = split_15[2];  split_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_15: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_15: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_15);  flatten_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_45 = index_select_15.chunk(2, dim = -1);  index_select_15 = None
            getitem_198: "bf16[s0, 64]" = chunk_45[0]
            getitem_199: "bf16[s0, 64]" = chunk_45[1];  chunk_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_45 = getitem_195.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_105: "bf16[s0, 32, 128]" = getitem_195.view(s0, -1, 128);  getitem_195 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_200: "bf16[s0, 32, 128]" = view_105[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_201: "bf16[s0, 32, 0]" = view_105[(Ellipsis, slice(128, None, None))];  view_105 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_60: "bf16[s0, 1, 64]" = getitem_198.unsqueeze(-2)
            to_60: "bf16[s0, 1, 64]" = unsqueeze_60.to(torch.bfloat16);  unsqueeze_60 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_61: "bf16[s0, 1, 64]" = getitem_199.unsqueeze(-2)
            to_61: "bf16[s0, 1, 64]" = unsqueeze_61.to(torch.bfloat16);  unsqueeze_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_46 = torch.chunk(getitem_200, 2, dim = -1);  getitem_200 = None
            getitem_202: "bf16[s0, 32, 64]" = chunk_46[0]
            getitem_203: "bf16[s0, 32, 64]" = chunk_46[1];  chunk_46 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_120: "bf16[s0, 32, 64]" = getitem_202 * to_60
            mul_121: "bf16[s0, 32, 64]" = getitem_203 * to_61
            sub_30: "bf16[s0, 32, 64]" = mul_120 - mul_121;  mul_120 = mul_121 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_122: "bf16[s0, 32, 64]" = getitem_203 * to_60;  getitem_203 = to_60 = None
            mul_123: "bf16[s0, 32, 64]" = getitem_202 * to_61;  getitem_202 = to_61 = None
            add_30: "bf16[s0, 32, 64]" = mul_122 + mul_123;  mul_122 = mul_123 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_60: "bf16[s0, 32, 128]" = torch.cat((sub_30, add_30), dim = -1);  sub_30 = add_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_61: "bf16[s0, 32, 128]" = torch.cat((cat_60, getitem_201), dim = -1);  cat_60 = getitem_201 = None
            reshape_30: "bf16[s0, 4096]" = cat_61.reshape(size_45);  cat_61 = size_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_46 = getitem_196.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_106: "bf16[s0, 8, 128]" = getitem_196.view(s0, -1, 128);  getitem_196 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_204: "bf16[s0, 8, 128]" = view_106[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_205: "bf16[s0, 8, 0]" = view_106[(Ellipsis, slice(128, None, None))];  view_106 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_62: "bf16[s0, 1, 64]" = getitem_198.unsqueeze(-2);  getitem_198 = None
            to_62: "bf16[s0, 1, 64]" = unsqueeze_62.to(torch.bfloat16);  unsqueeze_62 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_63: "bf16[s0, 1, 64]" = getitem_199.unsqueeze(-2);  getitem_199 = None
            to_63: "bf16[s0, 1, 64]" = unsqueeze_63.to(torch.bfloat16);  unsqueeze_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_47 = torch.chunk(getitem_204, 2, dim = -1);  getitem_204 = None
            getitem_206: "bf16[s0, 8, 64]" = chunk_47[0]
            getitem_207: "bf16[s0, 8, 64]" = chunk_47[1];  chunk_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_124: "bf16[s0, 8, 64]" = getitem_206 * to_62
            mul_125: "bf16[s0, 8, 64]" = getitem_207 * to_63
            sub_31: "bf16[s0, 8, 64]" = mul_124 - mul_125;  mul_124 = mul_125 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_126: "bf16[s0, 8, 64]" = getitem_207 * to_62;  getitem_207 = to_62 = None
            mul_127: "bf16[s0, 8, 64]" = getitem_206 * to_63;  getitem_206 = to_63 = None
            add_31: "bf16[s0, 8, 64]" = mul_126 + mul_127;  mul_126 = mul_127 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_62: "bf16[s0, 8, 128]" = torch.cat((sub_31, add_31), dim = -1);  sub_31 = add_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_63: "bf16[s0, 8, 128]" = torch.cat((cat_62, getitem_205), dim = -1);  cat_62 = getitem_205 = None
            reshape_31: "bf16[s0, 1024]" = cat_63.reshape(size_46);  cat_63 = size_46 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_47 = reshape_30.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_15: "bf16[s0, 4096]" = torch.zeros(size_47, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_107: "bf16[s0, 32, 128]" = reshape_30.view(-1, 32, 128);  reshape_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_108: "bf16[s0, 32, 128]" = zeros_15.view(-1, 32, 128);  zeros_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_109: "bf16[s0, 8, 128]" = reshape_31.view(-1, 8, 128);  reshape_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_110: "bf16[s0, 8, 128]" = getitem_197.view(-1, 8, 128);  getitem_197 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_15 = torch.ops.vllm.unified_attention_with_output(view_107, view_109, view_110, view_108, 'model.layers.15.self_attn.attn');  view_107 = view_109 = view_110 = unified_attention_with_output_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_111: "bf16[s0, 4096]" = view_108.view(-1, 4096);  view_108 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_61: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_111, l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_111 = l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_31: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_61: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_61)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_62: "bf16[s0, 4096]" = torch.empty_like(empty_like_60)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_30 = torch.ops._C.fused_add_rms_norm(empty_like_61, rocm_unquantized_gemm_impl_61, empty_like_62, empty_like_60, _get_data_attr_31, 1e-05);  rocm_unquantized_gemm_impl_61 = empty_like_60 = _get_data_attr_31 = fused_add_rms_norm_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_62: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_61, l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_61 = l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_15: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_15 = torch.ops._C.silu_and_mul(empty_15, rocm_unquantized_gemm_impl_62);  rocm_unquantized_gemm_impl_62 = silu_and_mul_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_63: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_15, l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_15 = l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_32: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_63: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_63)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_64: "bf16[s0, 4096]" = torch.empty_like(empty_like_62)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_31 = torch.ops._C.fused_add_rms_norm(empty_like_63, rocm_unquantized_gemm_impl_63, empty_like_64, empty_like_62, _get_data_attr_32, 1e-05);  rocm_unquantized_gemm_impl_63 = empty_like_62 = _get_data_attr_32 = fused_add_rms_norm_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_64: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_63, l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_63 = l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_16 = rocm_unquantized_gemm_impl_64.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_64 = None
            getitem_208: "bf16[s0, 4096]" = split_16[0]
            getitem_209: "bf16[s0, 1024]" = split_16[1]
            getitem_210: "bf16[s0, 1024]" = split_16[2];  split_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_16: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_16: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_16);  flatten_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_48 = index_select_16.chunk(2, dim = -1);  index_select_16 = None
            getitem_211: "bf16[s0, 64]" = chunk_48[0]
            getitem_212: "bf16[s0, 64]" = chunk_48[1];  chunk_48 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_48 = getitem_208.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_112: "bf16[s0, 32, 128]" = getitem_208.view(s0, -1, 128);  getitem_208 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_213: "bf16[s0, 32, 128]" = view_112[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_214: "bf16[s0, 32, 0]" = view_112[(Ellipsis, slice(128, None, None))];  view_112 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_64: "bf16[s0, 1, 64]" = getitem_211.unsqueeze(-2)
            to_64: "bf16[s0, 1, 64]" = unsqueeze_64.to(torch.bfloat16);  unsqueeze_64 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_65: "bf16[s0, 1, 64]" = getitem_212.unsqueeze(-2)
            to_65: "bf16[s0, 1, 64]" = unsqueeze_65.to(torch.bfloat16);  unsqueeze_65 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_49 = torch.chunk(getitem_213, 2, dim = -1);  getitem_213 = None
            getitem_215: "bf16[s0, 32, 64]" = chunk_49[0]
            getitem_216: "bf16[s0, 32, 64]" = chunk_49[1];  chunk_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_128: "bf16[s0, 32, 64]" = getitem_215 * to_64
            mul_129: "bf16[s0, 32, 64]" = getitem_216 * to_65
            sub_32: "bf16[s0, 32, 64]" = mul_128 - mul_129;  mul_128 = mul_129 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_130: "bf16[s0, 32, 64]" = getitem_216 * to_64;  getitem_216 = to_64 = None
            mul_131: "bf16[s0, 32, 64]" = getitem_215 * to_65;  getitem_215 = to_65 = None
            add_32: "bf16[s0, 32, 64]" = mul_130 + mul_131;  mul_130 = mul_131 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_64: "bf16[s0, 32, 128]" = torch.cat((sub_32, add_32), dim = -1);  sub_32 = add_32 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_65: "bf16[s0, 32, 128]" = torch.cat((cat_64, getitem_214), dim = -1);  cat_64 = getitem_214 = None
            reshape_32: "bf16[s0, 4096]" = cat_65.reshape(size_48);  cat_65 = size_48 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_49 = getitem_209.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_113: "bf16[s0, 8, 128]" = getitem_209.view(s0, -1, 128);  getitem_209 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_217: "bf16[s0, 8, 128]" = view_113[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_218: "bf16[s0, 8, 0]" = view_113[(Ellipsis, slice(128, None, None))];  view_113 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_66: "bf16[s0, 1, 64]" = getitem_211.unsqueeze(-2);  getitem_211 = None
            to_66: "bf16[s0, 1, 64]" = unsqueeze_66.to(torch.bfloat16);  unsqueeze_66 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_67: "bf16[s0, 1, 64]" = getitem_212.unsqueeze(-2);  getitem_212 = None
            to_67: "bf16[s0, 1, 64]" = unsqueeze_67.to(torch.bfloat16);  unsqueeze_67 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_50 = torch.chunk(getitem_217, 2, dim = -1);  getitem_217 = None
            getitem_219: "bf16[s0, 8, 64]" = chunk_50[0]
            getitem_220: "bf16[s0, 8, 64]" = chunk_50[1];  chunk_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_132: "bf16[s0, 8, 64]" = getitem_219 * to_66
            mul_133: "bf16[s0, 8, 64]" = getitem_220 * to_67
            sub_33: "bf16[s0, 8, 64]" = mul_132 - mul_133;  mul_132 = mul_133 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_134: "bf16[s0, 8, 64]" = getitem_220 * to_66;  getitem_220 = to_66 = None
            mul_135: "bf16[s0, 8, 64]" = getitem_219 * to_67;  getitem_219 = to_67 = None
            add_33: "bf16[s0, 8, 64]" = mul_134 + mul_135;  mul_134 = mul_135 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_66: "bf16[s0, 8, 128]" = torch.cat((sub_33, add_33), dim = -1);  sub_33 = add_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_67: "bf16[s0, 8, 128]" = torch.cat((cat_66, getitem_218), dim = -1);  cat_66 = getitem_218 = None
            reshape_33: "bf16[s0, 1024]" = cat_67.reshape(size_49);  cat_67 = size_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_50 = reshape_32.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_16: "bf16[s0, 4096]" = torch.zeros(size_50, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_114: "bf16[s0, 32, 128]" = reshape_32.view(-1, 32, 128);  reshape_32 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_115: "bf16[s0, 32, 128]" = zeros_16.view(-1, 32, 128);  zeros_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_116: "bf16[s0, 8, 128]" = reshape_33.view(-1, 8, 128);  reshape_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_117: "bf16[s0, 8, 128]" = getitem_210.view(-1, 8, 128);  getitem_210 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_16 = torch.ops.vllm.unified_attention_with_output(view_114, view_116, view_117, view_115, 'model.layers.16.self_attn.attn');  view_114 = view_116 = view_117 = unified_attention_with_output_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_118: "bf16[s0, 4096]" = view_115.view(-1, 4096);  view_115 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_65: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_118, l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_118 = l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_33: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_65: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_65)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_66: "bf16[s0, 4096]" = torch.empty_like(empty_like_64)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_32 = torch.ops._C.fused_add_rms_norm(empty_like_65, rocm_unquantized_gemm_impl_65, empty_like_66, empty_like_64, _get_data_attr_33, 1e-05);  rocm_unquantized_gemm_impl_65 = empty_like_64 = _get_data_attr_33 = fused_add_rms_norm_32 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_66: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_65, l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_65 = l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_16: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_16 = torch.ops._C.silu_and_mul(empty_16, rocm_unquantized_gemm_impl_66);  rocm_unquantized_gemm_impl_66 = silu_and_mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_67: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_16, l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_16 = l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_34: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_67: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_67)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_68: "bf16[s0, 4096]" = torch.empty_like(empty_like_66)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_33 = torch.ops._C.fused_add_rms_norm(empty_like_67, rocm_unquantized_gemm_impl_67, empty_like_68, empty_like_66, _get_data_attr_34, 1e-05);  rocm_unquantized_gemm_impl_67 = empty_like_66 = _get_data_attr_34 = fused_add_rms_norm_33 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_68: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_67, l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_67 = l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_17 = rocm_unquantized_gemm_impl_68.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_68 = None
            getitem_221: "bf16[s0, 4096]" = split_17[0]
            getitem_222: "bf16[s0, 1024]" = split_17[1]
            getitem_223: "bf16[s0, 1024]" = split_17[2];  split_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_17: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_17: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_17);  flatten_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_51 = index_select_17.chunk(2, dim = -1);  index_select_17 = None
            getitem_224: "bf16[s0, 64]" = chunk_51[0]
            getitem_225: "bf16[s0, 64]" = chunk_51[1];  chunk_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_51 = getitem_221.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_119: "bf16[s0, 32, 128]" = getitem_221.view(s0, -1, 128);  getitem_221 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_226: "bf16[s0, 32, 128]" = view_119[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_227: "bf16[s0, 32, 0]" = view_119[(Ellipsis, slice(128, None, None))];  view_119 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_68: "bf16[s0, 1, 64]" = getitem_224.unsqueeze(-2)
            to_68: "bf16[s0, 1, 64]" = unsqueeze_68.to(torch.bfloat16);  unsqueeze_68 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_69: "bf16[s0, 1, 64]" = getitem_225.unsqueeze(-2)
            to_69: "bf16[s0, 1, 64]" = unsqueeze_69.to(torch.bfloat16);  unsqueeze_69 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_52 = torch.chunk(getitem_226, 2, dim = -1);  getitem_226 = None
            getitem_228: "bf16[s0, 32, 64]" = chunk_52[0]
            getitem_229: "bf16[s0, 32, 64]" = chunk_52[1];  chunk_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_136: "bf16[s0, 32, 64]" = getitem_228 * to_68
            mul_137: "bf16[s0, 32, 64]" = getitem_229 * to_69
            sub_34: "bf16[s0, 32, 64]" = mul_136 - mul_137;  mul_136 = mul_137 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_138: "bf16[s0, 32, 64]" = getitem_229 * to_68;  getitem_229 = to_68 = None
            mul_139: "bf16[s0, 32, 64]" = getitem_228 * to_69;  getitem_228 = to_69 = None
            add_34: "bf16[s0, 32, 64]" = mul_138 + mul_139;  mul_138 = mul_139 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_68: "bf16[s0, 32, 128]" = torch.cat((sub_34, add_34), dim = -1);  sub_34 = add_34 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_69: "bf16[s0, 32, 128]" = torch.cat((cat_68, getitem_227), dim = -1);  cat_68 = getitem_227 = None
            reshape_34: "bf16[s0, 4096]" = cat_69.reshape(size_51);  cat_69 = size_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_52 = getitem_222.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_120: "bf16[s0, 8, 128]" = getitem_222.view(s0, -1, 128);  getitem_222 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_230: "bf16[s0, 8, 128]" = view_120[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_231: "bf16[s0, 8, 0]" = view_120[(Ellipsis, slice(128, None, None))];  view_120 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_70: "bf16[s0, 1, 64]" = getitem_224.unsqueeze(-2);  getitem_224 = None
            to_70: "bf16[s0, 1, 64]" = unsqueeze_70.to(torch.bfloat16);  unsqueeze_70 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_71: "bf16[s0, 1, 64]" = getitem_225.unsqueeze(-2);  getitem_225 = None
            to_71: "bf16[s0, 1, 64]" = unsqueeze_71.to(torch.bfloat16);  unsqueeze_71 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_53 = torch.chunk(getitem_230, 2, dim = -1);  getitem_230 = None
            getitem_232: "bf16[s0, 8, 64]" = chunk_53[0]
            getitem_233: "bf16[s0, 8, 64]" = chunk_53[1];  chunk_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_140: "bf16[s0, 8, 64]" = getitem_232 * to_70
            mul_141: "bf16[s0, 8, 64]" = getitem_233 * to_71
            sub_35: "bf16[s0, 8, 64]" = mul_140 - mul_141;  mul_140 = mul_141 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_142: "bf16[s0, 8, 64]" = getitem_233 * to_70;  getitem_233 = to_70 = None
            mul_143: "bf16[s0, 8, 64]" = getitem_232 * to_71;  getitem_232 = to_71 = None
            add_35: "bf16[s0, 8, 64]" = mul_142 + mul_143;  mul_142 = mul_143 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_70: "bf16[s0, 8, 128]" = torch.cat((sub_35, add_35), dim = -1);  sub_35 = add_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_71: "bf16[s0, 8, 128]" = torch.cat((cat_70, getitem_231), dim = -1);  cat_70 = getitem_231 = None
            reshape_35: "bf16[s0, 1024]" = cat_71.reshape(size_52);  cat_71 = size_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_53 = reshape_34.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_17: "bf16[s0, 4096]" = torch.zeros(size_53, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_121: "bf16[s0, 32, 128]" = reshape_34.view(-1, 32, 128);  reshape_34 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_122: "bf16[s0, 32, 128]" = zeros_17.view(-1, 32, 128);  zeros_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_123: "bf16[s0, 8, 128]" = reshape_35.view(-1, 8, 128);  reshape_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_124: "bf16[s0, 8, 128]" = getitem_223.view(-1, 8, 128);  getitem_223 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_17 = torch.ops.vllm.unified_attention_with_output(view_121, view_123, view_124, view_122, 'model.layers.17.self_attn.attn');  view_121 = view_123 = view_124 = unified_attention_with_output_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_125: "bf16[s0, 4096]" = view_122.view(-1, 4096);  view_122 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_69: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_125, l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_125 = l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_35: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_69: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_69)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_70: "bf16[s0, 4096]" = torch.empty_like(empty_like_68)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_34 = torch.ops._C.fused_add_rms_norm(empty_like_69, rocm_unquantized_gemm_impl_69, empty_like_70, empty_like_68, _get_data_attr_35, 1e-05);  rocm_unquantized_gemm_impl_69 = empty_like_68 = _get_data_attr_35 = fused_add_rms_norm_34 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_70: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_69, l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_69 = l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_17: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_17 = torch.ops._C.silu_and_mul(empty_17, rocm_unquantized_gemm_impl_70);  rocm_unquantized_gemm_impl_70 = silu_and_mul_17 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_71: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_17, l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_17 = l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_36: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_71: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_71)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_72: "bf16[s0, 4096]" = torch.empty_like(empty_like_70)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_35 = torch.ops._C.fused_add_rms_norm(empty_like_71, rocm_unquantized_gemm_impl_71, empty_like_72, empty_like_70, _get_data_attr_36, 1e-05);  rocm_unquantized_gemm_impl_71 = empty_like_70 = _get_data_attr_36 = fused_add_rms_norm_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_72: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_71, l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_71 = l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_18 = rocm_unquantized_gemm_impl_72.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_72 = None
            getitem_234: "bf16[s0, 4096]" = split_18[0]
            getitem_235: "bf16[s0, 1024]" = split_18[1]
            getitem_236: "bf16[s0, 1024]" = split_18[2];  split_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_18: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_18: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_18);  flatten_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_54 = index_select_18.chunk(2, dim = -1);  index_select_18 = None
            getitem_237: "bf16[s0, 64]" = chunk_54[0]
            getitem_238: "bf16[s0, 64]" = chunk_54[1];  chunk_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_54 = getitem_234.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_126: "bf16[s0, 32, 128]" = getitem_234.view(s0, -1, 128);  getitem_234 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_239: "bf16[s0, 32, 128]" = view_126[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_240: "bf16[s0, 32, 0]" = view_126[(Ellipsis, slice(128, None, None))];  view_126 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_72: "bf16[s0, 1, 64]" = getitem_237.unsqueeze(-2)
            to_72: "bf16[s0, 1, 64]" = unsqueeze_72.to(torch.bfloat16);  unsqueeze_72 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_73: "bf16[s0, 1, 64]" = getitem_238.unsqueeze(-2)
            to_73: "bf16[s0, 1, 64]" = unsqueeze_73.to(torch.bfloat16);  unsqueeze_73 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_55 = torch.chunk(getitem_239, 2, dim = -1);  getitem_239 = None
            getitem_241: "bf16[s0, 32, 64]" = chunk_55[0]
            getitem_242: "bf16[s0, 32, 64]" = chunk_55[1];  chunk_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_144: "bf16[s0, 32, 64]" = getitem_241 * to_72
            mul_145: "bf16[s0, 32, 64]" = getitem_242 * to_73
            sub_36: "bf16[s0, 32, 64]" = mul_144 - mul_145;  mul_144 = mul_145 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_146: "bf16[s0, 32, 64]" = getitem_242 * to_72;  getitem_242 = to_72 = None
            mul_147: "bf16[s0, 32, 64]" = getitem_241 * to_73;  getitem_241 = to_73 = None
            add_36: "bf16[s0, 32, 64]" = mul_146 + mul_147;  mul_146 = mul_147 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_72: "bf16[s0, 32, 128]" = torch.cat((sub_36, add_36), dim = -1);  sub_36 = add_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_73: "bf16[s0, 32, 128]" = torch.cat((cat_72, getitem_240), dim = -1);  cat_72 = getitem_240 = None
            reshape_36: "bf16[s0, 4096]" = cat_73.reshape(size_54);  cat_73 = size_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_55 = getitem_235.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_127: "bf16[s0, 8, 128]" = getitem_235.view(s0, -1, 128);  getitem_235 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_243: "bf16[s0, 8, 128]" = view_127[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_244: "bf16[s0, 8, 0]" = view_127[(Ellipsis, slice(128, None, None))];  view_127 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_74: "bf16[s0, 1, 64]" = getitem_237.unsqueeze(-2);  getitem_237 = None
            to_74: "bf16[s0, 1, 64]" = unsqueeze_74.to(torch.bfloat16);  unsqueeze_74 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_75: "bf16[s0, 1, 64]" = getitem_238.unsqueeze(-2);  getitem_238 = None
            to_75: "bf16[s0, 1, 64]" = unsqueeze_75.to(torch.bfloat16);  unsqueeze_75 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_56 = torch.chunk(getitem_243, 2, dim = -1);  getitem_243 = None
            getitem_245: "bf16[s0, 8, 64]" = chunk_56[0]
            getitem_246: "bf16[s0, 8, 64]" = chunk_56[1];  chunk_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_148: "bf16[s0, 8, 64]" = getitem_245 * to_74
            mul_149: "bf16[s0, 8, 64]" = getitem_246 * to_75
            sub_37: "bf16[s0, 8, 64]" = mul_148 - mul_149;  mul_148 = mul_149 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_150: "bf16[s0, 8, 64]" = getitem_246 * to_74;  getitem_246 = to_74 = None
            mul_151: "bf16[s0, 8, 64]" = getitem_245 * to_75;  getitem_245 = to_75 = None
            add_37: "bf16[s0, 8, 64]" = mul_150 + mul_151;  mul_150 = mul_151 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_74: "bf16[s0, 8, 128]" = torch.cat((sub_37, add_37), dim = -1);  sub_37 = add_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_75: "bf16[s0, 8, 128]" = torch.cat((cat_74, getitem_244), dim = -1);  cat_74 = getitem_244 = None
            reshape_37: "bf16[s0, 1024]" = cat_75.reshape(size_55);  cat_75 = size_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_56 = reshape_36.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_18: "bf16[s0, 4096]" = torch.zeros(size_56, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_128: "bf16[s0, 32, 128]" = reshape_36.view(-1, 32, 128);  reshape_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_129: "bf16[s0, 32, 128]" = zeros_18.view(-1, 32, 128);  zeros_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_130: "bf16[s0, 8, 128]" = reshape_37.view(-1, 8, 128);  reshape_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_131: "bf16[s0, 8, 128]" = getitem_236.view(-1, 8, 128);  getitem_236 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_18 = torch.ops.vllm.unified_attention_with_output(view_128, view_130, view_131, view_129, 'model.layers.18.self_attn.attn');  view_128 = view_130 = view_131 = unified_attention_with_output_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_132: "bf16[s0, 4096]" = view_129.view(-1, 4096);  view_129 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_73: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_132, l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_132 = l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_37: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_73: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_73)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_74: "bf16[s0, 4096]" = torch.empty_like(empty_like_72)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_36 = torch.ops._C.fused_add_rms_norm(empty_like_73, rocm_unquantized_gemm_impl_73, empty_like_74, empty_like_72, _get_data_attr_37, 1e-05);  rocm_unquantized_gemm_impl_73 = empty_like_72 = _get_data_attr_37 = fused_add_rms_norm_36 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_74: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_73, l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_73 = l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_18: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_18 = torch.ops._C.silu_and_mul(empty_18, rocm_unquantized_gemm_impl_74);  rocm_unquantized_gemm_impl_74 = silu_and_mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_75: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_18, l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_18 = l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_38: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_75: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_75)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_76: "bf16[s0, 4096]" = torch.empty_like(empty_like_74)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_37 = torch.ops._C.fused_add_rms_norm(empty_like_75, rocm_unquantized_gemm_impl_75, empty_like_76, empty_like_74, _get_data_attr_38, 1e-05);  rocm_unquantized_gemm_impl_75 = empty_like_74 = _get_data_attr_38 = fused_add_rms_norm_37 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_76: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_75, l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_75 = l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_19 = rocm_unquantized_gemm_impl_76.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_76 = None
            getitem_247: "bf16[s0, 4096]" = split_19[0]
            getitem_248: "bf16[s0, 1024]" = split_19[1]
            getitem_249: "bf16[s0, 1024]" = split_19[2];  split_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_19: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_19: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_19);  flatten_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_57 = index_select_19.chunk(2, dim = -1);  index_select_19 = None
            getitem_250: "bf16[s0, 64]" = chunk_57[0]
            getitem_251: "bf16[s0, 64]" = chunk_57[1];  chunk_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_57 = getitem_247.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_133: "bf16[s0, 32, 128]" = getitem_247.view(s0, -1, 128);  getitem_247 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_252: "bf16[s0, 32, 128]" = view_133[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_253: "bf16[s0, 32, 0]" = view_133[(Ellipsis, slice(128, None, None))];  view_133 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_76: "bf16[s0, 1, 64]" = getitem_250.unsqueeze(-2)
            to_76: "bf16[s0, 1, 64]" = unsqueeze_76.to(torch.bfloat16);  unsqueeze_76 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_77: "bf16[s0, 1, 64]" = getitem_251.unsqueeze(-2)
            to_77: "bf16[s0, 1, 64]" = unsqueeze_77.to(torch.bfloat16);  unsqueeze_77 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_58 = torch.chunk(getitem_252, 2, dim = -1);  getitem_252 = None
            getitem_254: "bf16[s0, 32, 64]" = chunk_58[0]
            getitem_255: "bf16[s0, 32, 64]" = chunk_58[1];  chunk_58 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_152: "bf16[s0, 32, 64]" = getitem_254 * to_76
            mul_153: "bf16[s0, 32, 64]" = getitem_255 * to_77
            sub_38: "bf16[s0, 32, 64]" = mul_152 - mul_153;  mul_152 = mul_153 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_154: "bf16[s0, 32, 64]" = getitem_255 * to_76;  getitem_255 = to_76 = None
            mul_155: "bf16[s0, 32, 64]" = getitem_254 * to_77;  getitem_254 = to_77 = None
            add_38: "bf16[s0, 32, 64]" = mul_154 + mul_155;  mul_154 = mul_155 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_76: "bf16[s0, 32, 128]" = torch.cat((sub_38, add_38), dim = -1);  sub_38 = add_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_77: "bf16[s0, 32, 128]" = torch.cat((cat_76, getitem_253), dim = -1);  cat_76 = getitem_253 = None
            reshape_38: "bf16[s0, 4096]" = cat_77.reshape(size_57);  cat_77 = size_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_58 = getitem_248.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_134: "bf16[s0, 8, 128]" = getitem_248.view(s0, -1, 128);  getitem_248 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_256: "bf16[s0, 8, 128]" = view_134[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_257: "bf16[s0, 8, 0]" = view_134[(Ellipsis, slice(128, None, None))];  view_134 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_78: "bf16[s0, 1, 64]" = getitem_250.unsqueeze(-2);  getitem_250 = None
            to_78: "bf16[s0, 1, 64]" = unsqueeze_78.to(torch.bfloat16);  unsqueeze_78 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_79: "bf16[s0, 1, 64]" = getitem_251.unsqueeze(-2);  getitem_251 = None
            to_79: "bf16[s0, 1, 64]" = unsqueeze_79.to(torch.bfloat16);  unsqueeze_79 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_59 = torch.chunk(getitem_256, 2, dim = -1);  getitem_256 = None
            getitem_258: "bf16[s0, 8, 64]" = chunk_59[0]
            getitem_259: "bf16[s0, 8, 64]" = chunk_59[1];  chunk_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_156: "bf16[s0, 8, 64]" = getitem_258 * to_78
            mul_157: "bf16[s0, 8, 64]" = getitem_259 * to_79
            sub_39: "bf16[s0, 8, 64]" = mul_156 - mul_157;  mul_156 = mul_157 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_158: "bf16[s0, 8, 64]" = getitem_259 * to_78;  getitem_259 = to_78 = None
            mul_159: "bf16[s0, 8, 64]" = getitem_258 * to_79;  getitem_258 = to_79 = None
            add_39: "bf16[s0, 8, 64]" = mul_158 + mul_159;  mul_158 = mul_159 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_78: "bf16[s0, 8, 128]" = torch.cat((sub_39, add_39), dim = -1);  sub_39 = add_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_79: "bf16[s0, 8, 128]" = torch.cat((cat_78, getitem_257), dim = -1);  cat_78 = getitem_257 = None
            reshape_39: "bf16[s0, 1024]" = cat_79.reshape(size_58);  cat_79 = size_58 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_59 = reshape_38.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_19: "bf16[s0, 4096]" = torch.zeros(size_59, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_135: "bf16[s0, 32, 128]" = reshape_38.view(-1, 32, 128);  reshape_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_136: "bf16[s0, 32, 128]" = zeros_19.view(-1, 32, 128);  zeros_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_137: "bf16[s0, 8, 128]" = reshape_39.view(-1, 8, 128);  reshape_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_138: "bf16[s0, 8, 128]" = getitem_249.view(-1, 8, 128);  getitem_249 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_19 = torch.ops.vllm.unified_attention_with_output(view_135, view_137, view_138, view_136, 'model.layers.19.self_attn.attn');  view_135 = view_137 = view_138 = unified_attention_with_output_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_139: "bf16[s0, 4096]" = view_136.view(-1, 4096);  view_136 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_77: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_139, l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_139 = l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_39: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_77: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_77)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_78: "bf16[s0, 4096]" = torch.empty_like(empty_like_76)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_38 = torch.ops._C.fused_add_rms_norm(empty_like_77, rocm_unquantized_gemm_impl_77, empty_like_78, empty_like_76, _get_data_attr_39, 1e-05);  rocm_unquantized_gemm_impl_77 = empty_like_76 = _get_data_attr_39 = fused_add_rms_norm_38 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_78: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_77, l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_77 = l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_19: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_19 = torch.ops._C.silu_and_mul(empty_19, rocm_unquantized_gemm_impl_78);  rocm_unquantized_gemm_impl_78 = silu_and_mul_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_79: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_19, l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_19 = l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_40: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_79: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_79)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_80: "bf16[s0, 4096]" = torch.empty_like(empty_like_78)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_39 = torch.ops._C.fused_add_rms_norm(empty_like_79, rocm_unquantized_gemm_impl_79, empty_like_80, empty_like_78, _get_data_attr_40, 1e-05);  rocm_unquantized_gemm_impl_79 = empty_like_78 = _get_data_attr_40 = fused_add_rms_norm_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_80: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_79, l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_79 = l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_20 = rocm_unquantized_gemm_impl_80.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_80 = None
            getitem_260: "bf16[s0, 4096]" = split_20[0]
            getitem_261: "bf16[s0, 1024]" = split_20[1]
            getitem_262: "bf16[s0, 1024]" = split_20[2];  split_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_20: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_20: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_20);  flatten_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_60 = index_select_20.chunk(2, dim = -1);  index_select_20 = None
            getitem_263: "bf16[s0, 64]" = chunk_60[0]
            getitem_264: "bf16[s0, 64]" = chunk_60[1];  chunk_60 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_60 = getitem_260.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_140: "bf16[s0, 32, 128]" = getitem_260.view(s0, -1, 128);  getitem_260 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_265: "bf16[s0, 32, 128]" = view_140[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_266: "bf16[s0, 32, 0]" = view_140[(Ellipsis, slice(128, None, None))];  view_140 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_80: "bf16[s0, 1, 64]" = getitem_263.unsqueeze(-2)
            to_80: "bf16[s0, 1, 64]" = unsqueeze_80.to(torch.bfloat16);  unsqueeze_80 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_81: "bf16[s0, 1, 64]" = getitem_264.unsqueeze(-2)
            to_81: "bf16[s0, 1, 64]" = unsqueeze_81.to(torch.bfloat16);  unsqueeze_81 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_61 = torch.chunk(getitem_265, 2, dim = -1);  getitem_265 = None
            getitem_267: "bf16[s0, 32, 64]" = chunk_61[0]
            getitem_268: "bf16[s0, 32, 64]" = chunk_61[1];  chunk_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_160: "bf16[s0, 32, 64]" = getitem_267 * to_80
            mul_161: "bf16[s0, 32, 64]" = getitem_268 * to_81
            sub_40: "bf16[s0, 32, 64]" = mul_160 - mul_161;  mul_160 = mul_161 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_162: "bf16[s0, 32, 64]" = getitem_268 * to_80;  getitem_268 = to_80 = None
            mul_163: "bf16[s0, 32, 64]" = getitem_267 * to_81;  getitem_267 = to_81 = None
            add_40: "bf16[s0, 32, 64]" = mul_162 + mul_163;  mul_162 = mul_163 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_80: "bf16[s0, 32, 128]" = torch.cat((sub_40, add_40), dim = -1);  sub_40 = add_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_81: "bf16[s0, 32, 128]" = torch.cat((cat_80, getitem_266), dim = -1);  cat_80 = getitem_266 = None
            reshape_40: "bf16[s0, 4096]" = cat_81.reshape(size_60);  cat_81 = size_60 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_61 = getitem_261.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_141: "bf16[s0, 8, 128]" = getitem_261.view(s0, -1, 128);  getitem_261 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_269: "bf16[s0, 8, 128]" = view_141[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_270: "bf16[s0, 8, 0]" = view_141[(Ellipsis, slice(128, None, None))];  view_141 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_82: "bf16[s0, 1, 64]" = getitem_263.unsqueeze(-2);  getitem_263 = None
            to_82: "bf16[s0, 1, 64]" = unsqueeze_82.to(torch.bfloat16);  unsqueeze_82 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_83: "bf16[s0, 1, 64]" = getitem_264.unsqueeze(-2);  getitem_264 = None
            to_83: "bf16[s0, 1, 64]" = unsqueeze_83.to(torch.bfloat16);  unsqueeze_83 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_62 = torch.chunk(getitem_269, 2, dim = -1);  getitem_269 = None
            getitem_271: "bf16[s0, 8, 64]" = chunk_62[0]
            getitem_272: "bf16[s0, 8, 64]" = chunk_62[1];  chunk_62 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_164: "bf16[s0, 8, 64]" = getitem_271 * to_82
            mul_165: "bf16[s0, 8, 64]" = getitem_272 * to_83
            sub_41: "bf16[s0, 8, 64]" = mul_164 - mul_165;  mul_164 = mul_165 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_166: "bf16[s0, 8, 64]" = getitem_272 * to_82;  getitem_272 = to_82 = None
            mul_167: "bf16[s0, 8, 64]" = getitem_271 * to_83;  getitem_271 = to_83 = None
            add_41: "bf16[s0, 8, 64]" = mul_166 + mul_167;  mul_166 = mul_167 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_82: "bf16[s0, 8, 128]" = torch.cat((sub_41, add_41), dim = -1);  sub_41 = add_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_83: "bf16[s0, 8, 128]" = torch.cat((cat_82, getitem_270), dim = -1);  cat_82 = getitem_270 = None
            reshape_41: "bf16[s0, 1024]" = cat_83.reshape(size_61);  cat_83 = size_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_62 = reshape_40.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_20: "bf16[s0, 4096]" = torch.zeros(size_62, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_62 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_142: "bf16[s0, 32, 128]" = reshape_40.view(-1, 32, 128);  reshape_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_143: "bf16[s0, 32, 128]" = zeros_20.view(-1, 32, 128);  zeros_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_144: "bf16[s0, 8, 128]" = reshape_41.view(-1, 8, 128);  reshape_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_145: "bf16[s0, 8, 128]" = getitem_262.view(-1, 8, 128);  getitem_262 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_20 = torch.ops.vllm.unified_attention_with_output(view_142, view_144, view_145, view_143, 'model.layers.20.self_attn.attn');  view_142 = view_144 = view_145 = unified_attention_with_output_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_146: "bf16[s0, 4096]" = view_143.view(-1, 4096);  view_143 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_81: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_146, l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_146 = l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_41: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_81: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_81)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_82: "bf16[s0, 4096]" = torch.empty_like(empty_like_80)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_40 = torch.ops._C.fused_add_rms_norm(empty_like_81, rocm_unquantized_gemm_impl_81, empty_like_82, empty_like_80, _get_data_attr_41, 1e-05);  rocm_unquantized_gemm_impl_81 = empty_like_80 = _get_data_attr_41 = fused_add_rms_norm_40 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_82: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_81, l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_81 = l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_20: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_20 = torch.ops._C.silu_and_mul(empty_20, rocm_unquantized_gemm_impl_82);  rocm_unquantized_gemm_impl_82 = silu_and_mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_83: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_20, l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_20 = l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_42: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_83: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_83)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_84: "bf16[s0, 4096]" = torch.empty_like(empty_like_82)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_41 = torch.ops._C.fused_add_rms_norm(empty_like_83, rocm_unquantized_gemm_impl_83, empty_like_84, empty_like_82, _get_data_attr_42, 1e-05);  rocm_unquantized_gemm_impl_83 = empty_like_82 = _get_data_attr_42 = fused_add_rms_norm_41 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_84: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_83, l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_83 = l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_21 = rocm_unquantized_gemm_impl_84.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_84 = None
            getitem_273: "bf16[s0, 4096]" = split_21[0]
            getitem_274: "bf16[s0, 1024]" = split_21[1]
            getitem_275: "bf16[s0, 1024]" = split_21[2];  split_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_21: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_21: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_21);  flatten_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_63 = index_select_21.chunk(2, dim = -1);  index_select_21 = None
            getitem_276: "bf16[s0, 64]" = chunk_63[0]
            getitem_277: "bf16[s0, 64]" = chunk_63[1];  chunk_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_63 = getitem_273.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_147: "bf16[s0, 32, 128]" = getitem_273.view(s0, -1, 128);  getitem_273 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_278: "bf16[s0, 32, 128]" = view_147[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_279: "bf16[s0, 32, 0]" = view_147[(Ellipsis, slice(128, None, None))];  view_147 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_84: "bf16[s0, 1, 64]" = getitem_276.unsqueeze(-2)
            to_84: "bf16[s0, 1, 64]" = unsqueeze_84.to(torch.bfloat16);  unsqueeze_84 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_85: "bf16[s0, 1, 64]" = getitem_277.unsqueeze(-2)
            to_85: "bf16[s0, 1, 64]" = unsqueeze_85.to(torch.bfloat16);  unsqueeze_85 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_64 = torch.chunk(getitem_278, 2, dim = -1);  getitem_278 = None
            getitem_280: "bf16[s0, 32, 64]" = chunk_64[0]
            getitem_281: "bf16[s0, 32, 64]" = chunk_64[1];  chunk_64 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_168: "bf16[s0, 32, 64]" = getitem_280 * to_84
            mul_169: "bf16[s0, 32, 64]" = getitem_281 * to_85
            sub_42: "bf16[s0, 32, 64]" = mul_168 - mul_169;  mul_168 = mul_169 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_170: "bf16[s0, 32, 64]" = getitem_281 * to_84;  getitem_281 = to_84 = None
            mul_171: "bf16[s0, 32, 64]" = getitem_280 * to_85;  getitem_280 = to_85 = None
            add_42: "bf16[s0, 32, 64]" = mul_170 + mul_171;  mul_170 = mul_171 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_84: "bf16[s0, 32, 128]" = torch.cat((sub_42, add_42), dim = -1);  sub_42 = add_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_85: "bf16[s0, 32, 128]" = torch.cat((cat_84, getitem_279), dim = -1);  cat_84 = getitem_279 = None
            reshape_42: "bf16[s0, 4096]" = cat_85.reshape(size_63);  cat_85 = size_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_64 = getitem_274.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_148: "bf16[s0, 8, 128]" = getitem_274.view(s0, -1, 128);  getitem_274 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_282: "bf16[s0, 8, 128]" = view_148[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_283: "bf16[s0, 8, 0]" = view_148[(Ellipsis, slice(128, None, None))];  view_148 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_86: "bf16[s0, 1, 64]" = getitem_276.unsqueeze(-2);  getitem_276 = None
            to_86: "bf16[s0, 1, 64]" = unsqueeze_86.to(torch.bfloat16);  unsqueeze_86 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_87: "bf16[s0, 1, 64]" = getitem_277.unsqueeze(-2);  getitem_277 = None
            to_87: "bf16[s0, 1, 64]" = unsqueeze_87.to(torch.bfloat16);  unsqueeze_87 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_65 = torch.chunk(getitem_282, 2, dim = -1);  getitem_282 = None
            getitem_284: "bf16[s0, 8, 64]" = chunk_65[0]
            getitem_285: "bf16[s0, 8, 64]" = chunk_65[1];  chunk_65 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_172: "bf16[s0, 8, 64]" = getitem_284 * to_86
            mul_173: "bf16[s0, 8, 64]" = getitem_285 * to_87
            sub_43: "bf16[s0, 8, 64]" = mul_172 - mul_173;  mul_172 = mul_173 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_174: "bf16[s0, 8, 64]" = getitem_285 * to_86;  getitem_285 = to_86 = None
            mul_175: "bf16[s0, 8, 64]" = getitem_284 * to_87;  getitem_284 = to_87 = None
            add_43: "bf16[s0, 8, 64]" = mul_174 + mul_175;  mul_174 = mul_175 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_86: "bf16[s0, 8, 128]" = torch.cat((sub_43, add_43), dim = -1);  sub_43 = add_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_87: "bf16[s0, 8, 128]" = torch.cat((cat_86, getitem_283), dim = -1);  cat_86 = getitem_283 = None
            reshape_43: "bf16[s0, 1024]" = cat_87.reshape(size_64);  cat_87 = size_64 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_65 = reshape_42.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_21: "bf16[s0, 4096]" = torch.zeros(size_65, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_65 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_149: "bf16[s0, 32, 128]" = reshape_42.view(-1, 32, 128);  reshape_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_150: "bf16[s0, 32, 128]" = zeros_21.view(-1, 32, 128);  zeros_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_151: "bf16[s0, 8, 128]" = reshape_43.view(-1, 8, 128);  reshape_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_152: "bf16[s0, 8, 128]" = getitem_275.view(-1, 8, 128);  getitem_275 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_21 = torch.ops.vllm.unified_attention_with_output(view_149, view_151, view_152, view_150, 'model.layers.21.self_attn.attn');  view_149 = view_151 = view_152 = unified_attention_with_output_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_153: "bf16[s0, 4096]" = view_150.view(-1, 4096);  view_150 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_85: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_153, l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_153 = l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_43: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_85: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_85)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_86: "bf16[s0, 4096]" = torch.empty_like(empty_like_84)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_42 = torch.ops._C.fused_add_rms_norm(empty_like_85, rocm_unquantized_gemm_impl_85, empty_like_86, empty_like_84, _get_data_attr_43, 1e-05);  rocm_unquantized_gemm_impl_85 = empty_like_84 = _get_data_attr_43 = fused_add_rms_norm_42 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_86: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_85, l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_85 = l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_21: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_21 = torch.ops._C.silu_and_mul(empty_21, rocm_unquantized_gemm_impl_86);  rocm_unquantized_gemm_impl_86 = silu_and_mul_21 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_87: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_21, l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_21 = l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_44: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_87: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_87)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_88: "bf16[s0, 4096]" = torch.empty_like(empty_like_86)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_43 = torch.ops._C.fused_add_rms_norm(empty_like_87, rocm_unquantized_gemm_impl_87, empty_like_88, empty_like_86, _get_data_attr_44, 1e-05);  rocm_unquantized_gemm_impl_87 = empty_like_86 = _get_data_attr_44 = fused_add_rms_norm_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_88: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_87, l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_87 = l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_22 = rocm_unquantized_gemm_impl_88.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_88 = None
            getitem_286: "bf16[s0, 4096]" = split_22[0]
            getitem_287: "bf16[s0, 1024]" = split_22[1]
            getitem_288: "bf16[s0, 1024]" = split_22[2];  split_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_22: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_22: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_22);  flatten_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_66 = index_select_22.chunk(2, dim = -1);  index_select_22 = None
            getitem_289: "bf16[s0, 64]" = chunk_66[0]
            getitem_290: "bf16[s0, 64]" = chunk_66[1];  chunk_66 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_66 = getitem_286.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_154: "bf16[s0, 32, 128]" = getitem_286.view(s0, -1, 128);  getitem_286 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_291: "bf16[s0, 32, 128]" = view_154[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_292: "bf16[s0, 32, 0]" = view_154[(Ellipsis, slice(128, None, None))];  view_154 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_88: "bf16[s0, 1, 64]" = getitem_289.unsqueeze(-2)
            to_88: "bf16[s0, 1, 64]" = unsqueeze_88.to(torch.bfloat16);  unsqueeze_88 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_89: "bf16[s0, 1, 64]" = getitem_290.unsqueeze(-2)
            to_89: "bf16[s0, 1, 64]" = unsqueeze_89.to(torch.bfloat16);  unsqueeze_89 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_67 = torch.chunk(getitem_291, 2, dim = -1);  getitem_291 = None
            getitem_293: "bf16[s0, 32, 64]" = chunk_67[0]
            getitem_294: "bf16[s0, 32, 64]" = chunk_67[1];  chunk_67 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_176: "bf16[s0, 32, 64]" = getitem_293 * to_88
            mul_177: "bf16[s0, 32, 64]" = getitem_294 * to_89
            sub_44: "bf16[s0, 32, 64]" = mul_176 - mul_177;  mul_176 = mul_177 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_178: "bf16[s0, 32, 64]" = getitem_294 * to_88;  getitem_294 = to_88 = None
            mul_179: "bf16[s0, 32, 64]" = getitem_293 * to_89;  getitem_293 = to_89 = None
            add_44: "bf16[s0, 32, 64]" = mul_178 + mul_179;  mul_178 = mul_179 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_88: "bf16[s0, 32, 128]" = torch.cat((sub_44, add_44), dim = -1);  sub_44 = add_44 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_89: "bf16[s0, 32, 128]" = torch.cat((cat_88, getitem_292), dim = -1);  cat_88 = getitem_292 = None
            reshape_44: "bf16[s0, 4096]" = cat_89.reshape(size_66);  cat_89 = size_66 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_67 = getitem_287.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_155: "bf16[s0, 8, 128]" = getitem_287.view(s0, -1, 128);  getitem_287 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_295: "bf16[s0, 8, 128]" = view_155[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_296: "bf16[s0, 8, 0]" = view_155[(Ellipsis, slice(128, None, None))];  view_155 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_90: "bf16[s0, 1, 64]" = getitem_289.unsqueeze(-2);  getitem_289 = None
            to_90: "bf16[s0, 1, 64]" = unsqueeze_90.to(torch.bfloat16);  unsqueeze_90 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_91: "bf16[s0, 1, 64]" = getitem_290.unsqueeze(-2);  getitem_290 = None
            to_91: "bf16[s0, 1, 64]" = unsqueeze_91.to(torch.bfloat16);  unsqueeze_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_68 = torch.chunk(getitem_295, 2, dim = -1);  getitem_295 = None
            getitem_297: "bf16[s0, 8, 64]" = chunk_68[0]
            getitem_298: "bf16[s0, 8, 64]" = chunk_68[1];  chunk_68 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_180: "bf16[s0, 8, 64]" = getitem_297 * to_90
            mul_181: "bf16[s0, 8, 64]" = getitem_298 * to_91
            sub_45: "bf16[s0, 8, 64]" = mul_180 - mul_181;  mul_180 = mul_181 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_182: "bf16[s0, 8, 64]" = getitem_298 * to_90;  getitem_298 = to_90 = None
            mul_183: "bf16[s0, 8, 64]" = getitem_297 * to_91;  getitem_297 = to_91 = None
            add_45: "bf16[s0, 8, 64]" = mul_182 + mul_183;  mul_182 = mul_183 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_90: "bf16[s0, 8, 128]" = torch.cat((sub_45, add_45), dim = -1);  sub_45 = add_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_91: "bf16[s0, 8, 128]" = torch.cat((cat_90, getitem_296), dim = -1);  cat_90 = getitem_296 = None
            reshape_45: "bf16[s0, 1024]" = cat_91.reshape(size_67);  cat_91 = size_67 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_68 = reshape_44.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_22: "bf16[s0, 4096]" = torch.zeros(size_68, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_68 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_156: "bf16[s0, 32, 128]" = reshape_44.view(-1, 32, 128);  reshape_44 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_157: "bf16[s0, 32, 128]" = zeros_22.view(-1, 32, 128);  zeros_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_158: "bf16[s0, 8, 128]" = reshape_45.view(-1, 8, 128);  reshape_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_159: "bf16[s0, 8, 128]" = getitem_288.view(-1, 8, 128);  getitem_288 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_22 = torch.ops.vllm.unified_attention_with_output(view_156, view_158, view_159, view_157, 'model.layers.22.self_attn.attn');  view_156 = view_158 = view_159 = unified_attention_with_output_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_160: "bf16[s0, 4096]" = view_157.view(-1, 4096);  view_157 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_89: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_160, l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_160 = l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_45: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_89: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_89)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_90: "bf16[s0, 4096]" = torch.empty_like(empty_like_88)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_44 = torch.ops._C.fused_add_rms_norm(empty_like_89, rocm_unquantized_gemm_impl_89, empty_like_90, empty_like_88, _get_data_attr_45, 1e-05);  rocm_unquantized_gemm_impl_89 = empty_like_88 = _get_data_attr_45 = fused_add_rms_norm_44 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_90: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_89, l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_89 = l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_22: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_22 = torch.ops._C.silu_and_mul(empty_22, rocm_unquantized_gemm_impl_90);  rocm_unquantized_gemm_impl_90 = silu_and_mul_22 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_91: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_22, l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_22 = l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_46: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_91: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_91)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_92: "bf16[s0, 4096]" = torch.empty_like(empty_like_90)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_45 = torch.ops._C.fused_add_rms_norm(empty_like_91, rocm_unquantized_gemm_impl_91, empty_like_92, empty_like_90, _get_data_attr_46, 1e-05);  rocm_unquantized_gemm_impl_91 = empty_like_90 = _get_data_attr_46 = fused_add_rms_norm_45 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_92: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_91, l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_91 = l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_23 = rocm_unquantized_gemm_impl_92.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_92 = None
            getitem_299: "bf16[s0, 4096]" = split_23[0]
            getitem_300: "bf16[s0, 1024]" = split_23[1]
            getitem_301: "bf16[s0, 1024]" = split_23[2];  split_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_23: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_23: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_23);  flatten_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_69 = index_select_23.chunk(2, dim = -1);  index_select_23 = None
            getitem_302: "bf16[s0, 64]" = chunk_69[0]
            getitem_303: "bf16[s0, 64]" = chunk_69[1];  chunk_69 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_69 = getitem_299.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_161: "bf16[s0, 32, 128]" = getitem_299.view(s0, -1, 128);  getitem_299 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_304: "bf16[s0, 32, 128]" = view_161[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_305: "bf16[s0, 32, 0]" = view_161[(Ellipsis, slice(128, None, None))];  view_161 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_92: "bf16[s0, 1, 64]" = getitem_302.unsqueeze(-2)
            to_92: "bf16[s0, 1, 64]" = unsqueeze_92.to(torch.bfloat16);  unsqueeze_92 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_93: "bf16[s0, 1, 64]" = getitem_303.unsqueeze(-2)
            to_93: "bf16[s0, 1, 64]" = unsqueeze_93.to(torch.bfloat16);  unsqueeze_93 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_70 = torch.chunk(getitem_304, 2, dim = -1);  getitem_304 = None
            getitem_306: "bf16[s0, 32, 64]" = chunk_70[0]
            getitem_307: "bf16[s0, 32, 64]" = chunk_70[1];  chunk_70 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_184: "bf16[s0, 32, 64]" = getitem_306 * to_92
            mul_185: "bf16[s0, 32, 64]" = getitem_307 * to_93
            sub_46: "bf16[s0, 32, 64]" = mul_184 - mul_185;  mul_184 = mul_185 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_186: "bf16[s0, 32, 64]" = getitem_307 * to_92;  getitem_307 = to_92 = None
            mul_187: "bf16[s0, 32, 64]" = getitem_306 * to_93;  getitem_306 = to_93 = None
            add_46: "bf16[s0, 32, 64]" = mul_186 + mul_187;  mul_186 = mul_187 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_92: "bf16[s0, 32, 128]" = torch.cat((sub_46, add_46), dim = -1);  sub_46 = add_46 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_93: "bf16[s0, 32, 128]" = torch.cat((cat_92, getitem_305), dim = -1);  cat_92 = getitem_305 = None
            reshape_46: "bf16[s0, 4096]" = cat_93.reshape(size_69);  cat_93 = size_69 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_70 = getitem_300.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_162: "bf16[s0, 8, 128]" = getitem_300.view(s0, -1, 128);  getitem_300 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_308: "bf16[s0, 8, 128]" = view_162[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_309: "bf16[s0, 8, 0]" = view_162[(Ellipsis, slice(128, None, None))];  view_162 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_94: "bf16[s0, 1, 64]" = getitem_302.unsqueeze(-2);  getitem_302 = None
            to_94: "bf16[s0, 1, 64]" = unsqueeze_94.to(torch.bfloat16);  unsqueeze_94 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_95: "bf16[s0, 1, 64]" = getitem_303.unsqueeze(-2);  getitem_303 = None
            to_95: "bf16[s0, 1, 64]" = unsqueeze_95.to(torch.bfloat16);  unsqueeze_95 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_71 = torch.chunk(getitem_308, 2, dim = -1);  getitem_308 = None
            getitem_310: "bf16[s0, 8, 64]" = chunk_71[0]
            getitem_311: "bf16[s0, 8, 64]" = chunk_71[1];  chunk_71 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_188: "bf16[s0, 8, 64]" = getitem_310 * to_94
            mul_189: "bf16[s0, 8, 64]" = getitem_311 * to_95
            sub_47: "bf16[s0, 8, 64]" = mul_188 - mul_189;  mul_188 = mul_189 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_190: "bf16[s0, 8, 64]" = getitem_311 * to_94;  getitem_311 = to_94 = None
            mul_191: "bf16[s0, 8, 64]" = getitem_310 * to_95;  getitem_310 = to_95 = None
            add_47: "bf16[s0, 8, 64]" = mul_190 + mul_191;  mul_190 = mul_191 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_94: "bf16[s0, 8, 128]" = torch.cat((sub_47, add_47), dim = -1);  sub_47 = add_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_95: "bf16[s0, 8, 128]" = torch.cat((cat_94, getitem_309), dim = -1);  cat_94 = getitem_309 = None
            reshape_47: "bf16[s0, 1024]" = cat_95.reshape(size_70);  cat_95 = size_70 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_71 = reshape_46.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_23: "bf16[s0, 4096]" = torch.zeros(size_71, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_71 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_163: "bf16[s0, 32, 128]" = reshape_46.view(-1, 32, 128);  reshape_46 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_164: "bf16[s0, 32, 128]" = zeros_23.view(-1, 32, 128);  zeros_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_165: "bf16[s0, 8, 128]" = reshape_47.view(-1, 8, 128);  reshape_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_166: "bf16[s0, 8, 128]" = getitem_301.view(-1, 8, 128);  getitem_301 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_23 = torch.ops.vllm.unified_attention_with_output(view_163, view_165, view_166, view_164, 'model.layers.23.self_attn.attn');  view_163 = view_165 = view_166 = unified_attention_with_output_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_167: "bf16[s0, 4096]" = view_164.view(-1, 4096);  view_164 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_93: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_167, l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_167 = l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_47: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_93: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_93)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_94: "bf16[s0, 4096]" = torch.empty_like(empty_like_92)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_46 = torch.ops._C.fused_add_rms_norm(empty_like_93, rocm_unquantized_gemm_impl_93, empty_like_94, empty_like_92, _get_data_attr_47, 1e-05);  rocm_unquantized_gemm_impl_93 = empty_like_92 = _get_data_attr_47 = fused_add_rms_norm_46 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_94: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_93, l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_93 = l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_23: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_23 = torch.ops._C.silu_and_mul(empty_23, rocm_unquantized_gemm_impl_94);  rocm_unquantized_gemm_impl_94 = silu_and_mul_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_95: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_23, l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_23 = l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_48: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_95: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_95)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_96: "bf16[s0, 4096]" = torch.empty_like(empty_like_94)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_47 = torch.ops._C.fused_add_rms_norm(empty_like_95, rocm_unquantized_gemm_impl_95, empty_like_96, empty_like_94, _get_data_attr_48, 1e-05);  rocm_unquantized_gemm_impl_95 = empty_like_94 = _get_data_attr_48 = fused_add_rms_norm_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_96: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_95, l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_95 = l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_24 = rocm_unquantized_gemm_impl_96.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_96 = None
            getitem_312: "bf16[s0, 4096]" = split_24[0]
            getitem_313: "bf16[s0, 1024]" = split_24[1]
            getitem_314: "bf16[s0, 1024]" = split_24[2];  split_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_24: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_24: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_24);  flatten_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_72 = index_select_24.chunk(2, dim = -1);  index_select_24 = None
            getitem_315: "bf16[s0, 64]" = chunk_72[0]
            getitem_316: "bf16[s0, 64]" = chunk_72[1];  chunk_72 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_72 = getitem_312.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_168: "bf16[s0, 32, 128]" = getitem_312.view(s0, -1, 128);  getitem_312 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_317: "bf16[s0, 32, 128]" = view_168[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_318: "bf16[s0, 32, 0]" = view_168[(Ellipsis, slice(128, None, None))];  view_168 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_96: "bf16[s0, 1, 64]" = getitem_315.unsqueeze(-2)
            to_96: "bf16[s0, 1, 64]" = unsqueeze_96.to(torch.bfloat16);  unsqueeze_96 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_97: "bf16[s0, 1, 64]" = getitem_316.unsqueeze(-2)
            to_97: "bf16[s0, 1, 64]" = unsqueeze_97.to(torch.bfloat16);  unsqueeze_97 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_73 = torch.chunk(getitem_317, 2, dim = -1);  getitem_317 = None
            getitem_319: "bf16[s0, 32, 64]" = chunk_73[0]
            getitem_320: "bf16[s0, 32, 64]" = chunk_73[1];  chunk_73 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_192: "bf16[s0, 32, 64]" = getitem_319 * to_96
            mul_193: "bf16[s0, 32, 64]" = getitem_320 * to_97
            sub_48: "bf16[s0, 32, 64]" = mul_192 - mul_193;  mul_192 = mul_193 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_194: "bf16[s0, 32, 64]" = getitem_320 * to_96;  getitem_320 = to_96 = None
            mul_195: "bf16[s0, 32, 64]" = getitem_319 * to_97;  getitem_319 = to_97 = None
            add_48: "bf16[s0, 32, 64]" = mul_194 + mul_195;  mul_194 = mul_195 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_96: "bf16[s0, 32, 128]" = torch.cat((sub_48, add_48), dim = -1);  sub_48 = add_48 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_97: "bf16[s0, 32, 128]" = torch.cat((cat_96, getitem_318), dim = -1);  cat_96 = getitem_318 = None
            reshape_48: "bf16[s0, 4096]" = cat_97.reshape(size_72);  cat_97 = size_72 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_73 = getitem_313.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_169: "bf16[s0, 8, 128]" = getitem_313.view(s0, -1, 128);  getitem_313 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_321: "bf16[s0, 8, 128]" = view_169[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_322: "bf16[s0, 8, 0]" = view_169[(Ellipsis, slice(128, None, None))];  view_169 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_98: "bf16[s0, 1, 64]" = getitem_315.unsqueeze(-2);  getitem_315 = None
            to_98: "bf16[s0, 1, 64]" = unsqueeze_98.to(torch.bfloat16);  unsqueeze_98 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_99: "bf16[s0, 1, 64]" = getitem_316.unsqueeze(-2);  getitem_316 = None
            to_99: "bf16[s0, 1, 64]" = unsqueeze_99.to(torch.bfloat16);  unsqueeze_99 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_74 = torch.chunk(getitem_321, 2, dim = -1);  getitem_321 = None
            getitem_323: "bf16[s0, 8, 64]" = chunk_74[0]
            getitem_324: "bf16[s0, 8, 64]" = chunk_74[1];  chunk_74 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_196: "bf16[s0, 8, 64]" = getitem_323 * to_98
            mul_197: "bf16[s0, 8, 64]" = getitem_324 * to_99
            sub_49: "bf16[s0, 8, 64]" = mul_196 - mul_197;  mul_196 = mul_197 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_198: "bf16[s0, 8, 64]" = getitem_324 * to_98;  getitem_324 = to_98 = None
            mul_199: "bf16[s0, 8, 64]" = getitem_323 * to_99;  getitem_323 = to_99 = None
            add_49: "bf16[s0, 8, 64]" = mul_198 + mul_199;  mul_198 = mul_199 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_98: "bf16[s0, 8, 128]" = torch.cat((sub_49, add_49), dim = -1);  sub_49 = add_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_99: "bf16[s0, 8, 128]" = torch.cat((cat_98, getitem_322), dim = -1);  cat_98 = getitem_322 = None
            reshape_49: "bf16[s0, 1024]" = cat_99.reshape(size_73);  cat_99 = size_73 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_74 = reshape_48.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_24: "bf16[s0, 4096]" = torch.zeros(size_74, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_74 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_170: "bf16[s0, 32, 128]" = reshape_48.view(-1, 32, 128);  reshape_48 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_171: "bf16[s0, 32, 128]" = zeros_24.view(-1, 32, 128);  zeros_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_172: "bf16[s0, 8, 128]" = reshape_49.view(-1, 8, 128);  reshape_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_173: "bf16[s0, 8, 128]" = getitem_314.view(-1, 8, 128);  getitem_314 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_24 = torch.ops.vllm.unified_attention_with_output(view_170, view_172, view_173, view_171, 'model.layers.24.self_attn.attn');  view_170 = view_172 = view_173 = unified_attention_with_output_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_174: "bf16[s0, 4096]" = view_171.view(-1, 4096);  view_171 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_97: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_174, l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_174 = l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_49: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_97: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_97)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_98: "bf16[s0, 4096]" = torch.empty_like(empty_like_96)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_48 = torch.ops._C.fused_add_rms_norm(empty_like_97, rocm_unquantized_gemm_impl_97, empty_like_98, empty_like_96, _get_data_attr_49, 1e-05);  rocm_unquantized_gemm_impl_97 = empty_like_96 = _get_data_attr_49 = fused_add_rms_norm_48 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_98: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_97, l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_97 = l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_24: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_24 = torch.ops._C.silu_and_mul(empty_24, rocm_unquantized_gemm_impl_98);  rocm_unquantized_gemm_impl_98 = silu_and_mul_24 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_99: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_24, l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_24 = l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_50: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_99: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_99)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_100: "bf16[s0, 4096]" = torch.empty_like(empty_like_98)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_49 = torch.ops._C.fused_add_rms_norm(empty_like_99, rocm_unquantized_gemm_impl_99, empty_like_100, empty_like_98, _get_data_attr_50, 1e-05);  rocm_unquantized_gemm_impl_99 = empty_like_98 = _get_data_attr_50 = fused_add_rms_norm_49 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_100: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_99, l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_99 = l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_25 = rocm_unquantized_gemm_impl_100.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_100 = None
            getitem_325: "bf16[s0, 4096]" = split_25[0]
            getitem_326: "bf16[s0, 1024]" = split_25[1]
            getitem_327: "bf16[s0, 1024]" = split_25[2];  split_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_25: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_25: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_25);  flatten_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_75 = index_select_25.chunk(2, dim = -1);  index_select_25 = None
            getitem_328: "bf16[s0, 64]" = chunk_75[0]
            getitem_329: "bf16[s0, 64]" = chunk_75[1];  chunk_75 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_75 = getitem_325.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_175: "bf16[s0, 32, 128]" = getitem_325.view(s0, -1, 128);  getitem_325 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_330: "bf16[s0, 32, 128]" = view_175[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_331: "bf16[s0, 32, 0]" = view_175[(Ellipsis, slice(128, None, None))];  view_175 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_100: "bf16[s0, 1, 64]" = getitem_328.unsqueeze(-2)
            to_100: "bf16[s0, 1, 64]" = unsqueeze_100.to(torch.bfloat16);  unsqueeze_100 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_101: "bf16[s0, 1, 64]" = getitem_329.unsqueeze(-2)
            to_101: "bf16[s0, 1, 64]" = unsqueeze_101.to(torch.bfloat16);  unsqueeze_101 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_76 = torch.chunk(getitem_330, 2, dim = -1);  getitem_330 = None
            getitem_332: "bf16[s0, 32, 64]" = chunk_76[0]
            getitem_333: "bf16[s0, 32, 64]" = chunk_76[1];  chunk_76 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_200: "bf16[s0, 32, 64]" = getitem_332 * to_100
            mul_201: "bf16[s0, 32, 64]" = getitem_333 * to_101
            sub_50: "bf16[s0, 32, 64]" = mul_200 - mul_201;  mul_200 = mul_201 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_202: "bf16[s0, 32, 64]" = getitem_333 * to_100;  getitem_333 = to_100 = None
            mul_203: "bf16[s0, 32, 64]" = getitem_332 * to_101;  getitem_332 = to_101 = None
            add_50: "bf16[s0, 32, 64]" = mul_202 + mul_203;  mul_202 = mul_203 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_100: "bf16[s0, 32, 128]" = torch.cat((sub_50, add_50), dim = -1);  sub_50 = add_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_101: "bf16[s0, 32, 128]" = torch.cat((cat_100, getitem_331), dim = -1);  cat_100 = getitem_331 = None
            reshape_50: "bf16[s0, 4096]" = cat_101.reshape(size_75);  cat_101 = size_75 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_76 = getitem_326.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_176: "bf16[s0, 8, 128]" = getitem_326.view(s0, -1, 128);  getitem_326 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_334: "bf16[s0, 8, 128]" = view_176[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_335: "bf16[s0, 8, 0]" = view_176[(Ellipsis, slice(128, None, None))];  view_176 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_102: "bf16[s0, 1, 64]" = getitem_328.unsqueeze(-2);  getitem_328 = None
            to_102: "bf16[s0, 1, 64]" = unsqueeze_102.to(torch.bfloat16);  unsqueeze_102 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_103: "bf16[s0, 1, 64]" = getitem_329.unsqueeze(-2);  getitem_329 = None
            to_103: "bf16[s0, 1, 64]" = unsqueeze_103.to(torch.bfloat16);  unsqueeze_103 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_77 = torch.chunk(getitem_334, 2, dim = -1);  getitem_334 = None
            getitem_336: "bf16[s0, 8, 64]" = chunk_77[0]
            getitem_337: "bf16[s0, 8, 64]" = chunk_77[1];  chunk_77 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_204: "bf16[s0, 8, 64]" = getitem_336 * to_102
            mul_205: "bf16[s0, 8, 64]" = getitem_337 * to_103
            sub_51: "bf16[s0, 8, 64]" = mul_204 - mul_205;  mul_204 = mul_205 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_206: "bf16[s0, 8, 64]" = getitem_337 * to_102;  getitem_337 = to_102 = None
            mul_207: "bf16[s0, 8, 64]" = getitem_336 * to_103;  getitem_336 = to_103 = None
            add_51: "bf16[s0, 8, 64]" = mul_206 + mul_207;  mul_206 = mul_207 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_102: "bf16[s0, 8, 128]" = torch.cat((sub_51, add_51), dim = -1);  sub_51 = add_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_103: "bf16[s0, 8, 128]" = torch.cat((cat_102, getitem_335), dim = -1);  cat_102 = getitem_335 = None
            reshape_51: "bf16[s0, 1024]" = cat_103.reshape(size_76);  cat_103 = size_76 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_77 = reshape_50.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_25: "bf16[s0, 4096]" = torch.zeros(size_77, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_77 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_177: "bf16[s0, 32, 128]" = reshape_50.view(-1, 32, 128);  reshape_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_178: "bf16[s0, 32, 128]" = zeros_25.view(-1, 32, 128);  zeros_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_179: "bf16[s0, 8, 128]" = reshape_51.view(-1, 8, 128);  reshape_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_180: "bf16[s0, 8, 128]" = getitem_327.view(-1, 8, 128);  getitem_327 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_25 = torch.ops.vllm.unified_attention_with_output(view_177, view_179, view_180, view_178, 'model.layers.25.self_attn.attn');  view_177 = view_179 = view_180 = unified_attention_with_output_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_181: "bf16[s0, 4096]" = view_178.view(-1, 4096);  view_178 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_101: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_181, l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_181 = l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_51: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_101: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_101)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_102: "bf16[s0, 4096]" = torch.empty_like(empty_like_100)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_50 = torch.ops._C.fused_add_rms_norm(empty_like_101, rocm_unquantized_gemm_impl_101, empty_like_102, empty_like_100, _get_data_attr_51, 1e-05);  rocm_unquantized_gemm_impl_101 = empty_like_100 = _get_data_attr_51 = fused_add_rms_norm_50 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_102: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_101, l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_101 = l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_25: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_25 = torch.ops._C.silu_and_mul(empty_25, rocm_unquantized_gemm_impl_102);  rocm_unquantized_gemm_impl_102 = silu_and_mul_25 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_103: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_25, l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_25 = l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_52: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_103: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_103)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_104: "bf16[s0, 4096]" = torch.empty_like(empty_like_102)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_51 = torch.ops._C.fused_add_rms_norm(empty_like_103, rocm_unquantized_gemm_impl_103, empty_like_104, empty_like_102, _get_data_attr_52, 1e-05);  rocm_unquantized_gemm_impl_103 = empty_like_102 = _get_data_attr_52 = fused_add_rms_norm_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_104: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_103, l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_103 = l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_26 = rocm_unquantized_gemm_impl_104.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_104 = None
            getitem_338: "bf16[s0, 4096]" = split_26[0]
            getitem_339: "bf16[s0, 1024]" = split_26[1]
            getitem_340: "bf16[s0, 1024]" = split_26[2];  split_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_26: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_26: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_26);  flatten_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_78 = index_select_26.chunk(2, dim = -1);  index_select_26 = None
            getitem_341: "bf16[s0, 64]" = chunk_78[0]
            getitem_342: "bf16[s0, 64]" = chunk_78[1];  chunk_78 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_78 = getitem_338.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_182: "bf16[s0, 32, 128]" = getitem_338.view(s0, -1, 128);  getitem_338 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_343: "bf16[s0, 32, 128]" = view_182[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_344: "bf16[s0, 32, 0]" = view_182[(Ellipsis, slice(128, None, None))];  view_182 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_104: "bf16[s0, 1, 64]" = getitem_341.unsqueeze(-2)
            to_104: "bf16[s0, 1, 64]" = unsqueeze_104.to(torch.bfloat16);  unsqueeze_104 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_105: "bf16[s0, 1, 64]" = getitem_342.unsqueeze(-2)
            to_105: "bf16[s0, 1, 64]" = unsqueeze_105.to(torch.bfloat16);  unsqueeze_105 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_79 = torch.chunk(getitem_343, 2, dim = -1);  getitem_343 = None
            getitem_345: "bf16[s0, 32, 64]" = chunk_79[0]
            getitem_346: "bf16[s0, 32, 64]" = chunk_79[1];  chunk_79 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_208: "bf16[s0, 32, 64]" = getitem_345 * to_104
            mul_209: "bf16[s0, 32, 64]" = getitem_346 * to_105
            sub_52: "bf16[s0, 32, 64]" = mul_208 - mul_209;  mul_208 = mul_209 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_210: "bf16[s0, 32, 64]" = getitem_346 * to_104;  getitem_346 = to_104 = None
            mul_211: "bf16[s0, 32, 64]" = getitem_345 * to_105;  getitem_345 = to_105 = None
            add_52: "bf16[s0, 32, 64]" = mul_210 + mul_211;  mul_210 = mul_211 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_104: "bf16[s0, 32, 128]" = torch.cat((sub_52, add_52), dim = -1);  sub_52 = add_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_105: "bf16[s0, 32, 128]" = torch.cat((cat_104, getitem_344), dim = -1);  cat_104 = getitem_344 = None
            reshape_52: "bf16[s0, 4096]" = cat_105.reshape(size_78);  cat_105 = size_78 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_79 = getitem_339.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_183: "bf16[s0, 8, 128]" = getitem_339.view(s0, -1, 128);  getitem_339 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_347: "bf16[s0, 8, 128]" = view_183[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_348: "bf16[s0, 8, 0]" = view_183[(Ellipsis, slice(128, None, None))];  view_183 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_106: "bf16[s0, 1, 64]" = getitem_341.unsqueeze(-2);  getitem_341 = None
            to_106: "bf16[s0, 1, 64]" = unsqueeze_106.to(torch.bfloat16);  unsqueeze_106 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_107: "bf16[s0, 1, 64]" = getitem_342.unsqueeze(-2);  getitem_342 = None
            to_107: "bf16[s0, 1, 64]" = unsqueeze_107.to(torch.bfloat16);  unsqueeze_107 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_80 = torch.chunk(getitem_347, 2, dim = -1);  getitem_347 = None
            getitem_349: "bf16[s0, 8, 64]" = chunk_80[0]
            getitem_350: "bf16[s0, 8, 64]" = chunk_80[1];  chunk_80 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_212: "bf16[s0, 8, 64]" = getitem_349 * to_106
            mul_213: "bf16[s0, 8, 64]" = getitem_350 * to_107
            sub_53: "bf16[s0, 8, 64]" = mul_212 - mul_213;  mul_212 = mul_213 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_214: "bf16[s0, 8, 64]" = getitem_350 * to_106;  getitem_350 = to_106 = None
            mul_215: "bf16[s0, 8, 64]" = getitem_349 * to_107;  getitem_349 = to_107 = None
            add_53: "bf16[s0, 8, 64]" = mul_214 + mul_215;  mul_214 = mul_215 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_106: "bf16[s0, 8, 128]" = torch.cat((sub_53, add_53), dim = -1);  sub_53 = add_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_107: "bf16[s0, 8, 128]" = torch.cat((cat_106, getitem_348), dim = -1);  cat_106 = getitem_348 = None
            reshape_53: "bf16[s0, 1024]" = cat_107.reshape(size_79);  cat_107 = size_79 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_80 = reshape_52.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_26: "bf16[s0, 4096]" = torch.zeros(size_80, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_80 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_184: "bf16[s0, 32, 128]" = reshape_52.view(-1, 32, 128);  reshape_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_185: "bf16[s0, 32, 128]" = zeros_26.view(-1, 32, 128);  zeros_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_186: "bf16[s0, 8, 128]" = reshape_53.view(-1, 8, 128);  reshape_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_187: "bf16[s0, 8, 128]" = getitem_340.view(-1, 8, 128);  getitem_340 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_26 = torch.ops.vllm.unified_attention_with_output(view_184, view_186, view_187, view_185, 'model.layers.26.self_attn.attn');  view_184 = view_186 = view_187 = unified_attention_with_output_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_188: "bf16[s0, 4096]" = view_185.view(-1, 4096);  view_185 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_105: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_188, l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_188 = l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_53: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_105: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_105)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_106: "bf16[s0, 4096]" = torch.empty_like(empty_like_104)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_52 = torch.ops._C.fused_add_rms_norm(empty_like_105, rocm_unquantized_gemm_impl_105, empty_like_106, empty_like_104, _get_data_attr_53, 1e-05);  rocm_unquantized_gemm_impl_105 = empty_like_104 = _get_data_attr_53 = fused_add_rms_norm_52 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_106: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_105, l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_105 = l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_26: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_26 = torch.ops._C.silu_and_mul(empty_26, rocm_unquantized_gemm_impl_106);  rocm_unquantized_gemm_impl_106 = silu_and_mul_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_107: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_26, l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_26 = l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_54: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_107: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_107)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_108: "bf16[s0, 4096]" = torch.empty_like(empty_like_106)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_53 = torch.ops._C.fused_add_rms_norm(empty_like_107, rocm_unquantized_gemm_impl_107, empty_like_108, empty_like_106, _get_data_attr_54, 1e-05);  rocm_unquantized_gemm_impl_107 = empty_like_106 = _get_data_attr_54 = fused_add_rms_norm_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_108: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_107, l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_107 = l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_27 = rocm_unquantized_gemm_impl_108.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_108 = None
            getitem_351: "bf16[s0, 4096]" = split_27[0]
            getitem_352: "bf16[s0, 1024]" = split_27[1]
            getitem_353: "bf16[s0, 1024]" = split_27[2];  split_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_27: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_27: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_27);  flatten_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_81 = index_select_27.chunk(2, dim = -1);  index_select_27 = None
            getitem_354: "bf16[s0, 64]" = chunk_81[0]
            getitem_355: "bf16[s0, 64]" = chunk_81[1];  chunk_81 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_81 = getitem_351.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_189: "bf16[s0, 32, 128]" = getitem_351.view(s0, -1, 128);  getitem_351 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_356: "bf16[s0, 32, 128]" = view_189[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_357: "bf16[s0, 32, 0]" = view_189[(Ellipsis, slice(128, None, None))];  view_189 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_108: "bf16[s0, 1, 64]" = getitem_354.unsqueeze(-2)
            to_108: "bf16[s0, 1, 64]" = unsqueeze_108.to(torch.bfloat16);  unsqueeze_108 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_109: "bf16[s0, 1, 64]" = getitem_355.unsqueeze(-2)
            to_109: "bf16[s0, 1, 64]" = unsqueeze_109.to(torch.bfloat16);  unsqueeze_109 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_82 = torch.chunk(getitem_356, 2, dim = -1);  getitem_356 = None
            getitem_358: "bf16[s0, 32, 64]" = chunk_82[0]
            getitem_359: "bf16[s0, 32, 64]" = chunk_82[1];  chunk_82 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_216: "bf16[s0, 32, 64]" = getitem_358 * to_108
            mul_217: "bf16[s0, 32, 64]" = getitem_359 * to_109
            sub_54: "bf16[s0, 32, 64]" = mul_216 - mul_217;  mul_216 = mul_217 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_218: "bf16[s0, 32, 64]" = getitem_359 * to_108;  getitem_359 = to_108 = None
            mul_219: "bf16[s0, 32, 64]" = getitem_358 * to_109;  getitem_358 = to_109 = None
            add_54: "bf16[s0, 32, 64]" = mul_218 + mul_219;  mul_218 = mul_219 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_108: "bf16[s0, 32, 128]" = torch.cat((sub_54, add_54), dim = -1);  sub_54 = add_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_109: "bf16[s0, 32, 128]" = torch.cat((cat_108, getitem_357), dim = -1);  cat_108 = getitem_357 = None
            reshape_54: "bf16[s0, 4096]" = cat_109.reshape(size_81);  cat_109 = size_81 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_82 = getitem_352.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_190: "bf16[s0, 8, 128]" = getitem_352.view(s0, -1, 128);  getitem_352 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_360: "bf16[s0, 8, 128]" = view_190[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_361: "bf16[s0, 8, 0]" = view_190[(Ellipsis, slice(128, None, None))];  view_190 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_110: "bf16[s0, 1, 64]" = getitem_354.unsqueeze(-2);  getitem_354 = None
            to_110: "bf16[s0, 1, 64]" = unsqueeze_110.to(torch.bfloat16);  unsqueeze_110 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_111: "bf16[s0, 1, 64]" = getitem_355.unsqueeze(-2);  getitem_355 = None
            to_111: "bf16[s0, 1, 64]" = unsqueeze_111.to(torch.bfloat16);  unsqueeze_111 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_83 = torch.chunk(getitem_360, 2, dim = -1);  getitem_360 = None
            getitem_362: "bf16[s0, 8, 64]" = chunk_83[0]
            getitem_363: "bf16[s0, 8, 64]" = chunk_83[1];  chunk_83 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_220: "bf16[s0, 8, 64]" = getitem_362 * to_110
            mul_221: "bf16[s0, 8, 64]" = getitem_363 * to_111
            sub_55: "bf16[s0, 8, 64]" = mul_220 - mul_221;  mul_220 = mul_221 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_222: "bf16[s0, 8, 64]" = getitem_363 * to_110;  getitem_363 = to_110 = None
            mul_223: "bf16[s0, 8, 64]" = getitem_362 * to_111;  getitem_362 = to_111 = None
            add_55: "bf16[s0, 8, 64]" = mul_222 + mul_223;  mul_222 = mul_223 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_110: "bf16[s0, 8, 128]" = torch.cat((sub_55, add_55), dim = -1);  sub_55 = add_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_111: "bf16[s0, 8, 128]" = torch.cat((cat_110, getitem_361), dim = -1);  cat_110 = getitem_361 = None
            reshape_55: "bf16[s0, 1024]" = cat_111.reshape(size_82);  cat_111 = size_82 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_83 = reshape_54.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_27: "bf16[s0, 4096]" = torch.zeros(size_83, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_83 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_191: "bf16[s0, 32, 128]" = reshape_54.view(-1, 32, 128);  reshape_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_192: "bf16[s0, 32, 128]" = zeros_27.view(-1, 32, 128);  zeros_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_193: "bf16[s0, 8, 128]" = reshape_55.view(-1, 8, 128);  reshape_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_194: "bf16[s0, 8, 128]" = getitem_353.view(-1, 8, 128);  getitem_353 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_27 = torch.ops.vllm.unified_attention_with_output(view_191, view_193, view_194, view_192, 'model.layers.27.self_attn.attn');  view_191 = view_193 = view_194 = unified_attention_with_output_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_195: "bf16[s0, 4096]" = view_192.view(-1, 4096);  view_192 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_109: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_195, l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_195 = l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_55: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_109: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_109)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_110: "bf16[s0, 4096]" = torch.empty_like(empty_like_108)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_54 = torch.ops._C.fused_add_rms_norm(empty_like_109, rocm_unquantized_gemm_impl_109, empty_like_110, empty_like_108, _get_data_attr_55, 1e-05);  rocm_unquantized_gemm_impl_109 = empty_like_108 = _get_data_attr_55 = fused_add_rms_norm_54 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_110: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_109, l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_109 = l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_27: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_27 = torch.ops._C.silu_and_mul(empty_27, rocm_unquantized_gemm_impl_110);  rocm_unquantized_gemm_impl_110 = silu_and_mul_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_111: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_27, l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_27 = l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_56: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_111: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_111)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_112: "bf16[s0, 4096]" = torch.empty_like(empty_like_110)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_55 = torch.ops._C.fused_add_rms_norm(empty_like_111, rocm_unquantized_gemm_impl_111, empty_like_112, empty_like_110, _get_data_attr_56, 1e-05);  rocm_unquantized_gemm_impl_111 = empty_like_110 = _get_data_attr_56 = fused_add_rms_norm_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_112: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_111, l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_111 = l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_28 = rocm_unquantized_gemm_impl_112.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_112 = None
            getitem_364: "bf16[s0, 4096]" = split_28[0]
            getitem_365: "bf16[s0, 1024]" = split_28[1]
            getitem_366: "bf16[s0, 1024]" = split_28[2];  split_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_28: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_28: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_28);  flatten_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_84 = index_select_28.chunk(2, dim = -1);  index_select_28 = None
            getitem_367: "bf16[s0, 64]" = chunk_84[0]
            getitem_368: "bf16[s0, 64]" = chunk_84[1];  chunk_84 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_84 = getitem_364.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_196: "bf16[s0, 32, 128]" = getitem_364.view(s0, -1, 128);  getitem_364 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_369: "bf16[s0, 32, 128]" = view_196[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_370: "bf16[s0, 32, 0]" = view_196[(Ellipsis, slice(128, None, None))];  view_196 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_112: "bf16[s0, 1, 64]" = getitem_367.unsqueeze(-2)
            to_112: "bf16[s0, 1, 64]" = unsqueeze_112.to(torch.bfloat16);  unsqueeze_112 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_113: "bf16[s0, 1, 64]" = getitem_368.unsqueeze(-2)
            to_113: "bf16[s0, 1, 64]" = unsqueeze_113.to(torch.bfloat16);  unsqueeze_113 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_85 = torch.chunk(getitem_369, 2, dim = -1);  getitem_369 = None
            getitem_371: "bf16[s0, 32, 64]" = chunk_85[0]
            getitem_372: "bf16[s0, 32, 64]" = chunk_85[1];  chunk_85 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_224: "bf16[s0, 32, 64]" = getitem_371 * to_112
            mul_225: "bf16[s0, 32, 64]" = getitem_372 * to_113
            sub_56: "bf16[s0, 32, 64]" = mul_224 - mul_225;  mul_224 = mul_225 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_226: "bf16[s0, 32, 64]" = getitem_372 * to_112;  getitem_372 = to_112 = None
            mul_227: "bf16[s0, 32, 64]" = getitem_371 * to_113;  getitem_371 = to_113 = None
            add_56: "bf16[s0, 32, 64]" = mul_226 + mul_227;  mul_226 = mul_227 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_112: "bf16[s0, 32, 128]" = torch.cat((sub_56, add_56), dim = -1);  sub_56 = add_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_113: "bf16[s0, 32, 128]" = torch.cat((cat_112, getitem_370), dim = -1);  cat_112 = getitem_370 = None
            reshape_56: "bf16[s0, 4096]" = cat_113.reshape(size_84);  cat_113 = size_84 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_85 = getitem_365.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_197: "bf16[s0, 8, 128]" = getitem_365.view(s0, -1, 128);  getitem_365 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_373: "bf16[s0, 8, 128]" = view_197[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_374: "bf16[s0, 8, 0]" = view_197[(Ellipsis, slice(128, None, None))];  view_197 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_114: "bf16[s0, 1, 64]" = getitem_367.unsqueeze(-2);  getitem_367 = None
            to_114: "bf16[s0, 1, 64]" = unsqueeze_114.to(torch.bfloat16);  unsqueeze_114 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_115: "bf16[s0, 1, 64]" = getitem_368.unsqueeze(-2);  getitem_368 = None
            to_115: "bf16[s0, 1, 64]" = unsqueeze_115.to(torch.bfloat16);  unsqueeze_115 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_86 = torch.chunk(getitem_373, 2, dim = -1);  getitem_373 = None
            getitem_375: "bf16[s0, 8, 64]" = chunk_86[0]
            getitem_376: "bf16[s0, 8, 64]" = chunk_86[1];  chunk_86 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_228: "bf16[s0, 8, 64]" = getitem_375 * to_114
            mul_229: "bf16[s0, 8, 64]" = getitem_376 * to_115
            sub_57: "bf16[s0, 8, 64]" = mul_228 - mul_229;  mul_228 = mul_229 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_230: "bf16[s0, 8, 64]" = getitem_376 * to_114;  getitem_376 = to_114 = None
            mul_231: "bf16[s0, 8, 64]" = getitem_375 * to_115;  getitem_375 = to_115 = None
            add_57: "bf16[s0, 8, 64]" = mul_230 + mul_231;  mul_230 = mul_231 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_114: "bf16[s0, 8, 128]" = torch.cat((sub_57, add_57), dim = -1);  sub_57 = add_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_115: "bf16[s0, 8, 128]" = torch.cat((cat_114, getitem_374), dim = -1);  cat_114 = getitem_374 = None
            reshape_57: "bf16[s0, 1024]" = cat_115.reshape(size_85);  cat_115 = size_85 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_86 = reshape_56.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_28: "bf16[s0, 4096]" = torch.zeros(size_86, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_86 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_198: "bf16[s0, 32, 128]" = reshape_56.view(-1, 32, 128);  reshape_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_199: "bf16[s0, 32, 128]" = zeros_28.view(-1, 32, 128);  zeros_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_200: "bf16[s0, 8, 128]" = reshape_57.view(-1, 8, 128);  reshape_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_201: "bf16[s0, 8, 128]" = getitem_366.view(-1, 8, 128);  getitem_366 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_28 = torch.ops.vllm.unified_attention_with_output(view_198, view_200, view_201, view_199, 'model.layers.28.self_attn.attn');  view_198 = view_200 = view_201 = unified_attention_with_output_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_202: "bf16[s0, 4096]" = view_199.view(-1, 4096);  view_199 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_113: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_202, l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_202 = l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_57: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_113: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_113)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_114: "bf16[s0, 4096]" = torch.empty_like(empty_like_112)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_56 = torch.ops._C.fused_add_rms_norm(empty_like_113, rocm_unquantized_gemm_impl_113, empty_like_114, empty_like_112, _get_data_attr_57, 1e-05);  rocm_unquantized_gemm_impl_113 = empty_like_112 = _get_data_attr_57 = fused_add_rms_norm_56 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_114: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_113, l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_113 = l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_28: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_28 = torch.ops._C.silu_and_mul(empty_28, rocm_unquantized_gemm_impl_114);  rocm_unquantized_gemm_impl_114 = silu_and_mul_28 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_115: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_28, l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_28 = l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_58: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_115: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_115)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_116: "bf16[s0, 4096]" = torch.empty_like(empty_like_114)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_57 = torch.ops._C.fused_add_rms_norm(empty_like_115, rocm_unquantized_gemm_impl_115, empty_like_116, empty_like_114, _get_data_attr_58, 1e-05);  rocm_unquantized_gemm_impl_115 = empty_like_114 = _get_data_attr_58 = fused_add_rms_norm_57 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_116: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_115, l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_115 = l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_29 = rocm_unquantized_gemm_impl_116.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_116 = None
            getitem_377: "bf16[s0, 4096]" = split_29[0]
            getitem_378: "bf16[s0, 1024]" = split_29[1]
            getitem_379: "bf16[s0, 1024]" = split_29[2];  split_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_29: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_29: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_29);  flatten_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_87 = index_select_29.chunk(2, dim = -1);  index_select_29 = None
            getitem_380: "bf16[s0, 64]" = chunk_87[0]
            getitem_381: "bf16[s0, 64]" = chunk_87[1];  chunk_87 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_87 = getitem_377.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_203: "bf16[s0, 32, 128]" = getitem_377.view(s0, -1, 128);  getitem_377 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_382: "bf16[s0, 32, 128]" = view_203[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_383: "bf16[s0, 32, 0]" = view_203[(Ellipsis, slice(128, None, None))];  view_203 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_116: "bf16[s0, 1, 64]" = getitem_380.unsqueeze(-2)
            to_116: "bf16[s0, 1, 64]" = unsqueeze_116.to(torch.bfloat16);  unsqueeze_116 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_117: "bf16[s0, 1, 64]" = getitem_381.unsqueeze(-2)
            to_117: "bf16[s0, 1, 64]" = unsqueeze_117.to(torch.bfloat16);  unsqueeze_117 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_88 = torch.chunk(getitem_382, 2, dim = -1);  getitem_382 = None
            getitem_384: "bf16[s0, 32, 64]" = chunk_88[0]
            getitem_385: "bf16[s0, 32, 64]" = chunk_88[1];  chunk_88 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_232: "bf16[s0, 32, 64]" = getitem_384 * to_116
            mul_233: "bf16[s0, 32, 64]" = getitem_385 * to_117
            sub_58: "bf16[s0, 32, 64]" = mul_232 - mul_233;  mul_232 = mul_233 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_234: "bf16[s0, 32, 64]" = getitem_385 * to_116;  getitem_385 = to_116 = None
            mul_235: "bf16[s0, 32, 64]" = getitem_384 * to_117;  getitem_384 = to_117 = None
            add_58: "bf16[s0, 32, 64]" = mul_234 + mul_235;  mul_234 = mul_235 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_116: "bf16[s0, 32, 128]" = torch.cat((sub_58, add_58), dim = -1);  sub_58 = add_58 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_117: "bf16[s0, 32, 128]" = torch.cat((cat_116, getitem_383), dim = -1);  cat_116 = getitem_383 = None
            reshape_58: "bf16[s0, 4096]" = cat_117.reshape(size_87);  cat_117 = size_87 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_88 = getitem_378.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_204: "bf16[s0, 8, 128]" = getitem_378.view(s0, -1, 128);  getitem_378 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_386: "bf16[s0, 8, 128]" = view_204[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_387: "bf16[s0, 8, 0]" = view_204[(Ellipsis, slice(128, None, None))];  view_204 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_118: "bf16[s0, 1, 64]" = getitem_380.unsqueeze(-2);  getitem_380 = None
            to_118: "bf16[s0, 1, 64]" = unsqueeze_118.to(torch.bfloat16);  unsqueeze_118 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_119: "bf16[s0, 1, 64]" = getitem_381.unsqueeze(-2);  getitem_381 = None
            to_119: "bf16[s0, 1, 64]" = unsqueeze_119.to(torch.bfloat16);  unsqueeze_119 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_89 = torch.chunk(getitem_386, 2, dim = -1);  getitem_386 = None
            getitem_388: "bf16[s0, 8, 64]" = chunk_89[0]
            getitem_389: "bf16[s0, 8, 64]" = chunk_89[1];  chunk_89 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_236: "bf16[s0, 8, 64]" = getitem_388 * to_118
            mul_237: "bf16[s0, 8, 64]" = getitem_389 * to_119
            sub_59: "bf16[s0, 8, 64]" = mul_236 - mul_237;  mul_236 = mul_237 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_238: "bf16[s0, 8, 64]" = getitem_389 * to_118;  getitem_389 = to_118 = None
            mul_239: "bf16[s0, 8, 64]" = getitem_388 * to_119;  getitem_388 = to_119 = None
            add_59: "bf16[s0, 8, 64]" = mul_238 + mul_239;  mul_238 = mul_239 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_118: "bf16[s0, 8, 128]" = torch.cat((sub_59, add_59), dim = -1);  sub_59 = add_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_119: "bf16[s0, 8, 128]" = torch.cat((cat_118, getitem_387), dim = -1);  cat_118 = getitem_387 = None
            reshape_59: "bf16[s0, 1024]" = cat_119.reshape(size_88);  cat_119 = size_88 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_89 = reshape_58.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_29: "bf16[s0, 4096]" = torch.zeros(size_89, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_89 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_205: "bf16[s0, 32, 128]" = reshape_58.view(-1, 32, 128);  reshape_58 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_206: "bf16[s0, 32, 128]" = zeros_29.view(-1, 32, 128);  zeros_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_207: "bf16[s0, 8, 128]" = reshape_59.view(-1, 8, 128);  reshape_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_208: "bf16[s0, 8, 128]" = getitem_379.view(-1, 8, 128);  getitem_379 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_29 = torch.ops.vllm.unified_attention_with_output(view_205, view_207, view_208, view_206, 'model.layers.29.self_attn.attn');  view_205 = view_207 = view_208 = unified_attention_with_output_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_209: "bf16[s0, 4096]" = view_206.view(-1, 4096);  view_206 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_117: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_209, l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_209 = l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_59: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_117: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_117)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_118: "bf16[s0, 4096]" = torch.empty_like(empty_like_116)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_58 = torch.ops._C.fused_add_rms_norm(empty_like_117, rocm_unquantized_gemm_impl_117, empty_like_118, empty_like_116, _get_data_attr_59, 1e-05);  rocm_unquantized_gemm_impl_117 = empty_like_116 = _get_data_attr_59 = fused_add_rms_norm_58 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_118: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_117, l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_117 = l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_29: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_29 = torch.ops._C.silu_and_mul(empty_29, rocm_unquantized_gemm_impl_118);  rocm_unquantized_gemm_impl_118 = silu_and_mul_29 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_119: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_29, l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_29 = l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_60: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_119: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_119)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_120: "bf16[s0, 4096]" = torch.empty_like(empty_like_118)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_59 = torch.ops._C.fused_add_rms_norm(empty_like_119, rocm_unquantized_gemm_impl_119, empty_like_120, empty_like_118, _get_data_attr_60, 1e-05);  rocm_unquantized_gemm_impl_119 = empty_like_118 = _get_data_attr_60 = fused_add_rms_norm_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_120: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_119, l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_119 = l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_30 = rocm_unquantized_gemm_impl_120.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_120 = None
            getitem_390: "bf16[s0, 4096]" = split_30[0]
            getitem_391: "bf16[s0, 1024]" = split_30[1]
            getitem_392: "bf16[s0, 1024]" = split_30[2];  split_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_30: "i64[s0]" = l_positions_.flatten()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_30: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_30);  flatten_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_90 = index_select_30.chunk(2, dim = -1);  index_select_30 = None
            getitem_393: "bf16[s0, 64]" = chunk_90[0]
            getitem_394: "bf16[s0, 64]" = chunk_90[1];  chunk_90 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_90 = getitem_390.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_210: "bf16[s0, 32, 128]" = getitem_390.view(s0, -1, 128);  getitem_390 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_395: "bf16[s0, 32, 128]" = view_210[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_396: "bf16[s0, 32, 0]" = view_210[(Ellipsis, slice(128, None, None))];  view_210 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_120: "bf16[s0, 1, 64]" = getitem_393.unsqueeze(-2)
            to_120: "bf16[s0, 1, 64]" = unsqueeze_120.to(torch.bfloat16);  unsqueeze_120 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_121: "bf16[s0, 1, 64]" = getitem_394.unsqueeze(-2)
            to_121: "bf16[s0, 1, 64]" = unsqueeze_121.to(torch.bfloat16);  unsqueeze_121 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_91 = torch.chunk(getitem_395, 2, dim = -1);  getitem_395 = None
            getitem_397: "bf16[s0, 32, 64]" = chunk_91[0]
            getitem_398: "bf16[s0, 32, 64]" = chunk_91[1];  chunk_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_240: "bf16[s0, 32, 64]" = getitem_397 * to_120
            mul_241: "bf16[s0, 32, 64]" = getitem_398 * to_121
            sub_60: "bf16[s0, 32, 64]" = mul_240 - mul_241;  mul_240 = mul_241 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_242: "bf16[s0, 32, 64]" = getitem_398 * to_120;  getitem_398 = to_120 = None
            mul_243: "bf16[s0, 32, 64]" = getitem_397 * to_121;  getitem_397 = to_121 = None
            add_60: "bf16[s0, 32, 64]" = mul_242 + mul_243;  mul_242 = mul_243 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_120: "bf16[s0, 32, 128]" = torch.cat((sub_60, add_60), dim = -1);  sub_60 = add_60 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_121: "bf16[s0, 32, 128]" = torch.cat((cat_120, getitem_396), dim = -1);  cat_120 = getitem_396 = None
            reshape_60: "bf16[s0, 4096]" = cat_121.reshape(size_90);  cat_121 = size_90 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_91 = getitem_391.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_211: "bf16[s0, 8, 128]" = getitem_391.view(s0, -1, 128);  getitem_391 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_399: "bf16[s0, 8, 128]" = view_211[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_400: "bf16[s0, 8, 0]" = view_211[(Ellipsis, slice(128, None, None))];  view_211 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_122: "bf16[s0, 1, 64]" = getitem_393.unsqueeze(-2);  getitem_393 = None
            to_122: "bf16[s0, 1, 64]" = unsqueeze_122.to(torch.bfloat16);  unsqueeze_122 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_123: "bf16[s0, 1, 64]" = getitem_394.unsqueeze(-2);  getitem_394 = None
            to_123: "bf16[s0, 1, 64]" = unsqueeze_123.to(torch.bfloat16);  unsqueeze_123 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_92 = torch.chunk(getitem_399, 2, dim = -1);  getitem_399 = None
            getitem_401: "bf16[s0, 8, 64]" = chunk_92[0]
            getitem_402: "bf16[s0, 8, 64]" = chunk_92[1];  chunk_92 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_244: "bf16[s0, 8, 64]" = getitem_401 * to_122
            mul_245: "bf16[s0, 8, 64]" = getitem_402 * to_123
            sub_61: "bf16[s0, 8, 64]" = mul_244 - mul_245;  mul_244 = mul_245 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_246: "bf16[s0, 8, 64]" = getitem_402 * to_122;  getitem_402 = to_122 = None
            mul_247: "bf16[s0, 8, 64]" = getitem_401 * to_123;  getitem_401 = to_123 = None
            add_61: "bf16[s0, 8, 64]" = mul_246 + mul_247;  mul_246 = mul_247 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_122: "bf16[s0, 8, 128]" = torch.cat((sub_61, add_61), dim = -1);  sub_61 = add_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_123: "bf16[s0, 8, 128]" = torch.cat((cat_122, getitem_400), dim = -1);  cat_122 = getitem_400 = None
            reshape_61: "bf16[s0, 1024]" = cat_123.reshape(size_91);  cat_123 = size_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_92 = reshape_60.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_30: "bf16[s0, 4096]" = torch.zeros(size_92, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_92 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_212: "bf16[s0, 32, 128]" = reshape_60.view(-1, 32, 128);  reshape_60 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_213: "bf16[s0, 32, 128]" = zeros_30.view(-1, 32, 128);  zeros_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_214: "bf16[s0, 8, 128]" = reshape_61.view(-1, 8, 128);  reshape_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_215: "bf16[s0, 8, 128]" = getitem_392.view(-1, 8, 128);  getitem_392 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_30 = torch.ops.vllm.unified_attention_with_output(view_212, view_214, view_215, view_213, 'model.layers.30.self_attn.attn');  view_212 = view_214 = view_215 = unified_attention_with_output_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_216: "bf16[s0, 4096]" = view_213.view(-1, 4096);  view_213 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_121: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_216, l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_216 = l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_61: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_121: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_121)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_122: "bf16[s0, 4096]" = torch.empty_like(empty_like_120)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_60 = torch.ops._C.fused_add_rms_norm(empty_like_121, rocm_unquantized_gemm_impl_121, empty_like_122, empty_like_120, _get_data_attr_61, 1e-05);  rocm_unquantized_gemm_impl_121 = empty_like_120 = _get_data_attr_61 = fused_add_rms_norm_60 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_122: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_121, l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_121 = l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_30: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0))
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_30 = torch.ops._C.silu_and_mul(empty_30, rocm_unquantized_gemm_impl_122);  rocm_unquantized_gemm_impl_122 = silu_and_mul_30 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_123: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_30, l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_30 = l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_62: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_123: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_123)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_124: "bf16[s0, 4096]" = torch.empty_like(empty_like_122)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_61 = torch.ops._C.fused_add_rms_norm(empty_like_123, rocm_unquantized_gemm_impl_123, empty_like_124, empty_like_122, _get_data_attr_62, 1e-05);  rocm_unquantized_gemm_impl_123 = empty_like_122 = _get_data_attr_62 = fused_add_rms_norm_61 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_124: "bf16[s0, 6144]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_123, l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  empty_like_123 = l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py:201 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split_31 = rocm_unquantized_gemm_impl_124.split([4096, 1024, 1024], dim = -1);  rocm_unquantized_gemm_impl_124 = None
            getitem_403: "bf16[s0, 4096]" = split_31[0]
            getitem_404: "bf16[s0, 1024]" = split_31[1]
            getitem_405: "bf16[s0, 1024]" = split_31[2];  split_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:70 in forward_native, code: positions = positions.flatten()
            flatten_31: "i64[s0]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:72 in forward_native, code: cos_sin = self.cos_sin_cache.index_select(0, positions)
            index_select_31: "bf16[s0, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_31);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:73 in forward_native, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk_93 = index_select_31.chunk(2, dim = -1);  index_select_31 = None
            getitem_406: "bf16[s0, 64]" = chunk_93[0]
            getitem_407: "bf16[s0, 64]" = chunk_93[1];  chunk_93 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:75 in forward_native, code: query_shape = query.shape
            size_93 = getitem_403.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:76 in forward_native, code: query = query.view(num_tokens, -1, self.head_size)
            view_217: "bf16[s0, 32, 128]" = getitem_403.view(s0, -1, 128);  getitem_403 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:77 in forward_native, code: query_rot = query[..., :self.rotary_dim]
            getitem_408: "bf16[s0, 32, 128]" = view_217[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:78 in forward_native, code: query_pass = query[..., self.rotary_dim:]
            getitem_409: "bf16[s0, 32, 0]" = view_217[(Ellipsis, slice(128, None, None))];  view_217 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_124: "bf16[s0, 1, 64]" = getitem_406.unsqueeze(-2)
            to_124: "bf16[s0, 1, 64]" = unsqueeze_124.to(torch.bfloat16);  unsqueeze_124 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_125: "bf16[s0, 1, 64]" = getitem_407.unsqueeze(-2)
            to_125: "bf16[s0, 1, 64]" = unsqueeze_125.to(torch.bfloat16);  unsqueeze_125 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_94 = torch.chunk(getitem_408, 2, dim = -1);  getitem_408 = None
            getitem_410: "bf16[s0, 32, 64]" = chunk_94[0]
            getitem_411: "bf16[s0, 32, 64]" = chunk_94[1];  chunk_94 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_248: "bf16[s0, 32, 64]" = getitem_410 * to_124
            mul_249: "bf16[s0, 32, 64]" = getitem_411 * to_125
            sub_62: "bf16[s0, 32, 64]" = mul_248 - mul_249;  mul_248 = mul_249 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_250: "bf16[s0, 32, 64]" = getitem_411 * to_124;  getitem_411 = to_124 = None
            mul_251: "bf16[s0, 32, 64]" = getitem_410 * to_125;  getitem_410 = to_125 = None
            add_62: "bf16[s0, 32, 64]" = mul_250 + mul_251;  mul_250 = mul_251 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_124: "bf16[s0, 32, 128]" = torch.cat((sub_62, add_62), dim = -1);  sub_62 = add_62 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:81 in forward_native, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_125: "bf16[s0, 32, 128]" = torch.cat((cat_124, getitem_409), dim = -1);  cat_124 = getitem_409 = None
            reshape_62: "bf16[s0, 4096]" = cat_125.reshape(size_93);  cat_125 = size_93 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:85 in forward_native, code: key_shape = key.shape
            size_94 = getitem_404.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:86 in forward_native, code: key = key.view(num_tokens, -1, self.head_size)
            view_218: "bf16[s0, 8, 128]" = getitem_404.view(s0, -1, 128);  getitem_404 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:87 in forward_native, code: key_rot = key[..., :self.rotary_dim]
            getitem_412: "bf16[s0, 8, 128]" = view_218[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:88 in forward_native, code: key_pass = key[..., self.rotary_dim:]
            getitem_413: "bf16[s0, 8, 0]" = view_218[(Ellipsis, slice(128, None, None))];  view_218 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:34 in apply_rotary_emb_torch, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_126: "bf16[s0, 1, 64]" = getitem_406.unsqueeze(-2);  getitem_406 = None
            to_126: "bf16[s0, 1, 64]" = unsqueeze_126.to(torch.bfloat16);  unsqueeze_126 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:35 in apply_rotary_emb_torch, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_127: "bf16[s0, 1, 64]" = getitem_407.unsqueeze(-2);  getitem_407 = None
            to_127: "bf16[s0, 1, 64]" = unsqueeze_127.to(torch.bfloat16);  unsqueeze_127 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:37 in apply_rotary_emb_torch, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_95 = torch.chunk(getitem_412, 2, dim = -1);  getitem_412 = None
            getitem_414: "bf16[s0, 8, 64]" = chunk_95[0]
            getitem_415: "bf16[s0, 8, 64]" = chunk_95[1];  chunk_95 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:41 in apply_rotary_emb_torch, code: o1 = x1 * cos - x2 * sin
            mul_252: "bf16[s0, 8, 64]" = getitem_414 * to_126
            mul_253: "bf16[s0, 8, 64]" = getitem_415 * to_127
            sub_63: "bf16[s0, 8, 64]" = mul_252 - mul_253;  mul_252 = mul_253 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:42 in apply_rotary_emb_torch, code: o2 = x2 * cos + x1 * sin
            mul_254: "bf16[s0, 8, 64]" = getitem_415 * to_126;  getitem_415 = to_126 = None
            mul_255: "bf16[s0, 8, 64]" = getitem_414 * to_127;  getitem_414 = to_127 = None
            add_63: "bf16[s0, 8, 64]" = mul_254 + mul_255;  mul_254 = mul_255 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:44 in apply_rotary_emb_torch, code: return torch.cat((o1, o2), dim=-1)
            cat_126: "bf16[s0, 8, 128]" = torch.cat((sub_63, add_63), dim = -1);  sub_63 = add_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:91 in forward_native, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_127: "bf16[s0, 8, 128]" = torch.cat((cat_126, getitem_413), dim = -1);  cat_126 = getitem_413 = None
            reshape_63: "bf16[s0, 1024]" = cat_127.reshape(size_94);  cat_127 = size_94 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:243 in forward, code: if output_shape is not None else query.shape)
            size_95 = reshape_62.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:244 in forward, code: output = torch.zeros(output_shape,
            zeros_31: "bf16[s0, 4096]" = torch.zeros(size_95, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_95 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:255 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_219: "bf16[s0, 32, 128]" = reshape_62.view(-1, 32, 128);  reshape_62 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:256 in forward, code: output = output.view(-1, self.num_heads, self.head_size)
            view_220: "bf16[s0, 32, 128]" = zeros_31.view(-1, 32, 128);  zeros_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:258 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_221: "bf16[s0, 8, 128]" = reshape_63.view(-1, 8, 128);  reshape_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:260 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size)
            view_222: "bf16[s0, 8, 128]" = getitem_405.view(-1, 8, 128);  getitem_405 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:275 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output_31 = torch.ops.vllm.unified_attention_with_output(view_219, view_221, view_222, view_220, 'model.layers.31.self_attn.attn');  view_219 = view_221 = view_222 = unified_attention_with_output_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:277 in forward, code: return output.view(-1, hidden_size)
            view_223: "bf16[s0, 4096]" = view_220.view(-1, 4096);  view_220 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_125: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(view_223, l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, None);  view_223 = l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_63: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_125: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_125)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_126: "bf16[s0, 4096]" = torch.empty_like(empty_like_124)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_62 = torch.ops._C.fused_add_rms_norm(empty_like_125, rocm_unquantized_gemm_impl_125, empty_like_126, empty_like_124, _get_data_attr_63, 1e-05);  rocm_unquantized_gemm_impl_125 = empty_like_124 = _get_data_attr_63 = fused_add_rms_norm_62 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_126: "bf16[s0, 28672]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_like_125, l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  empty_like_125 = l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:84 in forward_cuda, code: out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
            empty_31: "bf16[s0, 14336]" = torch.empty((s0, 14336), dtype = torch.bfloat16, device = device(type='cuda', index=0));  s0 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:85 in forward_cuda, code: self.op(out, x)
            silu_and_mul_31 = torch.ops._C.silu_and_mul(empty_31, rocm_unquantized_gemm_impl_126);  rocm_unquantized_gemm_impl_126 = silu_and_mul_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:112 in rocm_unquantized_gemm, code: return torch.ops.vllm.rocm_unquantized_gemm_impl(x, weight, bias)
            rocm_unquantized_gemm_impl_127: "bf16[s0, 4096]" = torch.ops.vllm.rocm_unquantized_gemm_impl(empty_31, l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, None);  empty_31 = l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:172 in forward_cuda, code: return norm_func(x, residual, self.weight.data,
            _get_data_attr_64: "bf16[4096]" = torch._C._autograd._get_data_attr(l_self_modules_norm_parameters_weight_);  l_self_modules_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:37 in fused_add_rms_norm, code: out = torch.empty_like(x)
            empty_like_127: "bf16[s0, 4096]" = torch.empty_like(rocm_unquantized_gemm_impl_127)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:38 in fused_add_rms_norm, code: residual_out = torch.empty_like(residual)
            empty_like_128: "bf16[s0, 4096]" = torch.empty_like(empty_like_126)
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py:281 in fused_add_rms_norm, code: torch.ops._C.fused_add_rms_norm(out, input, residual_out, residual, weight,
            fused_add_rms_norm_63 = torch.ops._C.fused_add_rms_norm(empty_like_127, rocm_unquantized_gemm_impl_127, empty_like_128, empty_like_126, _get_data_attr_64, 1e-05);  rocm_unquantized_gemm_impl_127 = empty_like_128 = empty_like_126 = _get_data_attr_64 = fused_add_rms_norm_63 = None
            return empty_like_127
            